{
    "docs": [
        {
            "location": "/",
            "text": "Welcome!\n\u00b6\n\n\nWelcome to the Delphix Virtualization SDK doc page! Here, we hope you'll find all you need to know in order to develop your own plugins.\n\n\nOverview\n\u00b6\n\n\nIs this your first time here? Are you wondering what a plugin is, and why you'd want to develop one? Read on!\n\n\nIf you already know about plugins, and are looking for something specific, you can use the links to the left, or the search bar above, to find what you're looking for.\n\n\nWhat Is a Plugin For?\n\u00b6\n\n\nThe Delphix Engine is an appliance that lets users quickly and cheaply make \"virtual copies\" of large datasets. The Delphix Engine has built-in support for interfacing with certain types of datasets (for example Oracle and SQL Server).\n\n\nBut what happens when you want to use a Delphix Engine with a dataset that does not have built-in support? That's where plugins come in.\n\n\nA plugin extends the functionality of the Delphix Engine to add support for some particular kind of dataset. The plugin teaches the Delphix Engine how to do some operations on such datasets: How are they stopped and started? Where is there data stored? How can they be copied? Etc.\n\n\nThese plugin operations are used as building blocks by the Delphix Engine. From these building blocks, the engine can provide all of the normal Delphix functionality: provisioning, rewinding, replication, syncing, etc.\n\n\nWhen you develop a plugin, you enable end users to use your dataset type just the same as if they were using a builtin dataset type.\n\n\nHow to Get Started\n\u00b6\n\n\nWe recommend that you read through the first few sections of this documentation. As you read, the docs will walk you through how to get setup for development, and how to develop, build, and deploy your first plugin.\n\n\nFirst, follow the Getting Started With the SDK \nTODO: link to this doc when it is done\n documentation. When you finish with this, you will have a full plugin development environment, and you'll be ready to start development.\n\n\nSecond, follow the \nYour First Plugin\n documentation. This is a step-by-step tutorial in which you will develop a very simple plugin. This first plugin is intended mainly as a way to learn the concepts and techniques you'll need to develop your own real plugins later. Nevertheless, this first plugin is not useless -- you will really be able to virtualize simple datasets with it.\n\n\nOnce you've completed these two sections, feel free to use the rest of the documentation however you feel. We have a full \nreference section\n, and a more detailed example of a more full-featured plugin that does more complicated work \nTODO: link to this doc when it exists\n.",
            "title": "Welcome!"
        },
        {
            "location": "/#welcome",
            "text": "Welcome to the Delphix Virtualization SDK doc page! Here, we hope you'll find all you need to know in order to develop your own plugins.",
            "title": "Welcome!"
        },
        {
            "location": "/#overview",
            "text": "Is this your first time here? Are you wondering what a plugin is, and why you'd want to develop one? Read on!  If you already know about plugins, and are looking for something specific, you can use the links to the left, or the search bar above, to find what you're looking for.",
            "title": "Overview"
        },
        {
            "location": "/#what-is-a-plugin-for",
            "text": "The Delphix Engine is an appliance that lets users quickly and cheaply make \"virtual copies\" of large datasets. The Delphix Engine has built-in support for interfacing with certain types of datasets (for example Oracle and SQL Server).  But what happens when you want to use a Delphix Engine with a dataset that does not have built-in support? That's where plugins come in.  A plugin extends the functionality of the Delphix Engine to add support for some particular kind of dataset. The plugin teaches the Delphix Engine how to do some operations on such datasets: How are they stopped and started? Where is there data stored? How can they be copied? Etc.  These plugin operations are used as building blocks by the Delphix Engine. From these building blocks, the engine can provide all of the normal Delphix functionality: provisioning, rewinding, replication, syncing, etc.  When you develop a plugin, you enable end users to use your dataset type just the same as if they were using a builtin dataset type.",
            "title": "What Is a Plugin For?"
        },
        {
            "location": "/#how-to-get-started",
            "text": "We recommend that you read through the first few sections of this documentation. As you read, the docs will walk you through how to get setup for development, and how to develop, build, and deploy your first plugin.  First, follow the Getting Started With the SDK  TODO: link to this doc when it is done  documentation. When you finish with this, you will have a full plugin development environment, and you'll be ready to start development.  Second, follow the  Your First Plugin  documentation. This is a step-by-step tutorial in which you will develop a very simple plugin. This first plugin is intended mainly as a way to learn the concepts and techniques you'll need to develop your own real plugins later. Nevertheless, this first plugin is not useless -- you will really be able to virtualize simple datasets with it.  Once you've completed these two sections, feel free to use the rest of the documentation however you feel. We have a full  reference section , and a more detailed example of a more full-featured plugin that does more complicated work  TODO: link to this doc when it exists .",
            "title": "How to Get Started"
        },
        {
            "location": "/Your_First_Plugin/Overview/",
            "text": "Overview\n\u00b6\n\n\nIn the following few pages, we'll walk through an example of making a simple, working plugin.\n\n\nOur plugin will virtualize simple directory trees on Unix systems. The actual contents of these directories could be anything: configuration files, documents, image libraries, etc. Our plugin won't care about the contents, though. To us, we just treat it as a directory tree full of files.\n\n\nData Flow in the Delphix Engine\n\u00b6\n\n\nHere we'll briefly overview how data moves through the Delphix Engine.\n\n\nIngestion\n\u00b6\n\n\nThe first step is that Delphix needs to ingest data. Essentially, this means copying some data from a \"source environment\" onto the Delphix Engine.\n\n\nThere are two basic strategies a plugin can use to do this copying, called \"direct linking\" and \"staged linking\". Our plugin will use the staged linking strategy, but see (link to reference) for more information about direct linking.\n\n\nWith staged linking, Delphix will mount an NFS share onto a \"staging environment\" . This environment can be the same as the source environment, but it could also be different. We'll write our plugin to handle both cases.\n\n\nOnce Delphix mounts the NFS share onto the staging environment, the plugin needs to arrange for the relevant data to be copied from the source environment onto the NFS share, which is backed by Delphix Engine storage.\n\n\nWhen this initial copy is complete, Delphix will take a snapshot of the backing storage.\n\n\n(diagram goes here)\n\n\nSubsequently, this same basic operation will be repeated: Delphix mounts an NFS share, the plugin copies data onto it, Delphix snapshots the result.\n\n\nProvisioning\n\u00b6\n\n\nAny of these Delphix Engine snapshots can be used to create a virtual dataset, by \"provisioning\".\n\n\nThe snapshot is cloned on the Delphix Engine, and this newly-cloned data is mounted onto a \"target environment\" as a new virtual dataset. Any updates made to this virtual dataset will not affect the original snapshot from which it was provisioned.\n\n\n(diagram goes here)\n\n\nParts of a Plugin\n\u00b6\n\n\nThere are three main parts of a plugin. We'll cover them briefly here, and then fill in more details later in the tutorial.\n\n\nManifest\n\u00b6\n\n\nThe manifest is where the plugin describes itself to the Delphix Engine. What is the plugin called? What version of the plugin is this? What type(s) of environments does the plugin work with? What features does the plugin offer?...\n\n\nOperations/Code\n\u00b6\n\n\nThe plugin will need to provide \"operations\". These are Python functions, each of which implement one small piece of functionality. This is how the plugin can customize Delphix behavior to work with a particular kind of dataset.\nOne operation will handle setting up a newly-configured virtual dataset. One operation will handle copying data from a source environment, and so on.\n\n\nLater in this tutorial we will cover the specific examples we need for our first plugin. See (link to reference) for full details on which operations are available, which are required, what each one is required to do, etc.. Also see (link to advanced) for a more-full featured example which uses many more operations.\n\n\nDatatype Definitions\n\u00b6\n\n\nAs part of normal operation, a plugin will need to generate and access certain pieces of information in order to do its job. For example, a plugin that works with Postgres might need to know which port number to connect to, or which credentials to use, etc.\n\n\nDifferent plugins will have vastly different needs for what information is required here, and the Delphix Engine needs to know the details. Therefore, a plugin can define its own datatypes, which it does by providing \"schemas\". We'll go into more detail on this later in the tutorial, and you can see full details here (link to reference).\n\n\nPrerequisites\n\u00b6\n\n\nIn order to complete the following tutorial, you'll need to make sure you have the following set up and ready to go:\n\n\n\n\nYou should have a SDK already downloaded and working, as described here (link)\n\n\nYou should have a Delphix Engine, version x.y.z or above.\n\n\nYou should have at least one, but preferably three, Unix hosts that can be added to the Delphix Engine as remote environments.\n\n\nSome tool for editing text files (mostly Python and JSON). A simple text editor would work fine, or you can use a full-fledged IDE.",
            "title": "Overview"
        },
        {
            "location": "/Your_First_Plugin/Overview/#overview",
            "text": "In the following few pages, we'll walk through an example of making a simple, working plugin.  Our plugin will virtualize simple directory trees on Unix systems. The actual contents of these directories could be anything: configuration files, documents, image libraries, etc. Our plugin won't care about the contents, though. To us, we just treat it as a directory tree full of files.",
            "title": "Overview"
        },
        {
            "location": "/Your_First_Plugin/Overview/#data-flow-in-the-delphix-engine",
            "text": "Here we'll briefly overview how data moves through the Delphix Engine.",
            "title": "Data Flow in the Delphix Engine"
        },
        {
            "location": "/Your_First_Plugin/Overview/#ingestion",
            "text": "The first step is that Delphix needs to ingest data. Essentially, this means copying some data from a \"source environment\" onto the Delphix Engine.  There are two basic strategies a plugin can use to do this copying, called \"direct linking\" and \"staged linking\". Our plugin will use the staged linking strategy, but see (link to reference) for more information about direct linking.  With staged linking, Delphix will mount an NFS share onto a \"staging environment\" . This environment can be the same as the source environment, but it could also be different. We'll write our plugin to handle both cases.  Once Delphix mounts the NFS share onto the staging environment, the plugin needs to arrange for the relevant data to be copied from the source environment onto the NFS share, which is backed by Delphix Engine storage.  When this initial copy is complete, Delphix will take a snapshot of the backing storage.  (diagram goes here)  Subsequently, this same basic operation will be repeated: Delphix mounts an NFS share, the plugin copies data onto it, Delphix snapshots the result.",
            "title": "Ingestion"
        },
        {
            "location": "/Your_First_Plugin/Overview/#provisioning",
            "text": "Any of these Delphix Engine snapshots can be used to create a virtual dataset, by \"provisioning\".  The snapshot is cloned on the Delphix Engine, and this newly-cloned data is mounted onto a \"target environment\" as a new virtual dataset. Any updates made to this virtual dataset will not affect the original snapshot from which it was provisioned.  (diagram goes here)",
            "title": "Provisioning"
        },
        {
            "location": "/Your_First_Plugin/Overview/#parts-of-a-plugin",
            "text": "There are three main parts of a plugin. We'll cover them briefly here, and then fill in more details later in the tutorial.",
            "title": "Parts of a Plugin"
        },
        {
            "location": "/Your_First_Plugin/Overview/#manifest",
            "text": "The manifest is where the plugin describes itself to the Delphix Engine. What is the plugin called? What version of the plugin is this? What type(s) of environments does the plugin work with? What features does the plugin offer?...",
            "title": "Manifest"
        },
        {
            "location": "/Your_First_Plugin/Overview/#operationscode",
            "text": "The plugin will need to provide \"operations\". These are Python functions, each of which implement one small piece of functionality. This is how the plugin can customize Delphix behavior to work with a particular kind of dataset.\nOne operation will handle setting up a newly-configured virtual dataset. One operation will handle copying data from a source environment, and so on.  Later in this tutorial we will cover the specific examples we need for our first plugin. See (link to reference) for full details on which operations are available, which are required, what each one is required to do, etc.. Also see (link to advanced) for a more-full featured example which uses many more operations.",
            "title": "Operations/Code"
        },
        {
            "location": "/Your_First_Plugin/Overview/#datatype-definitions",
            "text": "As part of normal operation, a plugin will need to generate and access certain pieces of information in order to do its job. For example, a plugin that works with Postgres might need to know which port number to connect to, or which credentials to use, etc.  Different plugins will have vastly different needs for what information is required here, and the Delphix Engine needs to know the details. Therefore, a plugin can define its own datatypes, which it does by providing \"schemas\". We'll go into more detail on this later in the tutorial, and you can see full details here (link to reference).",
            "title": "Datatype Definitions"
        },
        {
            "location": "/Your_First_Plugin/Overview/#prerequisites",
            "text": "In order to complete the following tutorial, you'll need to make sure you have the following set up and ready to go:   You should have a SDK already downloaded and working, as described here (link)  You should have a Delphix Engine, version x.y.z or above.  You should have at least one, but preferably three, Unix hosts that can be added to the Delphix Engine as remote environments.  Some tool for editing text files (mostly Python and JSON). A simple text editor would work fine, or you can use a full-fledged IDE.",
            "title": "Prerequisites"
        },
        {
            "location": "/Your_First_Plugin/Discovery/",
            "text": "Discovery\n\u00b6\n\n\nWhat is Discovery?\n\u00b6\n\n\nIn order to ingest data from a source environment, the Delphix Engine first needs to learn information about that data: Where does it live? How can it be accessed? What is it called?\n\n\nDiscovery\n is the process by which the Delphix Engine learns about remote data. Discovery can be either \nautomatic\n (where the plugin finds the remote data on its own), or \nmanual\n (where the user tells us about the remote data). For our first plugin, we'll be using a mix of these two techniques.\n\n\nSource Configs and Repositories\n\u00b6\n\n\nWhat are Source Configs and Repositories?\n\u00b6\n\n\nA \nsource config\n is a collection of information that Delphix uses to represent a dataset. Different plugins will have different ideas about what a \"dataset\" is (an entire database? a set of config files? an application?), but for our first plugin, it is simply a directory tree on the filesystem of the remote environment.\n\n\nA \nrepository\n represents what you might call \"data dependencies\" -- anything installed on the remote host that the dataset depends on. For example, if you're working with a Postgres database, then your repository will represent an installation of a particular version of the Postgres DBMS. In this plugin, we don't have any special dependencies, except for the simple existence of the unix system on which the directory lives.\n\n\nDefining Your Data Formats\n\u00b6\n\n\nBecause each plugin will have different ideas about what a repository or source config represents, different plugins will have different sets of information that they will need to collect/store for each of these.\n\n\nDelphix needs to know the format of this information. How many pieces of information are collected? What are they called? Are they strings? Numbers?\n\n\nFor our first plugin, we don't need a lot of information here, but see (link to advanced example) for a more complicated example. We use no special information about our repositories (except some way for the user to identify them). For source configs, we just need to know the path to the directory from which we will be ingesting data.\n\n\nThe plugin needs to describe all of this to the Delphix Engine, and it does it using \nschemas\n.  To create the necessary schemas for our first plugin, (TODO: describe here where exactly these schemas go, when that is finalized\u2026 e.g. \"create a new file called 'repositorySchema.json' and add the following content)\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"repoName\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n},\n\n  \n\"required\"\n:\n \n[\n\"repoName\"\n],\n\n  \n\"nameField\"\n:\n \n\"repoName\"\n,\n\n  \n\"identityFields\"\n:\n \n[\n\"repoName\"\n]\n\n\n}\n\n\n\n\n\n\n(TODO: if possible, autogenerate this snippet from actual checked-in tested code instead of having it manually typed out here, so that we guarantee our doc code snippets never stop working)\n(TODO: change this if we decide to keep the \"name\" and \"identity\" bits outside of the schema)\n\n\nFor detailed information about exactly how repository schemas works, see (link to reference). In essence, we are here defining only a single string property which will serve to uniquely identify the repository and will serve as a user-visible name.\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n\n      \n\"type\"\n:\n \n\"string\"\n,\n\n      \n\"prettyName\"\n:\n \n\"Dataset Name\"\n,\n\n      \n\"description\"\n:\n \n\"User-visible name for this dataset\"\n\n    \n}\n\n    \n\"path\"\n:\n \n{\n\n      \n\"type\"\n:\n \n\"string\"\n,\n\n      \n\"format\"\n:\n \n\"unixpath\"\n,\n\n      \n\"prettyName\"\n:\n \n\"Path\"\n,\n\n      \n\"description\"\n:\n \n\"Full path to data location on the remote environment\"\n\n    \n}\n\n  \n},\n\n  \n\"required\"\n:\n \n[\n\"name\"\n,\n \n\"path\"\n],\n\n  \n\"additionalFields\"\n:\n \nfalse\n,\n\n  \n\"nameField\"\n:\n \n\"name\"\n,\n\n  \n\"identityFields\"\n:\n \n[\n\"path\"\n]\n\n\n}\n\n\n\n\n\n\nBecause we are going to be asking the user to fill in these fields, we've added some extra information in our schemas. Our values for \"prettyName\" and \"description\" will be show to the user in the UI. In addition, we've used the \"additionalFields\" flag to stop the user from (accidentally?) providing extra information that we don't need. Finally, note that we're using the special \nunixpath\n format specifier, which will allow the UI to enforce that a valid Unix path is entered by the user.\n\n\nPlease see (link to reference) for full details about what you can do in these schemas.\n\n\nImplementing Discovery in Your Plugin\n\u00b6\n\n\nAbout Python Code\n\u00b6\n\n\nAs described in the overview section, plugins customize the behavior of the Delphix Engine by providing Python code. Each customizable piece of behavior is called a \"plugin operation\". The plugin provides separate Python functions for each of the operations that it wants to customize.\n\n\nRight now, we're concerned with discovery. There are two customizable operations related to automatic discovery, one for repositories and one for source configs. In both cases, the job of the Python method is to automatically collect whatever information the schemas (see above) require, and to return that information to the Delphix Engine. The Delphix Engine will run these customized operations whenever a new environment is added, or when an existing environment is rediscovered.\n\n\nRepository Discovery\n\u00b6\n\n\nFor repositories, we'll need to write a \"repository discovery\" operation in Python. The job of this operation is to examine a remote environment, find any repositories, and return information about them to the Delphix Engine.\n\n\nIn our case, we know that each remote environment is going to have exactly one repository, and so our \"repository discovery\" operation can be very simple.\n\n\nCreate a file in your preferred editor (TODO: add suggested/required filename), and add the following Python code. (TODO: revisit when decorators are finalized)\n\n\n@delphix.repository_discovery\n\n\ndef\n \ndo_discovery_for_repositories\n(\nenvironment\n):\n\n    \nthe_repo\n \n=\n \nRepository\n()\n\n    \nthe_repo\n.\nname\n \n=\n \n\"Directory Tree\"\n\n\n    \nreturn\n \n[\n \nthe_repo\n \n]\n\n\n\n\n\n\n\n\nGotcha\n\n\nBe careful to always use consistent indentation in Python code!\n\n\n\n\nTaking this line-by-line, here's what's happening in our new method:\n\n\n@delphix.repository_discovery\n\n\n\n\n\n\nThis is a Python \ndecorator\n which signals to the Delphix Engine that we want to customize the behavior of repository discovery, and that this function is the one that provides that customization\n\n\ndef\n \ndo_discovery_for_repositories\n(\nenvironment\n):\n\n\n\n\n\n\nThis begins a Python function definition. We can call this function whatever we like, but generally you'll want to pick something descriptive. As with most operations, the Delphix Engine will provide input to the repository discovery operation. In this particular case, the Engine provides one input which describes the remote environment which we are describing. For more details about operation inputs, see (link to reference).\n\n\nthe_repo\n \n=\n \nRepository\n()\n\n\n\n\n\n\nHere, we are constructing a new Python object which will conform to the repository schema we created in the previous section. This Python type \nRepository\n is automatically created for you based on that schema, and so this python object will have properties to correspond to the properties defined in that schema.\n\n\nthe_repo\n.\nname\n \n=\n \n\"Directory Tree\"\n\n\n\n\n\n\nOur repository only has one property: a user-visible name. Here we want to choose some name that will let the user understand what kind of data this repository can work with. We'll just call it \"Directory Tree\".\n\n\nreturn\n \n[\n \nthe_repo\n \n]\n\n\n\n\n\n\nThe job of this operation is to return a list of all repositories on the remote environment. In our case, we want exactly one repository per environment. So, here we just make a one-item Python list, and return it to the Delphix Engine.\n\n\nSource Config Discovery\n\u00b6\n\n\nFor source configs, we'll rely solely on manual discovery -- the user will tell us which directories they want to ingest from. Because we are not using automatic source config discovery, we will not need to write a custom operation here. But, see (link to advanced example) for a plugin that does define its own discovery operation for source configs,\n\n\nTo declare that we want to allow manual source config discovery, we just need to specify that in our manifest (link to manifest page). To do this,\n(TODO: We have not finalized what the manifest/metadata/main.json will look like. When we do, add in details here about how to modify it to allow manual discovery)\n\n\nWhen the user tries to add a new source config, the Delphix Engine will generate a UI based on the contents of your source config schema. Thus, the user will be able to enter all the data that your plugin needs.\n\n\nHow to Run Discovery in the Delphix Engine\n\u00b6\n\n\nLet's try it out and make sure discovery works!\n\n\nFirst, follow the instructions here (link to SDK doc) to build your plugin and install it onto a Delphix Engine.\n\n\nOnce the plugin is installed, add a remote unix environment to your engine. To do this, go to \"Manage/Environments\", click \"add\", answer the questions, and submit. (If you already have an environment set up, you can just refresh it).\n\n\nTo keep an eye on this discovery process, you may need to open the \"operations\" tab on the UI. If any errors happen, they'll be reported here.\n\n\n(TODO: animated GIF here showing where to click on the UI?)\n\n\nAfter the automatic discovery process completes, go to the \"Databases\" tab. You should see an entry for \"Directory Tree\". That's the repository you created in your Python code.\n\n\nBecause you've specified that manual discovery of source configs is allowed, you should be able to click the \"Add Dataset\" button next to \"Directory Tree\". This should bring up a UI which will allow you to enter all the pieces of information that your plugin has specified in the schema.\n\n\n(TODO: animated GIF here showing manual discovery?)\n\n\n\n\nGotcha\n\n\nOnce you've manually created a source config, you will not be allowed to modify your plugin's source config schema. We'll cover how to deal with this correctly later, in the upgrade section. For now, if you need to change your plugin's source config schema, you'll have to first delete any source configs you've manually added.",
            "title": "Discovery"
        },
        {
            "location": "/Your_First_Plugin/Discovery/#discovery",
            "text": "",
            "title": "Discovery"
        },
        {
            "location": "/Your_First_Plugin/Discovery/#what-is-discovery",
            "text": "In order to ingest data from a source environment, the Delphix Engine first needs to learn information about that data: Where does it live? How can it be accessed? What is it called?  Discovery  is the process by which the Delphix Engine learns about remote data. Discovery can be either  automatic  (where the plugin finds the remote data on its own), or  manual  (where the user tells us about the remote data). For our first plugin, we'll be using a mix of these two techniques.",
            "title": "What is Discovery?"
        },
        {
            "location": "/Your_First_Plugin/Discovery/#source-configs-and-repositories",
            "text": "",
            "title": "Source Configs and Repositories"
        },
        {
            "location": "/Your_First_Plugin/Discovery/#what-are-source-configs-and-repositories",
            "text": "A  source config  is a collection of information that Delphix uses to represent a dataset. Different plugins will have different ideas about what a \"dataset\" is (an entire database? a set of config files? an application?), but for our first plugin, it is simply a directory tree on the filesystem of the remote environment.  A  repository  represents what you might call \"data dependencies\" -- anything installed on the remote host that the dataset depends on. For example, if you're working with a Postgres database, then your repository will represent an installation of a particular version of the Postgres DBMS. In this plugin, we don't have any special dependencies, except for the simple existence of the unix system on which the directory lives.",
            "title": "What are Source Configs and Repositories?"
        },
        {
            "location": "/Your_First_Plugin/Discovery/#defining-your-data-formats",
            "text": "Because each plugin will have different ideas about what a repository or source config represents, different plugins will have different sets of information that they will need to collect/store for each of these.  Delphix needs to know the format of this information. How many pieces of information are collected? What are they called? Are they strings? Numbers?  For our first plugin, we don't need a lot of information here, but see (link to advanced example) for a more complicated example. We use no special information about our repositories (except some way for the user to identify them). For source configs, we just need to know the path to the directory from which we will be ingesting data.  The plugin needs to describe all of this to the Delphix Engine, and it does it using  schemas .  To create the necessary schemas for our first plugin, (TODO: describe here where exactly these schemas go, when that is finalized\u2026 e.g. \"create a new file called 'repositorySchema.json' and add the following content)  { \n   \"type\" :   \"object\" , \n   \"properties\" :   { \n     \"repoName\" :   {   \"type\" :   \"string\"   } \n   }, \n   \"required\" :   [ \"repoName\" ], \n   \"nameField\" :   \"repoName\" , \n   \"identityFields\" :   [ \"repoName\" ]  }   (TODO: if possible, autogenerate this snippet from actual checked-in tested code instead of having it manually typed out here, so that we guarantee our doc code snippets never stop working)\n(TODO: change this if we decide to keep the \"name\" and \"identity\" bits outside of the schema)  For detailed information about exactly how repository schemas works, see (link to reference). In essence, we are here defining only a single string property which will serve to uniquely identify the repository and will serve as a user-visible name.  { \n   \"type\" :   \"object\" , \n   \"properties\" :   { \n     \"name\" :   { \n       \"type\" :   \"string\" , \n       \"prettyName\" :   \"Dataset Name\" , \n       \"description\" :   \"User-visible name for this dataset\" \n     } \n     \"path\" :   { \n       \"type\" :   \"string\" , \n       \"format\" :   \"unixpath\" , \n       \"prettyName\" :   \"Path\" , \n       \"description\" :   \"Full path to data location on the remote environment\" \n     } \n   }, \n   \"required\" :   [ \"name\" ,   \"path\" ], \n   \"additionalFields\" :   false , \n   \"nameField\" :   \"name\" , \n   \"identityFields\" :   [ \"path\" ]  }   Because we are going to be asking the user to fill in these fields, we've added some extra information in our schemas. Our values for \"prettyName\" and \"description\" will be show to the user in the UI. In addition, we've used the \"additionalFields\" flag to stop the user from (accidentally?) providing extra information that we don't need. Finally, note that we're using the special  unixpath  format specifier, which will allow the UI to enforce that a valid Unix path is entered by the user.  Please see (link to reference) for full details about what you can do in these schemas.",
            "title": "Defining Your Data Formats"
        },
        {
            "location": "/Your_First_Plugin/Discovery/#implementing-discovery-in-your-plugin",
            "text": "",
            "title": "Implementing Discovery in Your Plugin"
        },
        {
            "location": "/Your_First_Plugin/Discovery/#about-python-code",
            "text": "As described in the overview section, plugins customize the behavior of the Delphix Engine by providing Python code. Each customizable piece of behavior is called a \"plugin operation\". The plugin provides separate Python functions for each of the operations that it wants to customize.  Right now, we're concerned with discovery. There are two customizable operations related to automatic discovery, one for repositories and one for source configs. In both cases, the job of the Python method is to automatically collect whatever information the schemas (see above) require, and to return that information to the Delphix Engine. The Delphix Engine will run these customized operations whenever a new environment is added, or when an existing environment is rediscovered.",
            "title": "About Python Code"
        },
        {
            "location": "/Your_First_Plugin/Discovery/#repository-discovery",
            "text": "For repositories, we'll need to write a \"repository discovery\" operation in Python. The job of this operation is to examine a remote environment, find any repositories, and return information about them to the Delphix Engine.  In our case, we know that each remote environment is going to have exactly one repository, and so our \"repository discovery\" operation can be very simple.  Create a file in your preferred editor (TODO: add suggested/required filename), and add the following Python code. (TODO: revisit when decorators are finalized)  @delphix.repository_discovery  def   do_discovery_for_repositories ( environment ): \n     the_repo   =   Repository () \n     the_repo . name   =   \"Directory Tree\" \n\n     return   [   the_repo   ]    Gotcha  Be careful to always use consistent indentation in Python code!   Taking this line-by-line, here's what's happening in our new method:  @delphix.repository_discovery   This is a Python  decorator  which signals to the Delphix Engine that we want to customize the behavior of repository discovery, and that this function is the one that provides that customization  def   do_discovery_for_repositories ( environment ):   This begins a Python function definition. We can call this function whatever we like, but generally you'll want to pick something descriptive. As with most operations, the Delphix Engine will provide input to the repository discovery operation. In this particular case, the Engine provides one input which describes the remote environment which we are describing. For more details about operation inputs, see (link to reference).  the_repo   =   Repository ()   Here, we are constructing a new Python object which will conform to the repository schema we created in the previous section. This Python type  Repository  is automatically created for you based on that schema, and so this python object will have properties to correspond to the properties defined in that schema.  the_repo . name   =   \"Directory Tree\"   Our repository only has one property: a user-visible name. Here we want to choose some name that will let the user understand what kind of data this repository can work with. We'll just call it \"Directory Tree\".  return   [   the_repo   ]   The job of this operation is to return a list of all repositories on the remote environment. In our case, we want exactly one repository per environment. So, here we just make a one-item Python list, and return it to the Delphix Engine.",
            "title": "Repository Discovery"
        },
        {
            "location": "/Your_First_Plugin/Discovery/#source-config-discovery",
            "text": "For source configs, we'll rely solely on manual discovery -- the user will tell us which directories they want to ingest from. Because we are not using automatic source config discovery, we will not need to write a custom operation here. But, see (link to advanced example) for a plugin that does define its own discovery operation for source configs,  To declare that we want to allow manual source config discovery, we just need to specify that in our manifest (link to manifest page). To do this,\n(TODO: We have not finalized what the manifest/metadata/main.json will look like. When we do, add in details here about how to modify it to allow manual discovery)  When the user tries to add a new source config, the Delphix Engine will generate a UI based on the contents of your source config schema. Thus, the user will be able to enter all the data that your plugin needs.",
            "title": "Source Config Discovery"
        },
        {
            "location": "/Your_First_Plugin/Discovery/#how-to-run-discovery-in-the-delphix-engine",
            "text": "Let's try it out and make sure discovery works!  First, follow the instructions here (link to SDK doc) to build your plugin and install it onto a Delphix Engine.  Once the plugin is installed, add a remote unix environment to your engine. To do this, go to \"Manage/Environments\", click \"add\", answer the questions, and submit. (If you already have an environment set up, you can just refresh it).  To keep an eye on this discovery process, you may need to open the \"operations\" tab on the UI. If any errors happen, they'll be reported here.  (TODO: animated GIF here showing where to click on the UI?)  After the automatic discovery process completes, go to the \"Databases\" tab. You should see an entry for \"Directory Tree\". That's the repository you created in your Python code.  Because you've specified that manual discovery of source configs is allowed, you should be able to click the \"Add Dataset\" button next to \"Directory Tree\". This should bring up a UI which will allow you to enter all the pieces of information that your plugin has specified in the schema.  (TODO: animated GIF here showing manual discovery?)   Gotcha  Once you've manually created a source config, you will not be allowed to modify your plugin's source config schema. We'll cover how to deal with this correctly later, in the upgrade section. For now, if you need to change your plugin's source config schema, you'll have to first delete any source configs you've manually added.",
            "title": "How to Run Discovery in the Delphix Engine"
        },
        {
            "location": "/Your_First_Plugin/Data_Ingestion/",
            "text": "Data Ingestion\n\u00b6\n\n\nHow Does Delphix Ingest Data?\n\u00b6\n\n\nAs discussed in the \nprevious page\n, the Delphix Engine uses the \ndiscovery\n process to learn about datasets that live on a \nsource environment\n. This section describes how we ingest such a dataset into the Delphix Engine. It is a two-step process.\n\n\nLinking\n\u00b6\n\n\nThe first step is called \nlinking\n. This is simply the creation of a new dataset on the Delphix Engine, which is associated with the dataset on the source environment. This new linked dataset is called a \ndSource\n.\n\n\nSyncing\n\u00b6\n\n\nImmediately after linking, the new dSource is \nsynced\n. This is the process by which data from the source environment is copied onto the Delphix Engine. Subsequent syncs may then be periodically performed in order to keep the dSource up-to-date.\n\n\nThe details of how this copying is done will vary significantly from plugin to plugin. For example, some plugins will simply copy files from the filesystem. Other plugins might contact a DBMS and instruct it to send backup or replication streams. There are many possibilities here, but they all break down into two main strategies that the plugin author can choose from:\n\n\nWith the \ndirect\n strategy, the plugin is not in charge of the data copying. Instead the Delphix Engine will directly pull raw data from the source environment, using the Unix tool \nrsync\n (or \nrobocopy\n on Windows). The plugin merely provides the location of the data. This is a very simple strategy, and it is also quite limiting.\n\n\nFor our first plugin, we'll be using the more flexible \nstaging\n strategy. With this strategy, the Delphix Engine will use NFS (or CIFS on Windows) to mount storage onto a \nstaging environment\n. Our plugin will then be in full control of how to get data from the source environment onto this NFS mount.\n\n\n\n\nGotcha\n\n\nAlthough it is not common, it is entirely possible that the staging environment is the same thing as the source environment. Be careful not to assume otherwise in your plugins.\n\n\n\n\nFor more details about deciding between using a direct or a staging strategy, please see (link to best practices section).\n\n\nOur Syncing Strategy\n\u00b6\n\n\nFor our purposes here in this intro plugin, we'll use a simple strategy. We'll simply copy files from the filesystem on the source environment onto the NFS mount on the staging environment. We'll do this by running \nscp\n from our staging environment, and use user-provided credentials to connect to the source environment.\n\n\nFor simplicity's sake, we will not bother handling the case mentioned above where the staging environment is the same as the source environment\n\n\nDefining Your Linked Source Data Format\n\u00b6\n\n\nIn order to be able to successfully do the copying required, plugins might need to get some information from the user. In our case, we will need to connect from the staging environment to the source environment using the \nscp\n tool. This means we will need to know a username and password.\n\n\nAgain, we will be using a JSON schema to define the data format. The user will be presented with a UI that will let them provide all the information our schema specifies.\n\n\n(TODO: describe where to put this schema)\n\n\n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"additionalProperties\"\n:\n \nfalse\n,\n\n    \n\"required\"\n:\n \n[\n\"username\"\n,\n \n\"password\"\n],\n\n    \n\"properties\"\n \n{\n\n        \n\"username\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"string\"\n,\n\n            \n\"prettyName\"\n:\n \n\"Username on Source Host\"\n,\n\n            \n\"description\"\n:\n \n\"Username for making SSH connection to source host\"\n\n        \n},\n\n        \n\"password\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"string\"\n,\n\n            \n\"prettyName\"\n:\n \n\"Password on Source Host\"\n,\n\n            \n\"description\"\n:\n \n\"Password for making SSH connection to source host\"\n,\n\n            \n\"format\"\n:\n \n\"password\"\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nThere is one new thing to notice about this schema, as compared with our discovery schemas. The \npassword\n property is tagged as \n\"format\": \"password\"\n. This ensures that the Delphix Engine will take precautions like not displaying the value on-screen. For full details, see (link to reference section).\n\n\nWith this schema, the user will be required to provide a username and password as part of the linking process.\n\n\nImplementing Syncing in Your Plugin\n\u00b6\n\n\nAs explained here (link to reference flowchart), the Delphix Engine will always run the plugin's \npreSnapshot\n operation just before taking a snapshot of the dsource. That means our \npreSnapshot\n operation has to get the NFS share into the desired state. For us, that means that's the time to do our data copy.\n\n\nCreate a file in your preferred editor (TODO: add suggested/required filename), and add the following Python code. (TODO: revisit when decorators are finalized)\n\n\n@delphix.linked_pre_snapshot\n\n\ndef\n \ncopy_data_from_source\n(\nsource_environment\n,\n \nstaging_environment\n,\n \nsource_config\n,\n \nlinked_source\n,\n \nmount_location\n):\n\n    \nenvironment_variables\n \n=\n \n{\n \n\"PASSWORD\"\n:\n \nlinked_source\n.\npassword\n \n}\n\n    \nscp_data_location\n \n=\n \n\"\n%s\n@\n%s\n:\n%s\n\"\n.\nformat\n(\nlinked_source\n.\nusername\n,\n \nsource_environment\n.\nhostname\n,\n \nsource_config\n.\npath\n)\n\n    \nscp_command\n \n=\n \n\"echo $PASSWORD | scp -r \n%s\n \n%s\n\"\n.\nformat\n(\nscp_data_location\n,\n \nmount_location\n)\n\n\n    \nresult\n \n=\n \ndx\n.\nrun_bash\n(\nstaging_environment\n,\n \ncopy_command\n,\n \nenvironment_variables\n)\n\n\n    \nif\n \nresult\n.\nexit_code\n \n!=\n \n0\n:\n\n        \nraise\n \nValueError\n(\n\"Could not copy files. Please check username and password.\n\\n\n%s\n\"\n.\nformat\n(\nresult\n.\nstderr\n)\n\n\n\n\n\n\nTaking this line-by-line, here's what's happening in our new method:\n\n\n@delphix.linked_pre_snapshot\n\n\n\n\n\n\nThis \ndecorator\n tells the Delphix Engine that this code will define our \"pre-snapshot\" operation.\n\n\ndef\n \ncopy_data_from_source\n(\nsource_environment\n,\n \nstaging_environment\n,\n \nsource_config\n,\n \nlinked_source\n,\n \nmount_location\n):\n\n\n\n\n\n\nThis begins the Python function that implements our pre-snapshot operation. As compared with our discovery code, there are a lot more inputs coming in from the Delphix Engine.\n\n\n\n\nGotcha\n\n\nThe order of these input arguments matters. That is, the first input argument is always going to represent the source environment, the second the staging environment, and so on. It's highly recommended to use these same variable names to avoid confusion.\n\n\n\n\n    \nenvironment_variables\n \n=\n \n{\n \n\"PASSWORD\"\n:\n \nlinked_source\n.\npassword\n \n}\n\n    \nscp_data_location\n \n=\n \n\"\n%s\n@\n%s\n:\n%s\n\"\n.\nformat\n(\nlinked_source\n.\nusername\n,\n \nsource_environment\n.\nhostname\n,\n \nsource_config\n.\npath\n)\n\n    \nscp_command\n \n=\n \n\"echo $PASSWORD | scp -r \n%s\n \n%s\n\"\n.\nformat\n(\nscp_data_location\n,\n \nmount_location\n)\n\n\n\n\n\n\nHere, we are preparing data to help us run a Bash command on the staging host. First, we set up a Python dictionary that represents the environment variables we want. Next, we construct a Python string that represents the actual command we want to run.\n\n\nNote that we are using the \nscp\n tool to copy files from the source environment onto the NFS share that is mounted to the staging environment.\n\n\n    \nresult\n \n=\n \ndx\n.\nrun_bash\n(\nstaging_environment\n,\n \ncopy_command\n,\n \nenvironment_variables\n)\n\n    \nif\n \nresult\n.\nexit_code\n \n!=\n \n0\n:\n\n        \nraise\n \nValueError\n(\n\"Could not copy files. Please check username and password.\n\\n\n%s\n\"\n.\nformat\n(\nresult\n.\nstderr\n)\n\n\n\n\n\n\nThis code actually runs the Bash command. The function \nrun_bash\n that we're calling here is a called a \ncallback\n. Plugins use callbacks to request that the Delphix Engine do work on the plugin's behalf. In our case, we are telling the Delphix Engine to run our bash command on the staging environment.\n\n\nWe also check that the command succeeded. If not, we will raise an error that explains the situation to the user.\n\n\nFor full details on the \nrun_bash\n callback, and on callbacks in general, please see (link to reference).\n\n\nHow to Link and Sync in the Delphix Engine\n\u00b6\n\n\nLet's try it out and make sure this works!\n\n\nYou should already have a respository and source config set up from the previous page.\n\n\nNext, you should set up a separate staging environment. (TODO: add instructions here)\n\n\nGo to \"Manage/Environments\", select your \nsource\n environment, and then go to the \"Databases\" tab. Find your \"Directory Tree\" repository, and your source config underneath it.\n\n\nClick \"Add dSource\" on your source config. This will begin the linking process.\n\n\nYou should be presented with a UI in which you will have to specify which environment you want to use for staging. You will also be asked to provide the username and password needed to connect to the source environment.\n\n\nAfter you have finished entering this information, the initial sync process will begin. This is what will call your pre-snapshot operation, thus copying data.\n\n\n\n\nGotcha\n\n\nOnce you've manually created a dsource, you will not be allowed to modify your plugin's linked source schema. We'll cover how to deal with this correctly later, in the upgrade section. For now, if you need to change your plugin's linked source schema, you'll have to first delete any dsources you've manually added.",
            "title": "Data Ingestion"
        },
        {
            "location": "/Your_First_Plugin/Data_Ingestion/#data-ingestion",
            "text": "",
            "title": "Data Ingestion"
        },
        {
            "location": "/Your_First_Plugin/Data_Ingestion/#how-does-delphix-ingest-data",
            "text": "As discussed in the  previous page , the Delphix Engine uses the  discovery  process to learn about datasets that live on a  source environment . This section describes how we ingest such a dataset into the Delphix Engine. It is a two-step process.",
            "title": "How Does Delphix Ingest Data?"
        },
        {
            "location": "/Your_First_Plugin/Data_Ingestion/#linking",
            "text": "The first step is called  linking . This is simply the creation of a new dataset on the Delphix Engine, which is associated with the dataset on the source environment. This new linked dataset is called a  dSource .",
            "title": "Linking"
        },
        {
            "location": "/Your_First_Plugin/Data_Ingestion/#syncing",
            "text": "Immediately after linking, the new dSource is  synced . This is the process by which data from the source environment is copied onto the Delphix Engine. Subsequent syncs may then be periodically performed in order to keep the dSource up-to-date.  The details of how this copying is done will vary significantly from plugin to plugin. For example, some plugins will simply copy files from the filesystem. Other plugins might contact a DBMS and instruct it to send backup or replication streams. There are many possibilities here, but they all break down into two main strategies that the plugin author can choose from:  With the  direct  strategy, the plugin is not in charge of the data copying. Instead the Delphix Engine will directly pull raw data from the source environment, using the Unix tool  rsync  (or  robocopy  on Windows). The plugin merely provides the location of the data. This is a very simple strategy, and it is also quite limiting.  For our first plugin, we'll be using the more flexible  staging  strategy. With this strategy, the Delphix Engine will use NFS (or CIFS on Windows) to mount storage onto a  staging environment . Our plugin will then be in full control of how to get data from the source environment onto this NFS mount.   Gotcha  Although it is not common, it is entirely possible that the staging environment is the same thing as the source environment. Be careful not to assume otherwise in your plugins.   For more details about deciding between using a direct or a staging strategy, please see (link to best practices section).",
            "title": "Syncing"
        },
        {
            "location": "/Your_First_Plugin/Data_Ingestion/#our-syncing-strategy",
            "text": "For our purposes here in this intro plugin, we'll use a simple strategy. We'll simply copy files from the filesystem on the source environment onto the NFS mount on the staging environment. We'll do this by running  scp  from our staging environment, and use user-provided credentials to connect to the source environment.  For simplicity's sake, we will not bother handling the case mentioned above where the staging environment is the same as the source environment",
            "title": "Our Syncing Strategy"
        },
        {
            "location": "/Your_First_Plugin/Data_Ingestion/#defining-your-linked-source-data-format",
            "text": "In order to be able to successfully do the copying required, plugins might need to get some information from the user. In our case, we will need to connect from the staging environment to the source environment using the  scp  tool. This means we will need to know a username and password.  Again, we will be using a JSON schema to define the data format. The user will be presented with a UI that will let them provide all the information our schema specifies.  (TODO: describe where to put this schema)  { \n     \"type\" :   \"object\" , \n     \"additionalProperties\" :   false , \n     \"required\" :   [ \"username\" ,   \"password\" ], \n     \"properties\"   { \n         \"username\" :   { \n             \"type\" :   \"string\" , \n             \"prettyName\" :   \"Username on Source Host\" , \n             \"description\" :   \"Username for making SSH connection to source host\" \n         }, \n         \"password\" :   { \n             \"type\" :   \"string\" , \n             \"prettyName\" :   \"Password on Source Host\" , \n             \"description\" :   \"Password for making SSH connection to source host\" , \n             \"format\" :   \"password\" \n         } \n     }  }   There is one new thing to notice about this schema, as compared with our discovery schemas. The  password  property is tagged as  \"format\": \"password\" . This ensures that the Delphix Engine will take precautions like not displaying the value on-screen. For full details, see (link to reference section).  With this schema, the user will be required to provide a username and password as part of the linking process.",
            "title": "Defining Your Linked Source Data Format"
        },
        {
            "location": "/Your_First_Plugin/Data_Ingestion/#implementing-syncing-in-your-plugin",
            "text": "As explained here (link to reference flowchart), the Delphix Engine will always run the plugin's  preSnapshot  operation just before taking a snapshot of the dsource. That means our  preSnapshot  operation has to get the NFS share into the desired state. For us, that means that's the time to do our data copy.  Create a file in your preferred editor (TODO: add suggested/required filename), and add the following Python code. (TODO: revisit when decorators are finalized)  @delphix.linked_pre_snapshot  def   copy_data_from_source ( source_environment ,   staging_environment ,   source_config ,   linked_source ,   mount_location ): \n     environment_variables   =   {   \"PASSWORD\" :   linked_source . password   } \n     scp_data_location   =   \" %s @ %s : %s \" . format ( linked_source . username ,   source_environment . hostname ,   source_config . path ) \n     scp_command   =   \"echo $PASSWORD | scp -r  %s   %s \" . format ( scp_data_location ,   mount_location ) \n\n     result   =   dx . run_bash ( staging_environment ,   copy_command ,   environment_variables ) \n\n     if   result . exit_code   !=   0 : \n         raise   ValueError ( \"Could not copy files. Please check username and password. \\n %s \" . format ( result . stderr )   Taking this line-by-line, here's what's happening in our new method:  @delphix.linked_pre_snapshot   This  decorator  tells the Delphix Engine that this code will define our \"pre-snapshot\" operation.  def   copy_data_from_source ( source_environment ,   staging_environment ,   source_config ,   linked_source ,   mount_location ):   This begins the Python function that implements our pre-snapshot operation. As compared with our discovery code, there are a lot more inputs coming in from the Delphix Engine.   Gotcha  The order of these input arguments matters. That is, the first input argument is always going to represent the source environment, the second the staging environment, and so on. It's highly recommended to use these same variable names to avoid confusion.        environment_variables   =   {   \"PASSWORD\" :   linked_source . password   } \n     scp_data_location   =   \" %s @ %s : %s \" . format ( linked_source . username ,   source_environment . hostname ,   source_config . path ) \n     scp_command   =   \"echo $PASSWORD | scp -r  %s   %s \" . format ( scp_data_location ,   mount_location )   Here, we are preparing data to help us run a Bash command on the staging host. First, we set up a Python dictionary that represents the environment variables we want. Next, we construct a Python string that represents the actual command we want to run.  Note that we are using the  scp  tool to copy files from the source environment onto the NFS share that is mounted to the staging environment.       result   =   dx . run_bash ( staging_environment ,   copy_command ,   environment_variables ) \n     if   result . exit_code   !=   0 : \n         raise   ValueError ( \"Could not copy files. Please check username and password. \\n %s \" . format ( result . stderr )   This code actually runs the Bash command. The function  run_bash  that we're calling here is a called a  callback . Plugins use callbacks to request that the Delphix Engine do work on the plugin's behalf. In our case, we are telling the Delphix Engine to run our bash command on the staging environment.  We also check that the command succeeded. If not, we will raise an error that explains the situation to the user.  For full details on the  run_bash  callback, and on callbacks in general, please see (link to reference).",
            "title": "Implementing Syncing in Your Plugin"
        },
        {
            "location": "/Your_First_Plugin/Data_Ingestion/#how-to-link-and-sync-in-the-delphix-engine",
            "text": "Let's try it out and make sure this works!  You should already have a respository and source config set up from the previous page.  Next, you should set up a separate staging environment. (TODO: add instructions here)  Go to \"Manage/Environments\", select your  source  environment, and then go to the \"Databases\" tab. Find your \"Directory Tree\" repository, and your source config underneath it.  Click \"Add dSource\" on your source config. This will begin the linking process.  You should be presented with a UI in which you will have to specify which environment you want to use for staging. You will also be asked to provide the username and password needed to connect to the source environment.  After you have finished entering this information, the initial sync process will begin. This is what will call your pre-snapshot operation, thus copying data.   Gotcha  Once you've manually created a dsource, you will not be allowed to modify your plugin's linked source schema. We'll cover how to deal with this correctly later, in the upgrade section. For now, if you need to change your plugin's linked source schema, you'll have to first delete any dsources you've manually added.",
            "title": "How to Link and Sync in the Delphix Engine"
        },
        {
            "location": "/Your_First_Plugin/Provisioning/",
            "text": "Provisioning\n\u00b6\n\n\nWhat is Provisioning?\n\u00b6\n\n\nOnce Delphix has a \nsnapshot\n of a dataset (for example of a dSource), it is possible to quickly clone that snapshot to create a new \nvirtual dataset\n. This new virtual dataset will be made available for use on a \ntarget environment\n. This process is called \nprovisioning\n.\n\n\nOur Provisioning Strategy\n\u00b6\n\n\nFor many plugins, there is a lot of work that needs to be done before a newly-provisioned virtual dataset can be made useful. For example, it might need to be registered with a running DBMS. Or, maybe some data inside the dataset needs to be changed so it behaves properly on the target environment.\n\n\nIn our case, however, there is very little to do. All we really require is that the files in the virtual dataset are accessible at some path on the target environment. Since the Delphix Engine takes care of mounting the data, we only need to worry about controlling \nwhere\n that data is mounted.\n\n\nDefining our Provision-Related Data Formats\n\u00b6\n\n\nWe've already seen three custom data formats: for repositories, source configs, and linked sources. There are two more customizable formats: for \nvirtual sources\n and \nsnapshots\n.\n\n\nSnapshots\n\u00b6\n\n\nFirst, let's consider snapshots. Plugins can store whatever information they want alongside each snapshot. This information can then be used later on when the user provisions a new virtual dataset from this snapshot. In our case, though, we have no need for any snapshot-specific information. So, our schema will be as simple as possible:\n\n\n{\n\n    \n\"type\"\n:\n \n\"object\"\n\n\n}\n\n\n\n\n\n\nNote that, because we plan to store no snapshot-specific data, we have defined no properties for our snapshot. Because snapshot-related data is only generated by the plugin, we don't have to worry about the other user-focussed schema properties we've seen before, like \nadditionalProperties\n and \ndescription\n.\n\n\nVirtual Source\n\u00b6\n\n\nFor our \nvirtual source\n, the only piece of data we really need is \"Where should this dataset live on the target environment?\". So, we can use a schema like this:\n\n\n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"required\"\n:\n \n\"mountPath\"\n,\n\n    \n\"additionalProperties\"\n:\n \nfalse\n,\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"mountPath\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"string\"\n,\n\n            \n\"format\"\n:\n \n\"unixpath\"\n,\n\n            \n\"prettyName\"\n:\n \n\"Mount Path\"\n,\n\n            \n\"description\"\n:\n \n\"Full path to where the VDB's data will be mounted on the target\"\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nBecause this data is entered by the user, again we are being careful to disallow \nadditionalProperties\n, to provide \nprettyName\n and \ndescription\n, and to specify that we want a valid Unix path.\n\n\nImplementing Provisioning\n\u00b6\n\n\nThere are numerous ways for a plugin to customize the provisioning process. For full details see (link to reference).\n\n\nFor our simple plugin, we just need to do two things:\n\n\n\n\nTell Delphix where to mount the virtual dataset\n\n\nCreate a \nsourceConfig\n to represent each newly-provisioned virtual dataset.\n\n\n\n\nControlling Mounting\n\u00b6\n\n\nIn order to tell Delphix where we want our dataset mounted, we provide something called a \nMount Spec\n. To do this, we provide a customized plugin operation.\n\n\n@delphix.mount_spec\n\n\ndef\n \nget_mount_spec\n(\nvirtual_source\n,\n \nsnapshot\n):\n\n    \nspec\n \n=\n \nMountSpec\n()\n\n    \nspec\n.\nprimary_mount\n \n=\n \nvirtual_source\n.\nmountPath\n\n    \nreturn\n \nspec\n\n\n\n\n\n\nAs we've seen in previous examples, we have a \ndecorator\n to tell Delphix that this is the function to call to get a mount spec.\n\n\nIn our case, the user has provided their desired mount location to us in the \nvirtual_source\n object. All we need to do is to create a \nMountSpec\n object, and set the \nprimary_mount\n accordingly.\n\n\nThis is perhaps the simplest possible mount spec operation. To see what other options are available to plugins here, please see (link to reference).\n\n\nCreating the Source Config\n\u00b6\n\n\nJust like we saw earlier with \nlinked datasets\n, each virtual dataset will need its own source config so that the Delphix Engine can interact with it. Our plugin is in charge of creating that source config.\n\n\nAs a reminder, here is what our schema looks like for source configs:\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n\n      \n\"type\"\n:\n \n\"string\"\n,\n\n      \n\"prettyName\"\n:\n \n\"Dataset Name\"\n,\n\n      \n\"description\"\n:\n \n\"User-visible name for this dataset\"\n\n    \n}\n\n    \n\"path\"\n:\n \n{\n\n      \n\"type\"\n:\n \n\"string\"\n,\n\n      \n\"format\"\n:\n \n\"unixpath\"\n,\n\n      \n\"prettyName\"\n:\n \n\"Path\"\n,\n\n      \n\"description\"\n:\n \n\"Full path to data location on the remote environment\"\n\n    \n}\n\n  \n},\n\n  \n\"required\"\n:\n \n[\n\"name\"\n,\n \n\"path\"\n],\n\n  \n\"additionalFields\"\n:\n \nfalse\n,\n\n  \n\"nameField\"\n:\n \n\"name\"\n,\n\n  \n\"identityFields\"\n:\n \n[\n\"path\"\n]\n\n\n}\n\n\n\n\n\n\nThat is, for each newly-cloned virtual dataset, we will have to create a new source config object with a name and a path. This is done by the \"configure\" plugin operation.\n\n\nIn general, the job of the configure operation is to get the newly-cloned dataset ready for use on the target environment, and to return a new source config representing the new dataset. For our simple plugin, the dataset does not require any setup work, and so we only have to worry about the source config.\n\n\n@delphix.configure\n\n\ndef\n \nconfigure_new_vdb\n(\nvirtual_source\n):\n\n    \nsourceConfig\n \n=\n \nSourceConfig\n();\n\n\n    \n# For our VDBs, the \"path\" is just the same thing as the location of the NFS mount\n\n    \nsourceConfig\n.\npath\n \n=\n \nvirtual_source\n.\nmountPath\n\n\n    \n# A user-readable name for this dataset\n\n    \nsourceConfig\n.\nname\n \n=\n \n\"Directory tree at \n%s\n\"\n.\nformat\n(\nvirtual_source\n.\nmountPath\n)\n\n\n    \nreturn\n \nsourceConfig\n\n\n\n\n\n\nFor more details about \nconfigure\n, please see (link to reference).\n\n\nHow To Provision in the Delphix Engine\n\u00b6\n\n\nFinally, let's try it out to make sure provisioning works!\n\n\nGo to \"Manage/Datasets\", and select the dSource you created in the last page. You should see at least one snapshot, and maybe more than one if you have manually taken a snapshot, or if you have a snapshot policy in place. Select one of these snapshots and click the \"provision\" button.\n\n\nThis will walk you through a wizard, during which you'll be asked for some information. One of the things you'll have to do is to provide the data required by your virtual source schema. In our case, that means you'll be asked to provide a value for \nmountPath\n.\n\n\nYou will also be asked to choose a target environment on which the new VDB will live.\n\n\nAfter the wizard finishes, you will see a job appear on the right-hand side of the screen. When that job completes, your new VDB should be ready.\n\n\nTo make sure everything has worked correctly, log into to your target environment. From there, you can examine the directory you specified as the \nmountPath\n. What you should see is a copy of the directory that you linked to with your dSource.",
            "title": "Provisioning"
        },
        {
            "location": "/Your_First_Plugin/Provisioning/#provisioning",
            "text": "",
            "title": "Provisioning"
        },
        {
            "location": "/Your_First_Plugin/Provisioning/#what-is-provisioning",
            "text": "Once Delphix has a  snapshot  of a dataset (for example of a dSource), it is possible to quickly clone that snapshot to create a new  virtual dataset . This new virtual dataset will be made available for use on a  target environment . This process is called  provisioning .",
            "title": "What is Provisioning?"
        },
        {
            "location": "/Your_First_Plugin/Provisioning/#our-provisioning-strategy",
            "text": "For many plugins, there is a lot of work that needs to be done before a newly-provisioned virtual dataset can be made useful. For example, it might need to be registered with a running DBMS. Or, maybe some data inside the dataset needs to be changed so it behaves properly on the target environment.  In our case, however, there is very little to do. All we really require is that the files in the virtual dataset are accessible at some path on the target environment. Since the Delphix Engine takes care of mounting the data, we only need to worry about controlling  where  that data is mounted.",
            "title": "Our Provisioning Strategy"
        },
        {
            "location": "/Your_First_Plugin/Provisioning/#defining-our-provision-related-data-formats",
            "text": "We've already seen three custom data formats: for repositories, source configs, and linked sources. There are two more customizable formats: for  virtual sources  and  snapshots .",
            "title": "Defining our Provision-Related Data Formats"
        },
        {
            "location": "/Your_First_Plugin/Provisioning/#snapshots",
            "text": "First, let's consider snapshots. Plugins can store whatever information they want alongside each snapshot. This information can then be used later on when the user provisions a new virtual dataset from this snapshot. In our case, though, we have no need for any snapshot-specific information. So, our schema will be as simple as possible:  { \n     \"type\" :   \"object\"  }   Note that, because we plan to store no snapshot-specific data, we have defined no properties for our snapshot. Because snapshot-related data is only generated by the plugin, we don't have to worry about the other user-focussed schema properties we've seen before, like  additionalProperties  and  description .",
            "title": "Snapshots"
        },
        {
            "location": "/Your_First_Plugin/Provisioning/#virtual-source",
            "text": "For our  virtual source , the only piece of data we really need is \"Where should this dataset live on the target environment?\". So, we can use a schema like this:  { \n     \"type\" :   \"object\" , \n     \"required\" :   \"mountPath\" , \n     \"additionalProperties\" :   false , \n     \"properties\" :   { \n         \"mountPath\" :   { \n             \"type\" :   \"string\" , \n             \"format\" :   \"unixpath\" , \n             \"prettyName\" :   \"Mount Path\" , \n             \"description\" :   \"Full path to where the VDB's data will be mounted on the target\" \n         } \n     }  }   Because this data is entered by the user, again we are being careful to disallow  additionalProperties , to provide  prettyName  and  description , and to specify that we want a valid Unix path.",
            "title": "Virtual Source"
        },
        {
            "location": "/Your_First_Plugin/Provisioning/#implementing-provisioning",
            "text": "There are numerous ways for a plugin to customize the provisioning process. For full details see (link to reference).  For our simple plugin, we just need to do two things:   Tell Delphix where to mount the virtual dataset  Create a  sourceConfig  to represent each newly-provisioned virtual dataset.",
            "title": "Implementing Provisioning"
        },
        {
            "location": "/Your_First_Plugin/Provisioning/#controlling-mounting",
            "text": "In order to tell Delphix where we want our dataset mounted, we provide something called a  Mount Spec . To do this, we provide a customized plugin operation.  @delphix.mount_spec  def   get_mount_spec ( virtual_source ,   snapshot ): \n     spec   =   MountSpec () \n     spec . primary_mount   =   virtual_source . mountPath \n     return   spec   As we've seen in previous examples, we have a  decorator  to tell Delphix that this is the function to call to get a mount spec.  In our case, the user has provided their desired mount location to us in the  virtual_source  object. All we need to do is to create a  MountSpec  object, and set the  primary_mount  accordingly.  This is perhaps the simplest possible mount spec operation. To see what other options are available to plugins here, please see (link to reference).",
            "title": "Controlling Mounting"
        },
        {
            "location": "/Your_First_Plugin/Provisioning/#creating-the-source-config",
            "text": "Just like we saw earlier with  linked datasets , each virtual dataset will need its own source config so that the Delphix Engine can interact with it. Our plugin is in charge of creating that source config.  As a reminder, here is what our schema looks like for source configs:  { \n   \"type\" :   \"object\" , \n   \"properties\" :   { \n     \"name\" :   { \n       \"type\" :   \"string\" , \n       \"prettyName\" :   \"Dataset Name\" , \n       \"description\" :   \"User-visible name for this dataset\" \n     } \n     \"path\" :   { \n       \"type\" :   \"string\" , \n       \"format\" :   \"unixpath\" , \n       \"prettyName\" :   \"Path\" , \n       \"description\" :   \"Full path to data location on the remote environment\" \n     } \n   }, \n   \"required\" :   [ \"name\" ,   \"path\" ], \n   \"additionalFields\" :   false , \n   \"nameField\" :   \"name\" , \n   \"identityFields\" :   [ \"path\" ]  }   That is, for each newly-cloned virtual dataset, we will have to create a new source config object with a name and a path. This is done by the \"configure\" plugin operation.  In general, the job of the configure operation is to get the newly-cloned dataset ready for use on the target environment, and to return a new source config representing the new dataset. For our simple plugin, the dataset does not require any setup work, and so we only have to worry about the source config.  @delphix.configure  def   configure_new_vdb ( virtual_source ): \n     sourceConfig   =   SourceConfig (); \n\n     # For our VDBs, the \"path\" is just the same thing as the location of the NFS mount \n     sourceConfig . path   =   virtual_source . mountPath \n\n     # A user-readable name for this dataset \n     sourceConfig . name   =   \"Directory tree at  %s \" . format ( virtual_source . mountPath ) \n\n     return   sourceConfig   For more details about  configure , please see (link to reference).",
            "title": "Creating the Source Config"
        },
        {
            "location": "/Your_First_Plugin/Provisioning/#how-to-provision-in-the-delphix-engine",
            "text": "Finally, let's try it out to make sure provisioning works!  Go to \"Manage/Datasets\", and select the dSource you created in the last page. You should see at least one snapshot, and maybe more than one if you have manually taken a snapshot, or if you have a snapshot policy in place. Select one of these snapshots and click the \"provision\" button.  This will walk you through a wizard, during which you'll be asked for some information. One of the things you'll have to do is to provide the data required by your virtual source schema. In our case, that means you'll be asked to provide a value for  mountPath .  You will also be asked to choose a target environment on which the new VDB will live.  After the wizard finishes, you will see a job appear on the right-hand side of the screen. When that job completes, your new VDB should be ready.  To make sure everything has worked correctly, log into to your target environment. From there, you can examine the directory you specified as the  mountPath . What you should see is a copy of the directory that you linked to with your dSource.",
            "title": "How To Provision in the Delphix Engine"
        },
        {
            "location": "/References/Decorators/",
            "text": "Decorators\n\u00b6\n\n\nThe Virtualization SDK exposes decorators to be able to annotate functions that correspond to each \nPlugin Operation\n. \nIn the example below, it first creates a \nplugin\n object by invoking \nplatform.plugin()\n, that can then be used to tag plugin operations.\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nplatform\n\n\n\n//\n \nInitialize\n \na\n \nplugin\n \nobject\n\n\nplugin\n \n=\n \nplatform\n.\nplugin\n()\n\n\n\n@plugin.virtual_source.start\n()\n\n\ndef\n \nstart\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \nprint\n \n\"running start\"\n \n\n\n\n\n\n\n\nNote\n\n\nDecorators exposed by the Virtualization SDK are inherently python function calls and needs parentheses \n()\n appended at the end.\n\n\n\n\nAssuming the name of the object, is \nplugin\n as above, the table below lists the corresponding decorators for each plugin operation.\n\n\n\n\n\n\n\n\nPlugin Operation\n\n\nDecorator\n\n\n\n\n\n\n\n\n\n\nSourceConfig Discovey\n\n\n@plugin.discovery.source_config()\n\n\n\n\n\n\nRepository Discovey\n\n\n@plugin.discovery.repository()\n\n\n\n\n\n\nDirect Pre-Snapshot\n\n\n@plugin.direct.pre_snapshot()\n\n\n\n\n\n\nDirect Post-Snapshot\n\n\n@plugin.direct.post_snapshot()\n\n\n\n\n\n\nStaged Pre-Snapshot\n\n\n@plugin.staged.pre_snapshot()\n\n\n\n\n\n\nStaged Post-Snapshot\n\n\n@plugin.staged.post_snapshot()\n\n\n\n\n\n\nStaged Start-Staging\n\n\n@plugin.staged.start_staging()\n\n\n\n\n\n\nStaged Stop-Staging\n\n\n@plugin.staged.stop_staging()\n\n\n\n\n\n\nStaged Status\n\n\n@plugin.staged.status()\n\n\n\n\n\n\nStaged Worker\n\n\n@plugin.staged.worker()\n\n\n\n\n\n\nStaged MountSpecification\n\n\n@plugin.staged.mount_specification()\n\n\n\n\n\n\nVirtualSource Configure\n\n\n@plugin.virtual.configure()\n\n\n\n\n\n\nVirtualSource Unconfigure\n\n\n@plugin.virtual.unconfigure()\n\n\n\n\n\n\nVirtualSource Reconfigure\n\n\n@plugin.virtual.reconfigure()\n\n\n\n\n\n\nVirtualSource Start\n\n\n@plugin.virtual.start()\n\n\n\n\n\n\nVirtualSource Stop\n\n\n@plugin.virtual.stop()\n\n\n\n\n\n\nVirtualSource Pre-Snapshot\n\n\n@plugin.virtual.pre_snapshot()\n\n\n\n\n\n\nVirtualSource Post-Snapshot\n\n\n@plugin.virtual.post_snapshot()\n\n\n\n\n\n\nVirtualSource MountSpecification\n\n\n@plugin.virtual.mount_specification()\n\n\n\n\n\n\nVirtualSource Status\n\n\n@plugin.virtual.status()\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nA plugin should only implement the \ndirect\n operations or the \nstaged\n operations based on the \nplugin type",
            "title": "Decorators"
        },
        {
            "location": "/References/Decorators/#decorators",
            "text": "The Virtualization SDK exposes decorators to be able to annotate functions that correspond to each  Plugin Operation . \nIn the example below, it first creates a  plugin  object by invoking  platform.plugin() , that can then be used to tag plugin operations.  from   dlpx.virtualization   import   platform  //   Initialize   a   plugin   object  plugin   =   platform . plugin ()  @plugin.virtual_source.start ()  def   start ( virtual_source ,   repository ,   source_config ): \n   print   \"running start\"     Note  Decorators exposed by the Virtualization SDK are inherently python function calls and needs parentheses  ()  appended at the end.   Assuming the name of the object, is  plugin  as above, the table below lists the corresponding decorators for each plugin operation.     Plugin Operation  Decorator      SourceConfig Discovey  @plugin.discovery.source_config()    Repository Discovey  @plugin.discovery.repository()    Direct Pre-Snapshot  @plugin.direct.pre_snapshot()    Direct Post-Snapshot  @plugin.direct.post_snapshot()    Staged Pre-Snapshot  @plugin.staged.pre_snapshot()    Staged Post-Snapshot  @plugin.staged.post_snapshot()    Staged Start-Staging  @plugin.staged.start_staging()    Staged Stop-Staging  @plugin.staged.stop_staging()    Staged Status  @plugin.staged.status()    Staged Worker  @plugin.staged.worker()    Staged MountSpecification  @plugin.staged.mount_specification()    VirtualSource Configure  @plugin.virtual.configure()    VirtualSource Unconfigure  @plugin.virtual.unconfigure()    VirtualSource Reconfigure  @plugin.virtual.reconfigure()    VirtualSource Start  @plugin.virtual.start()    VirtualSource Stop  @plugin.virtual.stop()    VirtualSource Pre-Snapshot  @plugin.virtual.pre_snapshot()    VirtualSource Post-Snapshot  @plugin.virtual.post_snapshot()    VirtualSource MountSpecification  @plugin.virtual.mount_specification()    VirtualSource Status  @plugin.virtual.status()      Note  A plugin should only implement the  direct  operations or the  staged  operations based on the  plugin type",
            "title": "Decorators"
        },
        {
            "location": "/References/Plugin_Operations/",
            "text": "Plugin Operations\n\u00b6\n\n\nRepository Discovery\n\u00b6\n\n\nDiscovers the set of \nrepositories\n for a plugin on an \nenvironment\n. For a DBMS, this can correspond to the set of binaries installed on a Unix host.\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nplatform\n\n\nfrom\n \ndlpx.virtualization.generated\n \nimport\n \nRepositoryDefinition\n\n\n\nplugin\n \n=\n \nplatform\n.\nplugin\n()\n\n\n\n@plugin.discovery.repository\n()\n\n\ndef\n \nrepository_discovery\n(\nsource_connection\n):\n  \n  \nrepository\n \n=\n \nRepositoryDefinition\n()\n\n  \nrepository\n.\ninstallPath\n \n=\n \n\"/usr/bin/install\"\n\n  \nrepository\n.\nversion\n \n=\n \n\"1.2.3\"\n\n  \nreturn\n \n[\nrepository\n]\n\n\n\n\n\n\n\n\nThe above command assumes a \nRepository Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"installPath\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"version\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"installPath\"\n],\n\n  \n\"nameField\"\n:\n \n[\n\"installPath\"\n]\n    \n\n}\n\n\n\n\n\n\nSignature\n\u00b6\n\n\ndef repository_discovery(source_connection)\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsource_connection\n\n\nInstance of the \nSourceConnection\n class.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nAn array of \nRepositoryDefinition\n objects.\n\n\nSource Config Discovery\n\u00b6\n\n\nDiscovers the set of \nsource configs\n for a plugin for a \nrepository\n. For a DBMS, this can correspond to the set of unique databases running using a particular installation on a Unix host.\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nplatform\n\n\nfrom\n \ndlpx.virtualization.generated\n \nimport\n \nSourceConfigDefinition\n\n\n\nplugin\n \n=\n \nplatform\n.\nplugin\n()\n\n\n\n@plugin.discovery.source_config\n()\n\n\ndef\n \nsource_config_discovery\n(\nsource_connection\n,\n \nrepository\n):\n\n  \nsource_config\n \n=\n \nSourceConfigDefinition\n()\n\n  \nsource_config\n.\nname\n \n=\n \n\"my_name\"\n\n  \nsource_config\n.\nport\n \n=\n \n10000\n\n  \nreturn\n \n[\nsource_config\n]\n\n\n\n\n\n\n\n\nThe above command assumes a \nSource Config Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"port\"\n:\n \n{\n \n\"type\"\n:\n \n\"number\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"name\"\n],\n\n  \n\"nameField\"\n:\n \n[\n\"name\"\n]\n    \n\n}\n\n\n\n\n\n\nSignature\n\u00b6\n\n\ndef source_config_discovery(source_connection)\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsource_connection\n\n\nInstance of the \nSourceConnection\n class.\n\n\n\n\n\n\nrepository\n\n\nInstance of the \nRepository\n class.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nAn array of \nSourceConfigDefinition\n objects.\n\n\nDirect Linked Soure Pre-Snapshot\n\u00b6\n\n\nDirect Linked Soure Post-Snapshot\n\u00b6\n\n\nStaged Linked Soure Pre-Snapshot\n\u00b6\n\n\nStaged Linked Soure Post-Snapshot\n\u00b6\n\n\nStaged Linked Soure Start-Staging\n\u00b6\n\n\nStaged Linked Soure Stop-Staging\n\u00b6\n\n\nStaged Linked Soure Status\n\u00b6\n\n\nStaged Linked Soure Worker\n\u00b6\n\n\nStaged Linked Soure Mount Specification\n\u00b6\n\n\nVirtual Source Configure\n\u00b6\n\n\nConfigures the data in a particular snapshot to be usable on a target environment. For database data files, this may mean recovering from a crash consistent format or backup. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks.\nRequired to implement for Delphix Engine operations:\n\n\n\n\nProvision\n\n\nRefresh\n\n\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nplatform\n\n\nfrom\n \ndlpx.virtualization.generated\n \nimport\n \nSourceConfigDefinition\n\n\n\nplugin\n \n=\n \nplatform\n.\nplugin\n()\n\n\n\n@plugin.virtual.configure\n()\n\n\ndef\n \nconfigure\n(\nvirtual_source\n,\n \nrepository\n,\n \nsnapshot\n):\n\n  \nname\n \n=\n \n\"config_name\"\n\n  \nsource_config\n \n=\n \nSourceConfigDefinition\n()\n\n  \nsource_config\n.\nname\n \n=\n \nname\n\n  \nreturn\n \nsource_config\n\n\n\n\n\n\n\n\nThe above command assumes a \nSourceConfig Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"required\"\n:\n \n[\n\"name\"\n],\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"name\"\n],\n\n  \n\"nameField\"\n:\n \n[\n\"name\"\n]\n\n\n}\n\n\n\n\n\n\nSignature\n\u00b6\n\n\ndef configure(virtual_source, snapshot, repository)\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nInstance of the \nVirtualSource\n class.\n\n\n\n\n\n\nsnapshot\n\n\nInstance of the Snapshot class.\n\n\n\n\n\n\nrepository\n\n\nInstance of the Repository class.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nSourceConfigDefinition\n\n\nVirtual Source Unconfigure\n\u00b6\n\n\nVirtual Source Reconfigure\n\u00b6\n\n\nVirtual Soure Start\n\u00b6\n\n\nExecuted whenever the data should be placed in a \"running\" state.\nRequired to implement for Delphix Engine operations:\n\n\n\n\nStart\n\n\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nplatform\n\n\n\nplugin\n \n=\n \nplatform\n.\nplugin\n()\n\n\n\n@plugin.virtual.start\n()\n\n\ndef\n \nstart\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \nname\n \n=\n \nsource_config\n.\nname\n\n  \nreturn\n\n\n\n\n\n\n\n\nThe above command assumes a \nSourceConfig Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"required\"\n:\n \n[\n\"name\"\n],\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"name\"\n],\n\n  \n\"nameField\"\n:\n \n[\n\"name\"\n]\n\n\n}\n\n\n\n\n\n\nSignature\n\u00b6\n\n\ndef start(virtual_source, repository, source_config)\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nInstance of the \nVirtualSource\n class.\n\n\n\n\n\n\nrepository\n\n\nInstance of the Repository class.\n\n\n\n\n\n\nsource_config\n\n\nInstance of the \nSourceConfigDefinition\n class.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nVirtual Source Stop\n\u00b6\n\n\nVirtual Source Pre-Snapshot\n\u00b6\n\n\nVirtual Source Post-Snapshot\n\u00b6\n\n\nVirtual Source Mount Specification\n\u00b6\n\n\nVirtual Source Status\n\u00b6",
            "title": "Plugin Operations"
        },
        {
            "location": "/References/Plugin_Operations/#plugin-operations",
            "text": "",
            "title": "Plugin Operations"
        },
        {
            "location": "/References/Plugin_Operations/#repository-discovery",
            "text": "Discovers the set of  repositories  for a plugin on an  environment . For a DBMS, this can correspond to the set of binaries installed on a Unix host.  from   dlpx.virtualization   import   platform  from   dlpx.virtualization.generated   import   RepositoryDefinition  plugin   =   platform . plugin ()  @plugin.discovery.repository ()  def   repository_discovery ( source_connection ):   \n   repository   =   RepositoryDefinition () \n   repository . installPath   =   \"/usr/bin/install\" \n   repository . version   =   \"1.2.3\" \n   return   [ repository ]    The above command assumes a  Repository Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"installPath\" :   {   \"type\" :   \"string\"   }, \n     \"version\" :   {   \"type\" :   \"string\"   } \n   }, \n   \"identityFields\" :   [ \"installPath\" ], \n   \"nameField\" :   [ \"installPath\" ]      }",
            "title": "Repository Discovery"
        },
        {
            "location": "/References/Plugin_Operations/#signature",
            "text": "def repository_discovery(source_connection)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#arguments",
            "text": "Parameter  Description      source_connection  Instance of the  SourceConnection  class.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns",
            "text": "An array of  RepositoryDefinition  objects.",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#source-config-discovery",
            "text": "Discovers the set of  source configs  for a plugin for a  repository . For a DBMS, this can correspond to the set of unique databases running using a particular installation on a Unix host.  from   dlpx.virtualization   import   platform  from   dlpx.virtualization.generated   import   SourceConfigDefinition  plugin   =   platform . plugin ()  @plugin.discovery.source_config ()  def   source_config_discovery ( source_connection ,   repository ): \n   source_config   =   SourceConfigDefinition () \n   source_config . name   =   \"my_name\" \n   source_config . port   =   10000 \n   return   [ source_config ]    The above command assumes a  Source Config Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   }, \n     \"port\" :   {   \"type\" :   \"number\"   } \n   }, \n   \"identityFields\" :   [ \"name\" ], \n   \"nameField\" :   [ \"name\" ]      }",
            "title": "Source Config Discovery"
        },
        {
            "location": "/References/Plugin_Operations/#signature_1",
            "text": "def source_config_discovery(source_connection)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_1",
            "text": "Parameter  Description      source_connection  Instance of the  SourceConnection  class.    repository  Instance of the  Repository  class.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_1",
            "text": "An array of  SourceConfigDefinition  objects.",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#direct-linked-soure-pre-snapshot",
            "text": "",
            "title": "Direct Linked Soure Pre-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#direct-linked-soure-post-snapshot",
            "text": "",
            "title": "Direct Linked Soure Post-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-soure-pre-snapshot",
            "text": "",
            "title": "Staged Linked Soure Pre-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-soure-post-snapshot",
            "text": "",
            "title": "Staged Linked Soure Post-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-soure-start-staging",
            "text": "",
            "title": "Staged Linked Soure Start-Staging"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-soure-stop-staging",
            "text": "",
            "title": "Staged Linked Soure Stop-Staging"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-soure-status",
            "text": "",
            "title": "Staged Linked Soure Status"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-soure-worker",
            "text": "",
            "title": "Staged Linked Soure Worker"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-soure-mount-specification",
            "text": "",
            "title": "Staged Linked Soure Mount Specification"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-configure",
            "text": "Configures the data in a particular snapshot to be usable on a target environment. For database data files, this may mean recovering from a crash consistent format or backup. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks.\nRequired to implement for Delphix Engine operations:   Provision  Refresh   from   dlpx.virtualization   import   platform  from   dlpx.virtualization.generated   import   SourceConfigDefinition  plugin   =   platform . plugin ()  @plugin.virtual.configure ()  def   configure ( virtual_source ,   repository ,   snapshot ): \n   name   =   \"config_name\" \n   source_config   =   SourceConfigDefinition () \n   source_config . name   =   name \n   return   source_config    The above command assumes a  SourceConfig Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"required\" :   [ \"name\" ], \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   } \n   }, \n   \"identityFields\" :   [ \"name\" ], \n   \"nameField\" :   [ \"name\" ]  }",
            "title": "Virtual Source Configure"
        },
        {
            "location": "/References/Plugin_Operations/#signature_2",
            "text": "def configure(virtual_source, snapshot, repository)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_2",
            "text": "Parameter  Description      source  Instance of the  VirtualSource  class.    snapshot  Instance of the Snapshot class.    repository  Instance of the Repository class.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_2",
            "text": "SourceConfigDefinition",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-unconfigure",
            "text": "",
            "title": "Virtual Source Unconfigure"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-reconfigure",
            "text": "",
            "title": "Virtual Source Reconfigure"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-soure-start",
            "text": "Executed whenever the data should be placed in a \"running\" state.\nRequired to implement for Delphix Engine operations:   Start   from   dlpx.virtualization   import   platform  plugin   =   platform . plugin ()  @plugin.virtual.start ()  def   start ( virtual_source ,   repository ,   source_config ): \n   name   =   source_config . name \n   return    The above command assumes a  SourceConfig Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"required\" :   [ \"name\" ], \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   } \n   }, \n   \"identityFields\" :   [ \"name\" ], \n   \"nameField\" :   [ \"name\" ]  }",
            "title": "Virtual Soure Start"
        },
        {
            "location": "/References/Plugin_Operations/#signature_3",
            "text": "def start(virtual_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_3",
            "text": "Parameter  Description      source  Instance of the  VirtualSource  class.    repository  Instance of the Repository class.    source_config  Instance of the  SourceConfigDefinition  class.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_3",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-stop",
            "text": "",
            "title": "Virtual Source Stop"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-pre-snapshot",
            "text": "",
            "title": "Virtual Source Pre-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-post-snapshot",
            "text": "",
            "title": "Virtual Source Post-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-mount-specification",
            "text": "",
            "title": "Virtual Source Mount Specification"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-status",
            "text": "",
            "title": "Virtual Source Status"
        },
        {
            "location": "/References/Classes/",
            "text": "Classes\n\u00b6\n\n\nVirtualSource\n\u00b6\n\n\nRepresents a Virtual Source object and its properties.\n\n\nfrom\n \ndlpx.virtualization.common\n \nimport\n \nVirtualSource\n\n\n\nvirtual_source\n \n=\n \nVirtualSource\n()\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nguid\n\n\nString\n\n\nUnique Identified for the source.\n\n\n\n\n\n\nconnection\n\n\nRemoteConnection\n\n\nConnection for the source\n\n\n\n\n\n\nparameters\n\n\nVirtualSourceDefinition\n\n\nUser input as per the \nVirtualSource schema\n.\n\n\n\n\n\n\n\n\nRemoteConnection\n\u00b6\n\n\nRepresents a connection to a source.\n\n\nfrom\n \ndlpx.virtualization.common\n \nimport\n \nRemoteConnection\n\n\n\nconnection\n \n=\n \nRemoteConnection\n()\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nenvironment\n\n\nRemoteEnvironment\n\n\nEnvironment for the connection.\n\n\n\n\n\n\nuser\n\n\nRemoteUser\n\n\nUser for the connection.",
            "title": "Classes"
        },
        {
            "location": "/References/Classes/#classes",
            "text": "",
            "title": "Classes"
        },
        {
            "location": "/References/Classes/#virtualsource",
            "text": "Represents a Virtual Source object and its properties.  from   dlpx.virtualization.common   import   VirtualSource  virtual_source   =   VirtualSource ()",
            "title": "VirtualSource"
        },
        {
            "location": "/References/Classes/#fields",
            "text": "Field  Type  Description      guid  String  Unique Identified for the source.    connection  RemoteConnection  Connection for the source    parameters  VirtualSourceDefinition  User input as per the  VirtualSource schema .",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#remoteconnection",
            "text": "Represents a connection to a source.  from   dlpx.virtualization.common   import   RemoteConnection  connection   =   RemoteConnection ()",
            "title": "RemoteConnection"
        },
        {
            "location": "/References/Classes/#fields_1",
            "text": "Field  Type  Description      environment  RemoteEnvironment  Environment for the connection.    user  RemoteUser  User for the connection.",
            "title": "Fields"
        },
        {
            "location": "/References/Schemas/",
            "text": "Schemas\n\u00b6\n\n\nAbout Schemas\n\u00b6\n\n\nAny time a plugin needs to store its own data, or needs to ask the user for data, the Delphix Engine needs to be told about the format of that data:\n\n\n\n\nWhat is the set of data needed and what should they be called?\n\n\nWhat is the type of each piece of data: Strings? Integers? Booleans?\n\n\n\n\nPlugins use \nschemas\n to describe the format of such data. Once a schema is defined, it is used in three ways\n\n\n\n\nIt tells the Delphix Engine how to store the data for later use.\n\n\nIt is used to autogenerate a custom user interface, and to validate user inputs.\n\n\nIt is used to \nautogenerate Python classes\n that can be used by plugin code to access and manipulate user input and stored data.\n\n\n\n\nThere are five plugin-customizable data formats:\n\n\n\n\n\n\n\n\nDelphix Object\n\n\nSchema\n\n\nAutogenerated Class\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nRepositoryDefinition\n\n\nRepositoryDefinition\n\n\n\n\n\n\nSource Config\n\n\nSourceConfigDefinition\n\n\nSourceConfigDefinition\n\n\n\n\n\n\nLinked Source\n\n\nLinkedSourceDefinition\n\n\nLinkedSourceDefinition\n\n\n\n\n\n\nVirtual Source\n\n\nVirtualSourceDefinition\n\n\nVirtualSourceDefinition\n\n\n\n\n\n\nSnapshot\n\n\nSnapshotDefinition\n\n\nSnapshotDefinition\n\n\n\n\n\n\n\n\nJSON Schemas\n\u00b6\n\n\nPlugins use JSON schemas for their custom datatypes. There are three main things to understand about them, which are explained just below:\n\n\n\n\nWhat is JSON?\n\n\nWhat is a JSON schema?\n\n\nHow has Delphix augmented JSON schemas?\n\n\n\n\nJSON\n\u00b6\n\n\nJSON stands for \"Javascript Object Notation\". JSON is a data-interchange format that is intended to be precise and also somewhat human-readable. Here are some simple examples of data in JSON format:\n\n\n\n\n\n\n\n\nJSON\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\"hello\"\n\n\nA string. Note the double quotes.\n\n\n\n\n\n\n17\n\n\nAn integer\n\n\n\n\n\n\ntrue\n\n\nA boolean\n\n\n\n\n\n\n{\"name\": \"Julie\", \"age\": 37}\n\n\nA JSON object with two fields, \nname\n (a string), and \nage\n (an integer). Objects are denoted with curly braces.\n\n\n\n\n\n\n[ true, false, true]\n\n\nA JSON array with three booleans. Arrays are denoted with square brackets.\n\n\n\n\n\n\n\n\nFor more details on JSON, please see https://www.json.org/.\n\n\nJSON Schemas\n\u00b6\n\n\nThe \"JSON schema\" format is built on top of JSON. This adds some special rules and keywords that are intended to facilitate the \ndescription\n of the format of data (whereas \"raw\" JSON is intended for storing data).\n\n\nHere is an example of a JSON schema that defines a (simplified) US address:\n\n\n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"required\"\n:\n \n[\n\"name\"\n,\n \n\"streetNumber\"\n,\n \n\"street\"\n,\n \n\"city\"\n,\n \n\"state\"\n,\n \n\"zip5\"\n],\n\n    \n\"additionalProperties\"\n:\n \nfalse\n,\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n        \n\"streetNumber\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n        \n\"street\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n        \n\"unit\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n        \n\"city\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n,\n \n\"pattern\"\n:\n \n\"^[A-Z][A-Za-z ]*$\"\n \n},\n\n        \n\"state\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n,\n \n\"pattern\"\n:\n \n\"^[A-Z]{2}$\"\n \n},\n\n        \n\"zip5\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n,\n \n\"pattern\"\n:\n \n\"^[0-9]{5}\"\n},\n\n        \n\"zipPlus4\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n,\n \n\"pattern\"\n:\n \n\"^[0-9]{4}\"\n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nNote that this is perfectly valid JSON data. It's a JSON object with four fields: \ntype\n (a JSON string), \nrequired\n (A JSON array), \nadditionalProperties\n (a JSON boolean), and \nproperties\n. \nproperties\n, in turn is a JSON object with with 8 fields, each of which is a JSON object, with its own properties, etc.\n\n\nBut, this isn't \njust\n a JSON object. This is a JSON schema. It uses special keywords like \ntype\n \nrequired\n, and \nadditionalProperties\n. These have specially-defined meanings in the context of JSON schemas.\n\n\nHere is a list of the special keywords used by the above schema. Note that this is only a small subset of JSON schema keywords.\n\n\n\n\n\n\n\n\nkeyword\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nadditionalProperties\n\n\nDetermines whether the schema allows properties that are not explicitly listed in the \nproperties\n specification. Must be a \ntrue\n or \nfalse\n.\n\n\n\n\n\n\npattern\n\n\nUsed with string types to specify a regular expression that the property must conform to.\n\n\n\n\n\n\nrequired\n\n\nA list of required properties. Properties not listed in this list are optional.\n\n\n\n\n\n\nstring\n\n\nUsed with \ntype\n to declare that a property must be a string.\n\n\n\n\n\n\ntype\n\n\nSpecifies a datatype. Common values are \nobject\n, \narray\n, \nnumber\n, \ninteger\n, \nboolean\n, and \nstring\n.\n\n\n\n\n\n\n\n\nSome points to note about the address schema above:\n\n\n\n\nBecause of the \nrequired\n list, all valid addresses must have fields called \nname\n, \nstreetNumber\n and so on.\n\n\nunit\n and \nzipPlus4\n do not appear in the \nrequired\n list, and therefore are optional.\n\n\nBecause of \nadditionalProperties\n being \nfalse\n, valid addresses cannot make up their own fields like \nnickname\n or \ndoorbellLocation\n.\n\n\nBecause of the \npattern\n, any \nstate\n field in a valid address must consist of exactly two capital letters.\n\n\nSimilarly, \ncity\n must only contain letters and spaces, and \nzip\n and \nzipPlus4\n must only contain digits.\n\n\nEach property has its own valid subschema that describes its own type definition.\n\n\n\n\nHere is a JSON object that conforms to the above schema:\n\n\n{\n\n  \n\"name\"\n:\n \n\"Delphix\"\n,\n\n  \n\"streetNumber\"\n:\n \n\"220\"\n,\n\n  \n\"street\"\n:\n \n\"Congress St.\"\n,\n\n  \n\"unit\"\n:\n \n\"200\"\n,\n\n  \n\"city\"\n:\n \n\"Boston\"\n,\n\n  \n\"state\"\n:\n \n\"MA\"\n,\n\n  \n\"zip\"\n:\n \n\"02210\"\n\n\n}\n\n\n\n\n\n\n\n\nNote\n\n\nA common point of confusion is the distinction between a JSON schema and a JSON object that conforms to a schema. Remember, a schema describes the form of data. In our example, the schema \ndescribes\n what an address looks like. The address itself is not a schema.\n\n\n\n\nFor much more detail on JSON schemas, including which keywords are available, what they mean, and where you can use them, see https://json-schema.org/understanding-json-schema/\n\n\nDelphix-specific Extensions to JSON Schema\n\u00b6\n\n\nThe JSON schema vocabulary is designed to be extensible for special uses, and Delphix has taken advantage of this to add some new Delphix-specific keywords.\n\n\nThe list below outlines each of these keywords, and provides minimal examples of how they might be used.\n\n\ndescription\n\u00b6\n\n\n\n\n\n\n\n\nSummary for \ndescription\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nOptional\n\n\n\n\n\n\nWhere?\n\n\nIn any property subschema, at the same level as \ntype\n.\n\n\n\n\n\n\n\n\nThe \ndescription\n keyword can optionally appear on any property. If it does appear, it is used by the UI as explanatory text for the UI widget associated with the property. If it does not appear, then no explanatory text is shown.\n\n\nIn this example, the UI would show \"User-readable name for the provisioned database\" in small text under the widget.\n\n\n{\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n\n      \n\"type\"\n:\n \n\"string\"\n,\n\n      \n\"description\"\n:\n \n\"User-readable name for the provisioned database\"\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nidentityFields\n\u00b6\n\n\n\n\n\n\n\n\nSummary for \nidentityFields\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nRequired (for repository and source config schemas only)\n\n\n\n\n\n\nWhere?\n\n\nAt the top level of a repository or source config schema, at the same level as \ntype\n and \nproperties\n.\n\n\n\n\n\n\n\n\nThe \nidentityFields\n is a list of property names that, together, serve as a unique identifier for a repository or source config.\n\n\nWhen a plugin's \nautomatic discovery\n code is called, it will return a list of repositories (or source configs). The Delphix Engine needs to be able to compare this new list with whatever repositories it already knows about.\n\n\nFor example, suppose the engine already knows about a single repository with data \n{\"dbname\": \"my_databsae\", \"path\": \"/var/db/db01\"}\n (note the misspelling!). And, then suppose that automatic discovery is re-run and it returns repository data \n{ \"dbname\": \"my_database\", \"path\": \"/var/db/db01\"}\n.\n\n\nWhat should the Delphix Engine do? Should it conclude that \"my_databsae\" has been deleted, and there is a completely new repository named \"my_database\"? Or, should it conclude that we still have the same old repository, but with an updated name?\n\n\nidentityFields\n is used to handle this. When the engine compares \"new\" data with \"old\" data, it concludes that they belong to the same repository if \nall\n of the identity fields match. If any of the identity fields do not match, then the \"new\" repository data is judged to represent a different repository than the old data.\n\n\nidentityFields\n is \nrequired\n for \nRepositoryDefinition\n and \nSourceConfigDefinition\n schemas, and may not be used in any other schemas.\n\n\nIn this example, we'll tell the Delphix Engine that \npath\n is the sole unique identifier.\n\n\n{\n\n  \n\"properties\"\n:\n \n{\n\n      \n\"dbname\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n},\n\n      \n\"path\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"path\"\n]\n\n\n}\n\n\n\n\n\n\nnameField\n\u00b6\n\n\n\n\n\n\n\n\nSummary for \nnameField\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nRequired (for repository and source config schemas only)\n\n\n\n\n\n\nWhere?\n\n\nAt the top level of a repository or source config schema, at the same level as \ntype\n and \nproperties\n.\n\n\n\n\n\n\n\n\nThe \nnameField\n keyword specifies a single property that is to be used to name the object in the Delphix Engine. The property must be a string field. This keyword is used at the same level as \nproperties\n. It is \nrequired\n for \nRepositoryDefinition\n and \nSourceConfigDefinition\n schemas, and may not be used in any other schemas.\n\n\nIn this example, we will use the \npath\n property as the user-visible name.\n\n\n{\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"path\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n        \n\"port\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n    \n},\n\n    \n\"nameField\"\n:\n \n\"path\"\n\n\n}\n\n\n\n\n\n\nSo, if we have an repository object that looks like\n\n\n{\n\n  \n\"path\"\n:\n \n\"/usr/bin\"\n,\n\n  \n\"port\"\n:\n \n8800\n\n\n}\n\n\n\n\n\n\nthen the user will be able to refer to this object as \n/usr/bin\n.\n\n\npassword\n\u00b6\n\n\n\n\n\n\n\n\nSummary for \npassword\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nOptional\n\n\n\n\n\n\nWhere?\n\n\nAs the value for the \nformat\n keyword in any string property's subschema.\n\n\n\n\n\n\n\n\nThe \npassword\n keyword can be used to specify the \nformat\n of a \nstring\n. (Note that \nformat\n is a standard keyword and is not Delphix-specific). If a property is tagged as a password, then the UI will never show the value on screen, and the value will be encrypted before being stored as described \nhere\n.\n\n\nIn this example, the \ndbPass\n field on any object will be treated as a password.\n\n\n{\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"dbPass\"\n:\n \n{\n\n      \n\"type\"\n:\n \n\"string\"\n,\n\n      \n\"format\"\n:\n \n\"password\"\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nprettyName\n\u00b6\n\n\n\n\n\n\n\n\nSummary for \npassword\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nOptional\n\n\n\n\n\n\nWhere?\n\n\nIn any property subschema, at the same level as \ntype\n.\n\n\n\n\n\n\n\n\nThe \nprettyName\n keyword can optionally appear on any property. If it does appear, it is used by the UI as a title for the UI widget associated with the property. If it does not appear, then the name of the property is used.\n\n\nIn this example, the user would see \"Name of Database\" on the UI, instead of just \"name\".\n\n\n{\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n\n      \n\"type\"\n:\n \n\"string\"\n,\n\n      \n\"prettyName\"\n:\n \n\"Name of Database\"\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nunixpath\n\u00b6\n\n\n\n\n\n\n\n\nSummary for \npassword\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nOptional\n\n\n\n\n\n\nWhere?\n\n\nAs the value for the \nformat\n keyword in any string property's subschema.\n\n\n\n\n\n\n\n\nThe \nunixpath\n keyword is used to specify the format of a string. This will allow the Delphix Engine to verify and enforce that a particular field can be parsed as a valid Unix path.\n\n\n{\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"datapath\"\n:\n \n{\n\n      \n\"type\"\n:\n \n\"string\"\n,\n\n      \n\"format\"\n:\n \n\"unixpath\"\n\n    \n}\n\n  \n}\n\n\n}",
            "title": "Schemas"
        },
        {
            "location": "/References/Schemas/#schemas",
            "text": "",
            "title": "Schemas"
        },
        {
            "location": "/References/Schemas/#about-schemas",
            "text": "Any time a plugin needs to store its own data, or needs to ask the user for data, the Delphix Engine needs to be told about the format of that data:   What is the set of data needed and what should they be called?  What is the type of each piece of data: Strings? Integers? Booleans?   Plugins use  schemas  to describe the format of such data. Once a schema is defined, it is used in three ways   It tells the Delphix Engine how to store the data for later use.  It is used to autogenerate a custom user interface, and to validate user inputs.  It is used to  autogenerate Python classes  that can be used by plugin code to access and manipulate user input and stored data.   There are five plugin-customizable data formats:     Delphix Object  Schema  Autogenerated Class      Repository  RepositoryDefinition  RepositoryDefinition    Source Config  SourceConfigDefinition  SourceConfigDefinition    Linked Source  LinkedSourceDefinition  LinkedSourceDefinition    Virtual Source  VirtualSourceDefinition  VirtualSourceDefinition    Snapshot  SnapshotDefinition  SnapshotDefinition",
            "title": "About Schemas"
        },
        {
            "location": "/References/Schemas/#json-schemas",
            "text": "Plugins use JSON schemas for their custom datatypes. There are three main things to understand about them, which are explained just below:   What is JSON?  What is a JSON schema?  How has Delphix augmented JSON schemas?",
            "title": "JSON Schemas"
        },
        {
            "location": "/References/Schemas/#json",
            "text": "JSON stands for \"Javascript Object Notation\". JSON is a data-interchange format that is intended to be precise and also somewhat human-readable. Here are some simple examples of data in JSON format:     JSON  Description      \"hello\"  A string. Note the double quotes.    17  An integer    true  A boolean    {\"name\": \"Julie\", \"age\": 37}  A JSON object with two fields,  name  (a string), and  age  (an integer). Objects are denoted with curly braces.    [ true, false, true]  A JSON array with three booleans. Arrays are denoted with square brackets.     For more details on JSON, please see https://www.json.org/.",
            "title": "JSON"
        },
        {
            "location": "/References/Schemas/#json-schemas_1",
            "text": "The \"JSON schema\" format is built on top of JSON. This adds some special rules and keywords that are intended to facilitate the  description  of the format of data (whereas \"raw\" JSON is intended for storing data).  Here is an example of a JSON schema that defines a (simplified) US address:  { \n     \"type\" :   \"object\" , \n     \"required\" :   [ \"name\" ,   \"streetNumber\" ,   \"street\" ,   \"city\" ,   \"state\" ,   \"zip5\" ], \n     \"additionalProperties\" :   false , \n     \"properties\" :   { \n         \"name\" :   {   \"type\" :   \"string\"   }, \n         \"streetNumber\" :   {   \"type\" :   \"string\"   }, \n         \"street\" :   {   \"type\" :   \"string\"   }, \n         \"unit\" :   {   \"type\" :   \"string\"   }, \n         \"city\" :   {   \"type\" :   \"string\" ,   \"pattern\" :   \"^[A-Z][A-Za-z ]*$\"   }, \n         \"state\" :   {   \"type\" :   \"string\" ,   \"pattern\" :   \"^[A-Z]{2}$\"   }, \n         \"zip5\" :   {   \"type\" :   \"string\" ,   \"pattern\" :   \"^[0-9]{5}\" }, \n         \"zipPlus4\" :   {   \"type\" :   \"string\" ,   \"pattern\" :   \"^[0-9]{4}\" } \n     }  }   Note that this is perfectly valid JSON data. It's a JSON object with four fields:  type  (a JSON string),  required  (A JSON array),  additionalProperties  (a JSON boolean), and  properties .  properties , in turn is a JSON object with with 8 fields, each of which is a JSON object, with its own properties, etc.  But, this isn't  just  a JSON object. This is a JSON schema. It uses special keywords like  type   required , and  additionalProperties . These have specially-defined meanings in the context of JSON schemas.  Here is a list of the special keywords used by the above schema. Note that this is only a small subset of JSON schema keywords.     keyword  description      additionalProperties  Determines whether the schema allows properties that are not explicitly listed in the  properties  specification. Must be a  true  or  false .    pattern  Used with string types to specify a regular expression that the property must conform to.    required  A list of required properties. Properties not listed in this list are optional.    string  Used with  type  to declare that a property must be a string.    type  Specifies a datatype. Common values are  object ,  array ,  number ,  integer ,  boolean , and  string .     Some points to note about the address schema above:   Because of the  required  list, all valid addresses must have fields called  name ,  streetNumber  and so on.  unit  and  zipPlus4  do not appear in the  required  list, and therefore are optional.  Because of  additionalProperties  being  false , valid addresses cannot make up their own fields like  nickname  or  doorbellLocation .  Because of the  pattern , any  state  field in a valid address must consist of exactly two capital letters.  Similarly,  city  must only contain letters and spaces, and  zip  and  zipPlus4  must only contain digits.  Each property has its own valid subschema that describes its own type definition.   Here is a JSON object that conforms to the above schema:  { \n   \"name\" :   \"Delphix\" , \n   \"streetNumber\" :   \"220\" , \n   \"street\" :   \"Congress St.\" , \n   \"unit\" :   \"200\" , \n   \"city\" :   \"Boston\" , \n   \"state\" :   \"MA\" , \n   \"zip\" :   \"02210\"  }    Note  A common point of confusion is the distinction between a JSON schema and a JSON object that conforms to a schema. Remember, a schema describes the form of data. In our example, the schema  describes  what an address looks like. The address itself is not a schema.   For much more detail on JSON schemas, including which keywords are available, what they mean, and where you can use them, see https://json-schema.org/understanding-json-schema/",
            "title": "JSON Schemas"
        },
        {
            "location": "/References/Schemas/#delphix-specific-extensions-to-json-schema",
            "text": "The JSON schema vocabulary is designed to be extensible for special uses, and Delphix has taken advantage of this to add some new Delphix-specific keywords.  The list below outlines each of these keywords, and provides minimal examples of how they might be used.",
            "title": "Delphix-specific Extensions to JSON Schema"
        },
        {
            "location": "/References/Schemas/#description",
            "text": "Summary for  description       Required or Optional?  Optional    Where?  In any property subschema, at the same level as  type .     The  description  keyword can optionally appear on any property. If it does appear, it is used by the UI as explanatory text for the UI widget associated with the property. If it does not appear, then no explanatory text is shown.  In this example, the UI would show \"User-readable name for the provisioned database\" in small text under the widget.  { \n   \"properties\" :   { \n     \"name\" :   { \n       \"type\" :   \"string\" , \n       \"description\" :   \"User-readable name for the provisioned database\" \n     } \n   }  }",
            "title": "description"
        },
        {
            "location": "/References/Schemas/#identityfields",
            "text": "Summary for  identityFields       Required or Optional?  Required (for repository and source config schemas only)    Where?  At the top level of a repository or source config schema, at the same level as  type  and  properties .     The  identityFields  is a list of property names that, together, serve as a unique identifier for a repository or source config.  When a plugin's  automatic discovery  code is called, it will return a list of repositories (or source configs). The Delphix Engine needs to be able to compare this new list with whatever repositories it already knows about.  For example, suppose the engine already knows about a single repository with data  {\"dbname\": \"my_databsae\", \"path\": \"/var/db/db01\"}  (note the misspelling!). And, then suppose that automatic discovery is re-run and it returns repository data  { \"dbname\": \"my_database\", \"path\": \"/var/db/db01\"} .  What should the Delphix Engine do? Should it conclude that \"my_databsae\" has been deleted, and there is a completely new repository named \"my_database\"? Or, should it conclude that we still have the same old repository, but with an updated name?  identityFields  is used to handle this. When the engine compares \"new\" data with \"old\" data, it concludes that they belong to the same repository if  all  of the identity fields match. If any of the identity fields do not match, then the \"new\" repository data is judged to represent a different repository than the old data.  identityFields  is  required  for  RepositoryDefinition  and  SourceConfigDefinition  schemas, and may not be used in any other schemas.  In this example, we'll tell the Delphix Engine that  path  is the sole unique identifier.  { \n   \"properties\" :   { \n       \"dbname\" :   { \"type\" :   \"string\" }, \n       \"path\" :   { \"type\" :   \"string\" } \n   }, \n   \"identityFields\" :   [ \"path\" ]  }",
            "title": "identityFields"
        },
        {
            "location": "/References/Schemas/#namefield",
            "text": "Summary for  nameField       Required or Optional?  Required (for repository and source config schemas only)    Where?  At the top level of a repository or source config schema, at the same level as  type  and  properties .     The  nameField  keyword specifies a single property that is to be used to name the object in the Delphix Engine. The property must be a string field. This keyword is used at the same level as  properties . It is  required  for  RepositoryDefinition  and  SourceConfigDefinition  schemas, and may not be used in any other schemas.  In this example, we will use the  path  property as the user-visible name.  { \n     \"properties\" :   { \n         \"path\" :   {   \"type\" :   \"string\"   }, \n         \"port\" :   {   \"type\" :   \"integer\"   } \n     }, \n     \"nameField\" :   \"path\"  }   So, if we have an repository object that looks like  { \n   \"path\" :   \"/usr/bin\" , \n   \"port\" :   8800  }   then the user will be able to refer to this object as  /usr/bin .",
            "title": "nameField"
        },
        {
            "location": "/References/Schemas/#password",
            "text": "Summary for  password       Required or Optional?  Optional    Where?  As the value for the  format  keyword in any string property's subschema.     The  password  keyword can be used to specify the  format  of a  string . (Note that  format  is a standard keyword and is not Delphix-specific). If a property is tagged as a password, then the UI will never show the value on screen, and the value will be encrypted before being stored as described  here .  In this example, the  dbPass  field on any object will be treated as a password.  { \n   \"properties\" :   { \n     \"dbPass\" :   { \n       \"type\" :   \"string\" , \n       \"format\" :   \"password\" \n     } \n   }  }",
            "title": "password"
        },
        {
            "location": "/References/Schemas/#prettyname",
            "text": "Summary for  password       Required or Optional?  Optional    Where?  In any property subschema, at the same level as  type .     The  prettyName  keyword can optionally appear on any property. If it does appear, it is used by the UI as a title for the UI widget associated with the property. If it does not appear, then the name of the property is used.  In this example, the user would see \"Name of Database\" on the UI, instead of just \"name\".  { \n   \"properties\" :   { \n     \"name\" :   { \n       \"type\" :   \"string\" , \n       \"prettyName\" :   \"Name of Database\" \n     } \n   }  }",
            "title": "prettyName"
        },
        {
            "location": "/References/Schemas/#unixpath",
            "text": "Summary for  password       Required or Optional?  Optional    Where?  As the value for the  format  keyword in any string property's subschema.     The  unixpath  keyword is used to specify the format of a string. This will allow the Delphix Engine to verify and enforce that a particular field can be parsed as a valid Unix path.  { \n   \"properties\" :   { \n     \"datapath\" :   { \n       \"type\" :   \"string\" , \n       \"format\" :   \"unixpath\" \n     } \n   }  }",
            "title": "unixpath"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/",
            "text": "Schemas and Autogenerated Classes\n\u00b6\n\n\nPlugin operations\n will sometimes need to work with data in these custom formats. For example, the \nconfigure\n operation will accept snapshot data as an input, and must produce source config data as an output.\n\n\nTo enable this, Python classes are generated from the snapshot schema. The aforementioned inputs and outputs are instances of these autogenerated classes.\n\n\nRepositoryDefinition\n\u00b6\n\n\nDefines properties used to identify a \nRepository\n.\n\n\nRepositoryDefinition Schema\n\u00b6\n\n\nThe plugin must also decide on a \nname\n field and a set of \nidentityFields\n to display and uniquely identify the \nrepository\n.\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"path\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"name\"\n,\n \n\"path\"\n],\n\n  \n\"nameField\"\n:\n \n\"name\"\n\n\n}\n\n\n\n\n\n\nRepositoryDefinition Class\n\u00b6\n\n\nAutogenerated based on the \nRepositoryDefinition Schema\n.\n\n\nclass\n \nRepositoryDefinition\n:\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nrepository_name\n,\n \nrepository_path\n):\n\n    \nself\n.\n_inner_dict\n \n=\n \n{\n\"name\"\n:\n \nrepository_name\n,\n \n\"path\"\n:\n \nrepository_path\n}\n\n\n\n\n\n\n\n\nTo use the class:\n\n\n\n\nimport\n \ndlpx\n\n\nfrom\n \ndlpx.virtualization.generated\n \nimport\n \nRepositoryDefinition\n\n\n\nrepository\n \n=\n \nRepositoryDefinition\n()\n\n\nrepository\n.\nname\n \n=\n \n\"name\"\n\n\nrepository\n.\npath\n \n=\n \n\"/some/path\"\n\n\n\n\n\n\nSourceConfigDefinition\n\u00b6\n\n\nDefines properties used to identify a \nSource Config\n.\n\n\nSourceConfigDefinition Schema\n\u00b6\n\n\nThe plugin must also decide on a \nname\n field and a set of \nidentityFields\n to display and uniquely identify the \nsource config\n.\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"path\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"name\"\n],\n\n  \n\"nameField\"\n:\n \n\"name\"\n\n\n}\n\n\n\n\n\n\nSourceConfigDefinition Class\n\u00b6\n\n\nAutogenerated based on the \nSourceConfigDefinition Schema\n.\n\n\nclass\n \nSourceConfigDefinition\n:\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nconfig_name\n,\n \nconfig_path\n):\n\n    \nself\n.\n_inner_dict\n \n=\n \n{\n\"name\"\n:\n \nconfig_name\n,\n \n\"path\"\n:\n \nconfig_path\n}\n\n\n\n\n\n\n\n\nTo use the class:\n\n\n\n\nimport\n \ndlpx\n\n\nfrom\n \ndlpx.virtualization.generated\n \nimport\n \nSourceConfigDefinition\n\n\n\nsource_config\n \n=\n \nSourceConfigDefinition\n()\n\n\nsource_config\n.\nname\n \n=\n \n\"name\"\n\n\nsource_config\n.\npath\n \n=\n \n\"/some/path\"\n\n\n\n\n\n\nLinkedSourceDefinition\n\u00b6\n\n\nDefines properties used to identify \nlinked sources\n.\n\n\nLinkedSourceDefinition Schema\n\u00b6\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"required\"\n:\n \n[\n\"name\"\n,\n \n\"port\"\n],\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"port\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n\n}\n\n\n\n\n\n\nLinkedSourceDefinition Class\n\u00b6\n\n\nAutogenerated based on the \nLinkedSourceDefinition Schema\n.\n\n\nclass\n \nLinkedSourceDefinition\n:\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nsource_name\n,\n \nsource_port\n):\n\n    \nself\n.\n_inner_dict\n \n=\n \n{\n\"name\"\n:\n \nsource_name\n,\n \n\"port\"\n:\n \nsource_port\n}\n\n\n\n\n\n\n\n\nTo use the class:\n\n\n\n\nimport\n \ndlpx\n\n\nfrom\n \ndlpx.virtualization.generated\n \nimport\n \nLinkedSourceDefinition\n\n\n\nsource\n \n=\n \nLinkedSourceDefinition\n(\n\"name\"\n,\n \n1000\n)\n\n\nname\n \n=\n \nsource\n.\nname\n\n\nport\n \n=\n \nsource\n.\nport\n\n\n\n\n\n\nVirtualSourceDefinition\n\u00b6\n\n\nDefines properties used to identify \nvirtual sources\n.\n\n\nVirtualSourceDefinition Schema\n\u00b6\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"required\"\n:\n \n[\n\"name\"\n,\n \n\"port\"\n],\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"port\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n\n}\n\n\n\n\n\n\nVirtualSourceDefinition Class\n\u00b6\n\n\nAutogenerated based on the \nVirtualSourceDefinition Schema\n.\n\n\nclass\n \nVirtualSourceDefinition\n:\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nsource_name\n,\n \nsource_port\n):\n\n    \nself\n.\n_inner_dict\n \n=\n \n{\n\"name\"\n:\n \nsource_name\n,\n \n\"port\"\n:\n \nsource_port\n}\n\n\n\n\n\n\n\n\nTo use the class:\n\n\n\n\nimport\n \ndlpx\n\n\nfrom\n \ndlpx.virtualization.generated\n \nimport\n \nVirtualSourceDefinition\n\n\n\nsource\n \n=\n \nVirtualSourceDefinition\n(\n\"name\"\n,\n \n1000\n)\n\n\nname\n \n=\n \nsource\n.\nname\n\n\nport\n \n=\n \nsource\n.\nport\n\n\n\n\n\n\nSnapshotDefinition\n\u00b6\n\n\nDefines properties used to \nsnapshots\n.\n\n\nSnapshotDefinition Schema\n\u00b6\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"version\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"version\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n\n}\n\n\n\n\n\n\nSnapshotDefinition Class\n\u00b6\n\n\nAutogenerated based on the \nVirtualSourceDefinition Schema\n.\n\n\nclass\n \nVirtualSourceDefinition\n:\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nsnapshot_version\n,\n \nsnapshot_transaction_id\n):\n\n    \nself\n.\n_inner_dict\n \n=\n \n      \n{\n\n        \n\"version\"\n:\n \nsnapshot_version\n,\n \n        \n\"transaction_id\"\n:\n \nsnapshot_transaction_id\n\n      \n}\n\n\n\n\n\n\n\n\nTo use the class:\n\n\n\n\nimport\n \ndlpx\n\n\nfrom\n \ndlpx.virtualization.generated\n \nimport\n \nSnapshotDefinition\n\n\n\nsnapshot\n \n=\n \nSnapshotDefinition\n()\n\n\nsnapshot\n.\nversion\n \n=\n \n\"1.2.3\"\n\n\nsnapshot\n.\ntransaction_id\n \n=\n \n1000",
            "title": "Schemas and Autogenerated Classes"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#schemas-and-autogenerated-classes",
            "text": "Plugin operations  will sometimes need to work with data in these custom formats. For example, the  configure  operation will accept snapshot data as an input, and must produce source config data as an output.  To enable this, Python classes are generated from the snapshot schema. The aforementioned inputs and outputs are instances of these autogenerated classes.",
            "title": "Schemas and Autogenerated Classes"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#repositorydefinition",
            "text": "Defines properties used to identify a  Repository .",
            "title": "RepositoryDefinition"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#repositorydefinition-schema",
            "text": "The plugin must also decide on a  name  field and a set of  identityFields  to display and uniquely identify the  repository .  { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   }, \n     \"path\" :   {   \"type\" :   \"string\"   } \n   }, \n   \"identityFields\" :   [ \"name\" ,   \"path\" ], \n   \"nameField\" :   \"name\"  }",
            "title": "RepositoryDefinition Schema"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#repositorydefinition-class",
            "text": "Autogenerated based on the  RepositoryDefinition Schema .  class   RepositoryDefinition : \n\n   def   __init__ ( self ,   repository_name ,   repository_path ): \n     self . _inner_dict   =   { \"name\" :   repository_name ,   \"path\" :   repository_path }    To use the class:   import   dlpx  from   dlpx.virtualization.generated   import   RepositoryDefinition  repository   =   RepositoryDefinition ()  repository . name   =   \"name\"  repository . path   =   \"/some/path\"",
            "title": "RepositoryDefinition Class"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#sourceconfigdefinition",
            "text": "Defines properties used to identify a  Source Config .",
            "title": "SourceConfigDefinition"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#sourceconfigdefinition-schema",
            "text": "The plugin must also decide on a  name  field and a set of  identityFields  to display and uniquely identify the  source config .  { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   }, \n     \"path\" :   {   \"type\" :   \"string\"   } \n   }, \n   \"identityFields\" :   [ \"name\" ], \n   \"nameField\" :   \"name\"  }",
            "title": "SourceConfigDefinition Schema"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#sourceconfigdefinition-class",
            "text": "Autogenerated based on the  SourceConfigDefinition Schema .  class   SourceConfigDefinition : \n\n   def   __init__ ( self ,   config_name ,   config_path ): \n     self . _inner_dict   =   { \"name\" :   config_name ,   \"path\" :   config_path }    To use the class:   import   dlpx  from   dlpx.virtualization.generated   import   SourceConfigDefinition  source_config   =   SourceConfigDefinition ()  source_config . name   =   \"name\"  source_config . path   =   \"/some/path\"",
            "title": "SourceConfigDefinition Class"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#linkedsourcedefinition",
            "text": "Defines properties used to identify  linked sources .",
            "title": "LinkedSourceDefinition"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#linkedsourcedefinition-schema",
            "text": "{ \n   \"type\" :   \"object\" , \n   \"required\" :   [ \"name\" ,   \"port\" ], \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   }, \n     \"port\" :   {   \"type\" :   \"integer\"   }  }",
            "title": "LinkedSourceDefinition Schema"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#linkedsourcedefinition-class",
            "text": "Autogenerated based on the  LinkedSourceDefinition Schema .  class   LinkedSourceDefinition : \n\n   def   __init__ ( self ,   source_name ,   source_port ): \n     self . _inner_dict   =   { \"name\" :   source_name ,   \"port\" :   source_port }    To use the class:   import   dlpx  from   dlpx.virtualization.generated   import   LinkedSourceDefinition  source   =   LinkedSourceDefinition ( \"name\" ,   1000 )  name   =   source . name  port   =   source . port",
            "title": "LinkedSourceDefinition Class"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#virtualsourcedefinition",
            "text": "Defines properties used to identify  virtual sources .",
            "title": "VirtualSourceDefinition"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#virtualsourcedefinition-schema",
            "text": "{ \n   \"type\" :   \"object\" , \n   \"required\" :   [ \"name\" ,   \"port\" ], \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   }, \n     \"port\" :   {   \"type\" :   \"integer\"   }  }",
            "title": "VirtualSourceDefinition Schema"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#virtualsourcedefinition-class",
            "text": "Autogenerated based on the  VirtualSourceDefinition Schema .  class   VirtualSourceDefinition : \n\n   def   __init__ ( self ,   source_name ,   source_port ): \n     self . _inner_dict   =   { \"name\" :   source_name ,   \"port\" :   source_port }    To use the class:   import   dlpx  from   dlpx.virtualization.generated   import   VirtualSourceDefinition  source   =   VirtualSourceDefinition ( \"name\" ,   1000 )  name   =   source . name  port   =   source . port",
            "title": "VirtualSourceDefinition Class"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#snapshotdefinition",
            "text": "Defines properties used to  snapshots .",
            "title": "SnapshotDefinition"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#snapshotdefinition-schema",
            "text": "{ \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"version\" :   {   \"type\" :   \"string\"   }, \n     \"version\" :   {   \"type\" :   \"integer\"   }  }",
            "title": "SnapshotDefinition Schema"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#snapshotdefinition-class",
            "text": "Autogenerated based on the  VirtualSourceDefinition Schema .  class   VirtualSourceDefinition : \n\n   def   __init__ ( self ,   snapshot_version ,   snapshot_transaction_id ): \n     self . _inner_dict   =  \n       { \n         \"version\" :   snapshot_version ,  \n         \"transaction_id\" :   snapshot_transaction_id \n       }    To use the class:   import   dlpx  from   dlpx.virtualization.generated   import   SnapshotDefinition  snapshot   =   SnapshotDefinition ()  snapshot . version   =   \"1.2.3\"  snapshot . transaction_id   =   1000",
            "title": "SnapshotDefinition Class"
        },
        {
            "location": "/References/Glossary/",
            "text": "Glossary\n\u00b6\n\n\nAutomatic Discovery\n\u00b6\n\n\nDiscovery\n which is done by the Delphix Engine (with help from a plugin) itself, with no need for the end user to provide any information.\n\n\nCallback\n\u00b6\n\n\nA Python function that is provided by the Delphix Engine. Plugins use callbacks to request that the Delphix Engine do some task on behalf of the plugin. For example, running a Bash command on an environment, or making an log entry.\n\n\nDecorator\n\u00b6\n\n\nA Python construct which is used by plugins to \"tag\" certain functions, so that the Delphix Engine knows which function corresponds to which plugin operation.\n\n\nDirect Linking/Syncing\n\u00b6\n\n\nA strategy that involves data being ingested directly from the source environment onto the Delphix Engine, without the assistance of a staging environment.\n\n\nDiscovery\n\u00b6\n\n\nThe process by which the Delphix Engine learns about how a particular environment can be used for ingesting or virtualizing datasets.\n\n\ndSource\n\u00b6\n\n\nSee [Linked Dataset] (#linked-dataset)\n\n\nEnvironment\n\u00b6\n\n\nA remote system that the Delphix Engine can interact with. An environment can be used as a \nsource\n, \nstaging\n or \ntarget\n environment (or any combination of those).  For example, a Linux machine that the Delphix Engine can connect to is an environment.\n\n\nLinked Dataset\n\u00b6\n\n\nA dataset on the Delphix Engine which holds an ingested copy of a pre-existing external dataset from a source environment. A linked dataset is often called a \ndSource\n.\n\n\nLinked Source\n\u00b6\n\n\nAn object on the Delphix Engine that holds information related to a \nlinked dataset\n.\n\n\nLinking\n\u00b6\n\n\nThe process by which the Delphix Engine connects a new \ndSource\n to a pre-existing dataset on a source environment.\n\n\nLogging\n\u00b6\n\n\nLogging is when a plugin writes out some human-readable information to a log file. The log file can then be examined, typically in order to debug a problem with the plugin.\n\n\nManual Discovery\n\u00b6\n\n\nDiscovery\n which the end user does by manually entering the necessary information into the Delphix Engine.\n\n\nMount Specification\n\u00b6\n\n\nA collection of information, provided by the plugin, which give all the details about how and where \nvirtual datasets\n should be mounted onto \ntarget environments\n. This term is often shortened to \"Mount Spec\".\n\n\nPassword Properties\n\u00b6\n\n\nIn \nschemas\n, any string property can be tagged with \n\"format\": \"password\"\n. This will let the Delphix Engine know that the property contains sensitive information. Any such values will only be stored in encrypted format, and the UI will not display the values on screen.\n\n\nPlugin\n\u00b6\n\n\nA tool that customizes the Delphix Engine so it knows how to interact with a particular kind of dataset.\n\n\nPlugin Operation\n\u00b6\n\n\nA piece of functionality that provided by a plugin in order to customize Delphix Engine behavior to work with a particular kind of dataset. A plugin operation is implemented as a Python function.\nFor example, a MySQL plugin might provide an operation called \"stop\" which knows how to stop a MySQL database.\n\n\nProvisioning\n\u00b6\n\n\nThe process of making a virtual copy of a dataset and making it available for use on a target environment.\n\n\nRepository\n\u00b6\n\n\nInformation that represents a set of dependencies that a dataset requires in order to be functional. For example, a particular Postgres database might require an installed Postgres 9.6 DBMS, and so its associated repository would contain all the information required to interact with that DBMS.\n\n\nSchema\n\u00b6\n\n\nA formal description of a data type. Plugins use JSON format for their \nschemas\n.\n\n\nSnapshot\n\u00b6\n\n\nRepresents a snapshot of a dataset and its associated metadata represented by the \nSnapshotDefinition Schema\n\n\nSnaphot\n\u00b6\n\n\nA point-in-time read-only copy of a dataset.\n\n\nSource Config\n\u00b6\n\n\nA collection of information that the Delphix Engine needs to interact with a dataset (whether \nlinked\n or \nvirtual\n on an environment.\n\n\nSource Environment\n\u00b6\n\n\nAn environment containing data that is ingested by the Delphix Engine.\n\n\nStaged Linking/Syncing\n\u00b6\n\n\nA strategy where a \nstaging environment\n is used to coordinate the ingestion of data into a \ndsource\n.\n\n\nStaging Environment\n\u00b6\n\n\nAn \nenvironment\n used by the Delphix Engine to coordinate ingestion from a \nsource environment\n.\n\n\nSyncing\n\u00b6\n\n\nThe process by which the Delphix Engine ingests data from a dataset on a \nsource environment\n into a \ndsource\n. Syncing always happens immediately after \nlinking\n, and typically is done periodically thereafter.\n\n\nTarget Environment\n\u00b6\n\n\nAn \nenvironment\n on which Delphix-provided virtualized datasets can be used.\n\n\nUpgrade Operation\n\u00b6\n\n\nA special plugin operation that takes data produced by an older version of a plugin, and transforms it into the format expected by the new version of the plugin.\n\n\nVDB\n\u00b6\n\n\nSee \nVirtual Dataset\n\n\nVersion\n\u00b6\n\n\nA string identifier that is unique for every public release of a plugin.\n\n\nVirtual Dataset\n\u00b6\n\n\nA dataset that has been cloned from a snapshot, and whose data is stored on the Delphix Engine. A virtual dataset is made available for use by mounting it to a \ntarget environment\n. A virtual dataset is often called a \"VDB\".\n\n\nVirtual Source\n\u00b6\n\n\nAn object on the Delphix Engine that holds information related to a \nvirtual dataset\n.",
            "title": "Glossary"
        },
        {
            "location": "/References/Glossary/#glossary",
            "text": "",
            "title": "Glossary"
        },
        {
            "location": "/References/Glossary/#automatic-discovery",
            "text": "Discovery  which is done by the Delphix Engine (with help from a plugin) itself, with no need for the end user to provide any information.",
            "title": "Automatic Discovery"
        },
        {
            "location": "/References/Glossary/#callback",
            "text": "A Python function that is provided by the Delphix Engine. Plugins use callbacks to request that the Delphix Engine do some task on behalf of the plugin. For example, running a Bash command on an environment, or making an log entry.",
            "title": "Callback"
        },
        {
            "location": "/References/Glossary/#decorator",
            "text": "A Python construct which is used by plugins to \"tag\" certain functions, so that the Delphix Engine knows which function corresponds to which plugin operation.",
            "title": "Decorator"
        },
        {
            "location": "/References/Glossary/#direct-linkingsyncing",
            "text": "A strategy that involves data being ingested directly from the source environment onto the Delphix Engine, without the assistance of a staging environment.",
            "title": "Direct Linking/Syncing"
        },
        {
            "location": "/References/Glossary/#discovery",
            "text": "The process by which the Delphix Engine learns about how a particular environment can be used for ingesting or virtualizing datasets.",
            "title": "Discovery"
        },
        {
            "location": "/References/Glossary/#dsource",
            "text": "See [Linked Dataset] (#linked-dataset)",
            "title": "dSource"
        },
        {
            "location": "/References/Glossary/#environment",
            "text": "A remote system that the Delphix Engine can interact with. An environment can be used as a  source ,  staging  or  target  environment (or any combination of those).  For example, a Linux machine that the Delphix Engine can connect to is an environment.",
            "title": "Environment"
        },
        {
            "location": "/References/Glossary/#linked-dataset",
            "text": "A dataset on the Delphix Engine which holds an ingested copy of a pre-existing external dataset from a source environment. A linked dataset is often called a  dSource .",
            "title": "Linked Dataset"
        },
        {
            "location": "/References/Glossary/#linked-source",
            "text": "An object on the Delphix Engine that holds information related to a  linked dataset .",
            "title": "Linked Source"
        },
        {
            "location": "/References/Glossary/#linking",
            "text": "The process by which the Delphix Engine connects a new  dSource  to a pre-existing dataset on a source environment.",
            "title": "Linking"
        },
        {
            "location": "/References/Glossary/#logging",
            "text": "Logging is when a plugin writes out some human-readable information to a log file. The log file can then be examined, typically in order to debug a problem with the plugin.",
            "title": "Logging"
        },
        {
            "location": "/References/Glossary/#manual-discovery",
            "text": "Discovery  which the end user does by manually entering the necessary information into the Delphix Engine.",
            "title": "Manual Discovery"
        },
        {
            "location": "/References/Glossary/#mount-specification",
            "text": "A collection of information, provided by the plugin, which give all the details about how and where  virtual datasets  should be mounted onto  target environments . This term is often shortened to \"Mount Spec\".",
            "title": "Mount Specification"
        },
        {
            "location": "/References/Glossary/#password-properties",
            "text": "In  schemas , any string property can be tagged with  \"format\": \"password\" . This will let the Delphix Engine know that the property contains sensitive information. Any such values will only be stored in encrypted format, and the UI will not display the values on screen.",
            "title": "Password Properties"
        },
        {
            "location": "/References/Glossary/#plugin",
            "text": "A tool that customizes the Delphix Engine so it knows how to interact with a particular kind of dataset.",
            "title": "Plugin"
        },
        {
            "location": "/References/Glossary/#plugin-operation",
            "text": "A piece of functionality that provided by a plugin in order to customize Delphix Engine behavior to work with a particular kind of dataset. A plugin operation is implemented as a Python function.\nFor example, a MySQL plugin might provide an operation called \"stop\" which knows how to stop a MySQL database.",
            "title": "Plugin Operation"
        },
        {
            "location": "/References/Glossary/#provisioning",
            "text": "The process of making a virtual copy of a dataset and making it available for use on a target environment.",
            "title": "Provisioning"
        },
        {
            "location": "/References/Glossary/#repository",
            "text": "Information that represents a set of dependencies that a dataset requires in order to be functional. For example, a particular Postgres database might require an installed Postgres 9.6 DBMS, and so its associated repository would contain all the information required to interact with that DBMS.",
            "title": "Repository"
        },
        {
            "location": "/References/Glossary/#schema",
            "text": "A formal description of a data type. Plugins use JSON format for their  schemas .",
            "title": "Schema"
        },
        {
            "location": "/References/Glossary/#snapshot",
            "text": "Represents a snapshot of a dataset and its associated metadata represented by the  SnapshotDefinition Schema",
            "title": "Snapshot"
        },
        {
            "location": "/References/Glossary/#snaphot",
            "text": "A point-in-time read-only copy of a dataset.",
            "title": "Snaphot"
        },
        {
            "location": "/References/Glossary/#source-config",
            "text": "A collection of information that the Delphix Engine needs to interact with a dataset (whether  linked  or  virtual  on an environment.",
            "title": "Source Config"
        },
        {
            "location": "/References/Glossary/#source-environment",
            "text": "An environment containing data that is ingested by the Delphix Engine.",
            "title": "Source Environment"
        },
        {
            "location": "/References/Glossary/#staged-linkingsyncing",
            "text": "A strategy where a  staging environment  is used to coordinate the ingestion of data into a  dsource .",
            "title": "Staged Linking/Syncing"
        },
        {
            "location": "/References/Glossary/#staging-environment",
            "text": "An  environment  used by the Delphix Engine to coordinate ingestion from a  source environment .",
            "title": "Staging Environment"
        },
        {
            "location": "/References/Glossary/#syncing",
            "text": "The process by which the Delphix Engine ingests data from a dataset on a  source environment  into a  dsource . Syncing always happens immediately after  linking , and typically is done periodically thereafter.",
            "title": "Syncing"
        },
        {
            "location": "/References/Glossary/#target-environment",
            "text": "An  environment  on which Delphix-provided virtualized datasets can be used.",
            "title": "Target Environment"
        },
        {
            "location": "/References/Glossary/#upgrade-operation",
            "text": "A special plugin operation that takes data produced by an older version of a plugin, and transforms it into the format expected by the new version of the plugin.",
            "title": "Upgrade Operation"
        },
        {
            "location": "/References/Glossary/#vdb",
            "text": "See  Virtual Dataset",
            "title": "VDB"
        },
        {
            "location": "/References/Glossary/#version",
            "text": "A string identifier that is unique for every public release of a plugin.",
            "title": "Version"
        },
        {
            "location": "/References/Glossary/#virtual-dataset",
            "text": "A dataset that has been cloned from a snapshot, and whose data is stored on the Delphix Engine. A virtual dataset is made available for use by mounting it to a  target environment . A virtual dataset is often called a \"VDB\".",
            "title": "Virtual Dataset"
        },
        {
            "location": "/References/Glossary/#virtual-source",
            "text": "An object on the Delphix Engine that holds information related to a  virtual dataset .",
            "title": "Virtual Source"
        },
        {
            "location": "/References/Plugin_Versioning/",
            "text": "Plugin Versioning\n\u00b6\n\n\nLike any other piece of software, a plugin will change over time. New features will be added, bugs will be fixed, and so on.\n\n\nTo keep track of this, plugins must specify a version string. There are rules about what sorts of plugin changes go along with changes to the version string. Before we get into the rules, let's talk about a problem that we want to avoid.\n\n\nProblems With Data Format Mismatches\n\u00b6\n\n\nPlugins supply \nschemas\n to define their own datatypes. Data that conforms to these schemas is saved by the Delphix Engine. Later, the Delphix Engine may read back that saved data, and provide it to plugin code.\n\n\nImagine this sequence of events:\n\n\n\n\nA plugin is initially released. In its snapshot schema, it defines two properties, \ndate\n and \ntime\n, that together specify when the snapshot was taken.\n\n\nA user installs the initial release of the plugin on their Delphix Engine.\n\n\nThe user takes a snapshot of a \ndSource\n. Along with this snapshot is stored the \ndate\n and \ntime\n.\n\n\nA new version of the same plugin is released. In this new version, the snapshot schema now only defines a single property called \ntimestamp\n, which specified both the date and the time together in a single property.\n\n\nThe user installs the new plugin version.\n\n\nThe user attempts to \nprovision\n a new \nVDB\n from the snapshot they took in step 3.\n\n\n\n\nNow, when provision-related plugin code is called (for example the \nconfigure\n operation), it is going to be handed the snapshot data that was stored in step 2.\n\n\nThe problem here is that we'll have a data format mismatch. The previously-saved snapshot data will have separate \ndate\n and \ntime\n fields, but the new plugin code will be expecting instead a single field called \ntimestamp\n.\n\n\nData Upgrading\n\u00b6\n\n\nThe solution to this problem is for the plugin to provide \nupgrade operations\n.\n\n\nTODO: UPDATE THIS SECTION WHEN UPGRADE SCRIPT FORMAT ARE FINALIZED\n\n\nVersioning Rules\n\u00b6\n\n\nEach plugin declares a version string in the format \n<major>.<minor>.<patch>\n. The \nmajor\n and \nminor\n parts must always be integers, but \npatch\n can be any alphanumeric string.\n\n\nThere are two scenarios where one version of a plugin can be installed on an engine that already has another version of the same plugin installed.\n\n\nPatch-only Changes\n\u00b6\n\n\nIf only the \npatch\n part of the version is changing, there are a relaxed set of rules:\n\n\n\n\nSchemas \nmay not\n change.\n\n\nThere is no defined ordering for patches. So long as \nmajor\n and \nminor\n do not change, any patch level can replace any other patch level.\n\n\n\n\nMajor/Minor Changes\n\u00b6\n\n\nIf either \nmajor\n or \nminor\n (or both) is changing, then the following rules are applied:\n\n\n\n\nThe major/minor pair \nmay not\n decrease. If you have version \n1.2.x\n already installed, then for example you can install \n1.3.y\n or \n2.0.y\n. But, you are not allowed to \"downgrade\" to version \n1.1.z\n.\n\n\nSchemas \nmay\n change.\n\n\nThe plugin \nmust\n provide upgrade operations so that old-format data can be converted as necessary.",
            "title": "Plugin Versioning"
        },
        {
            "location": "/References/Plugin_Versioning/#plugin-versioning",
            "text": "Like any other piece of software, a plugin will change over time. New features will be added, bugs will be fixed, and so on.  To keep track of this, plugins must specify a version string. There are rules about what sorts of plugin changes go along with changes to the version string. Before we get into the rules, let's talk about a problem that we want to avoid.",
            "title": "Plugin Versioning"
        },
        {
            "location": "/References/Plugin_Versioning/#problems-with-data-format-mismatches",
            "text": "Plugins supply  schemas  to define their own datatypes. Data that conforms to these schemas is saved by the Delphix Engine. Later, the Delphix Engine may read back that saved data, and provide it to plugin code.  Imagine this sequence of events:   A plugin is initially released. In its snapshot schema, it defines two properties,  date  and  time , that together specify when the snapshot was taken.  A user installs the initial release of the plugin on their Delphix Engine.  The user takes a snapshot of a  dSource . Along with this snapshot is stored the  date  and  time .  A new version of the same plugin is released. In this new version, the snapshot schema now only defines a single property called  timestamp , which specified both the date and the time together in a single property.  The user installs the new plugin version.  The user attempts to  provision  a new  VDB  from the snapshot they took in step 3.   Now, when provision-related plugin code is called (for example the  configure  operation), it is going to be handed the snapshot data that was stored in step 2.  The problem here is that we'll have a data format mismatch. The previously-saved snapshot data will have separate  date  and  time  fields, but the new plugin code will be expecting instead a single field called  timestamp .",
            "title": "Problems With Data Format Mismatches"
        },
        {
            "location": "/References/Plugin_Versioning/#data-upgrading",
            "text": "The solution to this problem is for the plugin to provide  upgrade operations .  TODO: UPDATE THIS SECTION WHEN UPGRADE SCRIPT FORMAT ARE FINALIZED",
            "title": "Data Upgrading"
        },
        {
            "location": "/References/Plugin_Versioning/#versioning-rules",
            "text": "Each plugin declares a version string in the format  <major>.<minor>.<patch> . The  major  and  minor  parts must always be integers, but  patch  can be any alphanumeric string.  There are two scenarios where one version of a plugin can be installed on an engine that already has another version of the same plugin installed.",
            "title": "Versioning Rules"
        },
        {
            "location": "/References/Plugin_Versioning/#patch-only-changes",
            "text": "If only the  patch  part of the version is changing, there are a relaxed set of rules:   Schemas  may not  change.  There is no defined ordering for patches. So long as  major  and  minor  do not change, any patch level can replace any other patch level.",
            "title": "Patch-only Changes"
        },
        {
            "location": "/References/Plugin_Versioning/#majorminor-changes",
            "text": "If either  major  or  minor  (or both) is changing, then the following rules are applied:   The major/minor pair  may not  decrease. If you have version  1.2.x  already installed, then for example you can install  1.3.y  or  2.0.y . But, you are not allowed to \"downgrade\" to version  1.1.z .  Schemas  may  change.  The plugin  must  provide upgrade operations so that old-format data can be converted as necessary.",
            "title": "Major/Minor Changes"
        },
        {
            "location": "/References/Logging/",
            "text": "Logging\n\u00b6\n\n\nWhat is logging?\n\u00b6\n\n\nThe Delphix Engine keeps plugin-specific log files. A plugin can, at any point in any of its \noperations\n, write out some text to its log file(s). These log files can be examined later, typically to try to debug a problem with the plugin.\n\n\nHow to log\n\u00b6\n\n\nThe Delphix engine provides \ncallbacks\n that plugins can use to write to log files.\n\n\nMaking a log entry is easy, and looks like this:\n\n\n \ndlpx\n.\nlogDebug\n(\n\"Here is the text that goes in the log file\"\n)\n\n\n\n\n\n\nInside the log file, you'll be able to see not only your text, but also a timestamp.\n\n\nWhy log?\n\u00b6\n\n\nLogging is typically done to enable your plugin to be more easily debugged.\n\n\nExample\n\u00b6\n\n\nImagine you notice that your plugin is taking a very long time to do discovery. Everything works, it just takes much longer than expected. You'd like to figure out why.\n\n\nSuppose your plugin has a discovery operation that looks like this (code is abbreviated to be easier to follow):\n\n\ndef\n \nmy_operation\n(\nresources\n):\n\n  \nversion_result\n \n=\n \ndlpx\n.\nrun_bash\n(\ncommand\n=\nresources\n[\n\"get_db_version\"\n])\n\n  \nusers_result\n \n=\n \ndlpx\n.\nrun_bash\n(\ncommand\n=\nresources\n[\n\"get_db_users\"\n])\n\n  \ndb_results\n \n=\n \ndlpx\n.\nrun_bash\n(\ncommand\n=\nresources\n[\n\"get_databases\"\n])\n\n  \nstatus_result\n \n=\n \ndlpx\n.\nrun_bash\n(\ncommand\n=\nresources\n[\n\"get_database_statuses\"\n])\n\n  \n# Later, do something will all these results\n\n\n\n\n\n\nNow, imagine that you notice that it's taking a long time to do discovery, and you'd like to try to figure out why. One thing that might help is to add logging, like this:\n\n\ndef\n \nmy_operation\n(\nresources\n):\n\n  \ndlpx\n.\nlogDebug\n(\n\"About to get DB version\"\n)\n\n  \nversion_result\n \n=\n \ndlpx\n.\nrun_bash\n(\ncommand\n=\nresources\n[\n\"get_db_version\"\n])\n\n  \ndlpx\n.\nlogDebug\n(\n\"About to get DB users\"\n)\n\n  \nusers_result\n \n=\n \ndlpx\n.\nrun_bash\n(\ncommand\n=\nresources\n[\n\"get_db_users\"\n])\n\n  \ndlpx\n.\nlogDebug\n(\n\"About to get databases\"\n)\n\n  \ndb_results\n \n=\n \ndlpx\n.\nrun_bash\n(\ncommand\n=\nresources\n[\n\"get_databases\"\n])\n\n  \ndlpx\n.\nlogDebug\n(\n\"About to get DB statuses\"\n)\n\n  \nstatus_result\n \n=\n \ndlpx\n.\nrun_bash\n(\ncommand\n=\nresources\n[\n\"get_database_statuses\"\n])\n\n  \ndlpx\n.\nlogDebug\n(\n\"Done collecting data\"\n)\n\n\n\n\n\n\nWhen you look at the log file, perhaps you'll see something like this:\n\n\nTODO: Convert this to actual format used in our log files\n\n\n2019-03-01T09:22:17|About to get DB version\n2019-03-01T09:22:19|About to get DB users\n2019-03-01T09:35:41|About to get databases\n2019-03-01T09:35:44|About to get DB statuses\n2019-03-01T09:35:49|Done collecting data\n\n\n\n\n\nYou can see that it only takes a few seconds for us do each of our data collection steps, with the exception of getting the users, which takes over 13 minutes!\n\n\nWe now know that our slowdown is something to do with how our bash script is collecting all the users. Logging has gotten us a lot closer to figuring out the problem.\n\n\nHow to retrieve logs\n\u00b6\n\n\nTODO: Add content here after we have a firm process for doing this (there may be a UI coming soon?)\n\n\nLogging Levels\n\u00b6\n\n\nWe already looked at an example of the \nlogDebug\n callback. There are two more: \nlogInfo\n and \nlogError\n. Both work exactly the same way.\n\n\nEach of these three logging levels has its own log file. Thus, calling \nlogError\n will write to an \nerror.log\n file.\n\n\nThese levels are also hierarchical:\n\nlogError\n writes only to \nerror.log\n.\n\nlogInfo\n writes to \ninfo.log\n and \nerror.log\n.\n\nlogDebug\n writes to \ndebug.log\n, \ninfo.log\n, and \nerror.log\n.\n\n\nlogError\n is meant to be used for cases where you know for sure there is a problem. The distinction between what counts as \"info\" and what counts as \"debug\" is entirely up to you. Some plugins will pick one of them and use it exclusively. Some will decide to count only the most important messages as \"info\", with everything else counting as \"debug\".\n\n\nSensitive data\n\u00b6\n\n\nRemember that logging data means writing that data out in cleartext. Make sure you never log any data that could be secret or sensitive (passwords, etc.). For more details please see our section on \nsensitive data",
            "title": "Logging"
        },
        {
            "location": "/References/Logging/#logging",
            "text": "",
            "title": "Logging"
        },
        {
            "location": "/References/Logging/#what-is-logging",
            "text": "The Delphix Engine keeps plugin-specific log files. A plugin can, at any point in any of its  operations , write out some text to its log file(s). These log files can be examined later, typically to try to debug a problem with the plugin.",
            "title": "What is logging?"
        },
        {
            "location": "/References/Logging/#how-to-log",
            "text": "The Delphix engine provides  callbacks  that plugins can use to write to log files.  Making a log entry is easy, and looks like this:    dlpx . logDebug ( \"Here is the text that goes in the log file\" )   Inside the log file, you'll be able to see not only your text, but also a timestamp.",
            "title": "How to log"
        },
        {
            "location": "/References/Logging/#why-log",
            "text": "Logging is typically done to enable your plugin to be more easily debugged.",
            "title": "Why log?"
        },
        {
            "location": "/References/Logging/#example",
            "text": "Imagine you notice that your plugin is taking a very long time to do discovery. Everything works, it just takes much longer than expected. You'd like to figure out why.  Suppose your plugin has a discovery operation that looks like this (code is abbreviated to be easier to follow):  def   my_operation ( resources ): \n   version_result   =   dlpx . run_bash ( command = resources [ \"get_db_version\" ]) \n   users_result   =   dlpx . run_bash ( command = resources [ \"get_db_users\" ]) \n   db_results   =   dlpx . run_bash ( command = resources [ \"get_databases\" ]) \n   status_result   =   dlpx . run_bash ( command = resources [ \"get_database_statuses\" ]) \n   # Later, do something will all these results   Now, imagine that you notice that it's taking a long time to do discovery, and you'd like to try to figure out why. One thing that might help is to add logging, like this:  def   my_operation ( resources ): \n   dlpx . logDebug ( \"About to get DB version\" ) \n   version_result   =   dlpx . run_bash ( command = resources [ \"get_db_version\" ]) \n   dlpx . logDebug ( \"About to get DB users\" ) \n   users_result   =   dlpx . run_bash ( command = resources [ \"get_db_users\" ]) \n   dlpx . logDebug ( \"About to get databases\" ) \n   db_results   =   dlpx . run_bash ( command = resources [ \"get_databases\" ]) \n   dlpx . logDebug ( \"About to get DB statuses\" ) \n   status_result   =   dlpx . run_bash ( command = resources [ \"get_database_statuses\" ]) \n   dlpx . logDebug ( \"Done collecting data\" )   When you look at the log file, perhaps you'll see something like this:  TODO: Convert this to actual format used in our log files  2019-03-01T09:22:17|About to get DB version\n2019-03-01T09:22:19|About to get DB users\n2019-03-01T09:35:41|About to get databases\n2019-03-01T09:35:44|About to get DB statuses\n2019-03-01T09:35:49|Done collecting data  You can see that it only takes a few seconds for us do each of our data collection steps, with the exception of getting the users, which takes over 13 minutes!  We now know that our slowdown is something to do with how our bash script is collecting all the users. Logging has gotten us a lot closer to figuring out the problem.",
            "title": "Example"
        },
        {
            "location": "/References/Logging/#how-to-retrieve-logs",
            "text": "TODO: Add content here after we have a firm process for doing this (there may be a UI coming soon?)",
            "title": "How to retrieve logs"
        },
        {
            "location": "/References/Logging/#logging-levels",
            "text": "We already looked at an example of the  logDebug  callback. There are two more:  logInfo  and  logError . Both work exactly the same way.  Each of these three logging levels has its own log file. Thus, calling  logError  will write to an  error.log  file.  These levels are also hierarchical: logError  writes only to  error.log . logInfo  writes to  info.log  and  error.log . logDebug  writes to  debug.log ,  info.log , and  error.log .  logError  is meant to be used for cases where you know for sure there is a problem. The distinction between what counts as \"info\" and what counts as \"debug\" is entirely up to you. Some plugins will pick one of them and use it exclusively. Some will decide to count only the most important messages as \"info\", with everything else counting as \"debug\".",
            "title": "Logging Levels"
        },
        {
            "location": "/References/Logging/#sensitive-data",
            "text": "Remember that logging data means writing that data out in cleartext. Make sure you never log any data that could be secret or sensitive (passwords, etc.). For more details please see our section on  sensitive data",
            "title": "Sensitive data"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/",
            "text": "Dealing With Sensitive Data\n\u00b6\n\n\nOften, a plugin will need to handle sensitive user-provided data. The most common example of this is a database password.\n\n\nPlugins must be careful to handle such sensitive data appropriately. Three tips for handling such data are:\n\n\n\n\nTell the Delphix Engine which parts of your data are sensitive.\n\n\nWhen passing sensitive data to remote callbacks (such as \nRunBash\n), use environment variables.\n\n\nAvoid logging, or otherwise writing out the sensitive data.\n\n\n\n\nEach of these tips are explained below.\n\n\nMarking Your Data As Sensitive\n\u00b6\n\n\nBecause the Delphix Engine mangages the storing and retreiving of plugin-defined data, it needs to know which pieces of data are sensitive. The plugin does this in its \nschemas\n, by using the special \npassword\n keyword.\n\n\nHere is an example of a schema that defines an object with three properties, one of which is sensitive and tagged with the \npassword\n keyword:\n\n\n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"db_connectionPort\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n},\n\n        \n\"db_username\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n},\n\n        \n\"db_password\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n,\n \n\"format\"\n:\n \n\"password\"\n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nThis tells the Delphix Engine to take special precautions with this password property, as follows:\n\n\n\n\nThe Delphix Engine will encrypt the password before storing it, and decrypt it only as necessary to pass pack to the plugin.\n\n\nThe Delphix Engine will not write this password anywhere (for example, it will not appear in any system logs)\n\n\nThe Delphix Engine's UI and CLI will not display the password.\n\n\nNo clients of the Delphix Engine's public API will be able to access the password.\n\n\n\n\nUsing Environment Variables For Remote Data Passing\n\u00b6\n\n\nSometimes, a plugin will need to pass sensitive data to a remote environment. For example, perhaps a database command needs to be run on a \nstaging environment\n, and that database command will need to use a password.\n\n\nExample\n\u00b6\n\n\nLet's take a look at a very simple example. Let's suppose we need to shutdown a database called \"inventory\" on a target environment. Further, suppose that the command to do so is \ndb_cmd shutdown inventory\n. Finally, suppose that command will ask for a password on \nstdin\n, and that our password is \"hunter2\".\n\n\nIf we were running this command by hand, it might look like this:\n\n\n$ db_cmd shutdown inventory\nConnecting to database instance...\nPlease enter database password:\n\n\n\n\n\nAt this point, we'd type in \"hunter2\", and the command would proceed to shut down the database.\n\n\nSince a plugin cannot type in the password by hand, it will have to do something like this instead:\n\n\n$ \necho\n \n\"hunter2\"\n \n|\n db_cmd shutdown inventory\n\n\n\n\n\nDon't Do This\n\u00b6\n\n\nFirst, let's take a look at how \nnot\n to do this! Here is a bit of plugin python code that will run the above command.\n\n\n# THIS IS INSECURE! DO NOT DO THIS!\n\n\nfull_command\n \n=\n \n\"echo {} | db_cmd shutdown {}\"\n.\nformat\n(\npassword\n,\n \ndb_name\n)\n\n\ndlpx\n.\nrun_bash\n(\nenv\n \n=\n \ntarget_env\n,\n \ncmd\n \n=\n \nfull_command\n,\n \nuser\n \n=\n \ntarget_user\n)\n\n\n\n\n\n\nThis will construct a Python string containing exactly the desired command from above. However, this is not recommended.\n\n\nThe problem here is that there is a cleartext password in the Python string. But, this Python string is not treated as sensitive by the Delphix Engine. For example, suppose the Delphix Engine cannot make a connection to the target environment. In that case, it will raise an error containing the Python string, so that people will know what command failed. But, in our example, that would result in the password being part of the cleartext error message!\n\n\nUsing Environment Variables\n\u00b6\n\n\nThe Delphix Engine provides a better way to pass sensitive data to remote bash (or powershell) calls: environment variables. Let's look at a different way to run the same command as above.\n\n\n# Use environment variables to pass sensitive data to remote commands\n\n\nenvironment_vars\n \n=\n \n{\n\n  \n\"DATABASE_PASSWORD\"\n \n:\n \npassword\n\n\n}\n\n\nfull_command\n \n=\n \n\"echo $DATABASE_PASSWORD | db_cmd shutdown {}\"\n.\nformat\n(\ndb_name\n)\n\n\ndlpx\n.\nrun_bash\n(\nenv\n \n=\n \ntarget_env\n,\n \ncmd\n \n=\n \nfull_command\n,\n \nuser\n \n=\n \ntarget_user\n,\n \nvars\n=\nenvironment_vars\n)\n\n\n\n\n\n\nNote that we are no longer putting the cleartext password into the Python command string. Instead, we are instructing the Delphix Engine to put the password into an environment variable on the target environment. The Python command string merely mentions the name of the environment variable, and does not contain the password itself.\n\n\nOnce the command runs on the target environment, Bash will substitute in the password, and the database shutdown will run as expected.\n\n\nUnlike with the command string, the Delphix Engine \ndoes\n treat environment variables as sensitive information, and will not include them in error messages or internal logs, etc.\n\n\nDon't Write Out Sensitive Data\n\u00b6\n\n\nPlugin writers are strongly advised to never write out unencrypted sensitive data. This is common-sense general advice that applies to all areas of programming, not just for plugins. However, there are a couple of special concerns for plugins.\n\n\nThe Delphix Engine provides logging capabilities to plugins. The generated logs are unencrypted and not treated as sensitive. Therefore, it is important for plugins to \nnever log sensitive data\n.\n\n\nIn addition, remember that your plugin itself is not treated as sensitive by the Delphix Engine. Plugin code is distributed unencrypted, and is viewable in cleartext by users of the Delphix Engine. So, you should not hard-code anything sensitive, like passwords, in your plugin code.",
            "title": "Dealing With Sensitive Data"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#dealing-with-sensitive-data",
            "text": "Often, a plugin will need to handle sensitive user-provided data. The most common example of this is a database password.  Plugins must be careful to handle such sensitive data appropriately. Three tips for handling such data are:   Tell the Delphix Engine which parts of your data are sensitive.  When passing sensitive data to remote callbacks (such as  RunBash ), use environment variables.  Avoid logging, or otherwise writing out the sensitive data.   Each of these tips are explained below.",
            "title": "Dealing With Sensitive Data"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#marking-your-data-as-sensitive",
            "text": "Because the Delphix Engine mangages the storing and retreiving of plugin-defined data, it needs to know which pieces of data are sensitive. The plugin does this in its  schemas , by using the special  password  keyword.  Here is an example of a schema that defines an object with three properties, one of which is sensitive and tagged with the  password  keyword:  { \n     \"type\" :   \"object\" , \n     \"properties\" :   { \n         \"db_connectionPort\" :   { \"type\" :   \"string\" }, \n         \"db_username\" :   { \"type\" :   \"string\" }, \n         \"db_password\" :   { \"type\" :   \"string\" ,   \"format\" :   \"password\" } \n     }  }   This tells the Delphix Engine to take special precautions with this password property, as follows:   The Delphix Engine will encrypt the password before storing it, and decrypt it only as necessary to pass pack to the plugin.  The Delphix Engine will not write this password anywhere (for example, it will not appear in any system logs)  The Delphix Engine's UI and CLI will not display the password.  No clients of the Delphix Engine's public API will be able to access the password.",
            "title": "Marking Your Data As Sensitive"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#using-environment-variables-for-remote-data-passing",
            "text": "Sometimes, a plugin will need to pass sensitive data to a remote environment. For example, perhaps a database command needs to be run on a  staging environment , and that database command will need to use a password.",
            "title": "Using Environment Variables For Remote Data Passing"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#example",
            "text": "Let's take a look at a very simple example. Let's suppose we need to shutdown a database called \"inventory\" on a target environment. Further, suppose that the command to do so is  db_cmd shutdown inventory . Finally, suppose that command will ask for a password on  stdin , and that our password is \"hunter2\".  If we were running this command by hand, it might look like this:  $ db_cmd shutdown inventory\nConnecting to database instance...\nPlease enter database password:  At this point, we'd type in \"hunter2\", and the command would proceed to shut down the database.  Since a plugin cannot type in the password by hand, it will have to do something like this instead:  $  echo   \"hunter2\"   |  db_cmd shutdown inventory",
            "title": "Example"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#dont-do-this",
            "text": "First, let's take a look at how  not  to do this! Here is a bit of plugin python code that will run the above command.  # THIS IS INSECURE! DO NOT DO THIS!  full_command   =   \"echo {} | db_cmd shutdown {}\" . format ( password ,   db_name )  dlpx . run_bash ( env   =   target_env ,   cmd   =   full_command ,   user   =   target_user )   This will construct a Python string containing exactly the desired command from above. However, this is not recommended.  The problem here is that there is a cleartext password in the Python string. But, this Python string is not treated as sensitive by the Delphix Engine. For example, suppose the Delphix Engine cannot make a connection to the target environment. In that case, it will raise an error containing the Python string, so that people will know what command failed. But, in our example, that would result in the password being part of the cleartext error message!",
            "title": "Don't Do This"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#using-environment-variables",
            "text": "The Delphix Engine provides a better way to pass sensitive data to remote bash (or powershell) calls: environment variables. Let's look at a different way to run the same command as above.  # Use environment variables to pass sensitive data to remote commands  environment_vars   =   { \n   \"DATABASE_PASSWORD\"   :   password  }  full_command   =   \"echo $DATABASE_PASSWORD | db_cmd shutdown {}\" . format ( db_name )  dlpx . run_bash ( env   =   target_env ,   cmd   =   full_command ,   user   =   target_user ,   vars = environment_vars )   Note that we are no longer putting the cleartext password into the Python command string. Instead, we are instructing the Delphix Engine to put the password into an environment variable on the target environment. The Python command string merely mentions the name of the environment variable, and does not contain the password itself.  Once the command runs on the target environment, Bash will substitute in the password, and the database shutdown will run as expected.  Unlike with the command string, the Delphix Engine  does  treat environment variables as sensitive information, and will not include them in error messages or internal logs, etc.",
            "title": "Using Environment Variables"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#dont-write-out-sensitive-data",
            "text": "Plugin writers are strongly advised to never write out unencrypted sensitive data. This is common-sense general advice that applies to all areas of programming, not just for plugins. However, there are a couple of special concerns for plugins.  The Delphix Engine provides logging capabilities to plugins. The generated logs are unencrypted and not treated as sensitive. Therefore, it is important for plugins to  never log sensitive data .  In addition, remember that your plugin itself is not treated as sensitive by the Delphix Engine. Plugin code is distributed unencrypted, and is viewable in cleartext by users of the Delphix Engine. So, you should not hard-code anything sensitive, like passwords, in your plugin code.",
            "title": "Don't Write Out Sensitive Data"
        }
    ]
}