{
    "docs": [
        {
            "location": "/",
            "text": "Welcome!\n\u00b6\n\n\nWith this Delphix Virtualization SDK documentation we hope you will find all you need to know in order to develop your own plugins!\n\n\nOverview\n\u00b6\n\n\nIf you already know about plugins, and are looking for something specific, use the links to the left to find what you are looking for or search.\n\n\nIf this is your first time here, and you are wondering what developing a Delphix plugin will do for you\u2014read on!\n\n\nWhat Does a Delphix Plugin do?\n\u00b6\n\n\nThe Delphix Engine is an appliance that lets you quickly and cheaply make \nvirtual copies\n of large datasets. The engine has built-in support for interfacing with certain types of datasets, such as Oracle, SQL Server and ASE.\n\n\nWhen you develop a plugin, you enable end users to use your dataset type as if they were using a built-in dataset type, whether it\u2019s MongoDB, Cassandra, or something else. Your plugin will extend the Delphix Engine\u2019s capabilities by teaching it how to run essential virtual data operations on your datasets:\n\n\n\n\nHow to stop and start them\n\n\nWhere to store their data\n\n\nHow to make virtual copies\n\n\n\n\nThese plugin operations are the building blocks of the Delphix Engine. From these building blocks, the engine can provide all of the normal Delphix functionality to the datasets you connect to such as:\n\n\n\n\nProvisioning\n\n\nRefreshing\n\n\nRewinding\n\n\nReplication\n\n\nSyncing\n\n\n\n\nWhere to Start\n\u00b6\n\n\nRead through the first few sections of this documentation, and we will walk you through how to get setup for development, then how to develop, build, and deploy your first plugin.\n\n\nGetting Started\n will show you how to setup the SDK. When you finish this section, you will have a full plugin development environment, and you will be ready to start building plugins.\n\n\nBuilding Your First Plugin\n will walk you step-by-step through the process of developing a very simple plugin. With it, you will learn the concepts and techniques that you will need to develop fully-fledged plugins. That does not mean this first plugin is useless\u2014you will be able to virtualize simple datasets with it.\n\n\nOnce you complete these sections, use the rest of the documentation whenever you would like.\n\n\nQuestions?\n\u00b6\n\n\nIf you have questions, bugs or feature requests reach out to us via the \nVirtualization SDK GitHub repository\n.",
            "title": "Welcome!"
        },
        {
            "location": "/#welcome",
            "text": "With this Delphix Virtualization SDK documentation we hope you will find all you need to know in order to develop your own plugins!",
            "title": "Welcome!"
        },
        {
            "location": "/#overview",
            "text": "If you already know about plugins, and are looking for something specific, use the links to the left to find what you are looking for or search.  If this is your first time here, and you are wondering what developing a Delphix plugin will do for you\u2014read on!",
            "title": "Overview"
        },
        {
            "location": "/#what-does-a-delphix-plugin-do",
            "text": "The Delphix Engine is an appliance that lets you quickly and cheaply make  virtual copies  of large datasets. The engine has built-in support for interfacing with certain types of datasets, such as Oracle, SQL Server and ASE.  When you develop a plugin, you enable end users to use your dataset type as if they were using a built-in dataset type, whether it\u2019s MongoDB, Cassandra, or something else. Your plugin will extend the Delphix Engine\u2019s capabilities by teaching it how to run essential virtual data operations on your datasets:   How to stop and start them  Where to store their data  How to make virtual copies   These plugin operations are the building blocks of the Delphix Engine. From these building blocks, the engine can provide all of the normal Delphix functionality to the datasets you connect to such as:   Provisioning  Refreshing  Rewinding  Replication  Syncing",
            "title": "What Does a Delphix Plugin do?"
        },
        {
            "location": "/#where-to-start",
            "text": "Read through the first few sections of this documentation, and we will walk you through how to get setup for development, then how to develop, build, and deploy your first plugin.  Getting Started  will show you how to setup the SDK. When you finish this section, you will have a full plugin development environment, and you will be ready to start building plugins.  Building Your First Plugin  will walk you step-by-step through the process of developing a very simple plugin. With it, you will learn the concepts and techniques that you will need to develop fully-fledged plugins. That does not mean this first plugin is useless\u2014you will be able to virtualize simple datasets with it.  Once you complete these sections, use the rest of the documentation whenever you would like.",
            "title": "Where to Start"
        },
        {
            "location": "/#questions",
            "text": "If you have questions, bugs or feature requests reach out to us via the  Virtualization SDK GitHub repository .",
            "title": "Questions?"
        },
        {
            "location": "/Getting_Started/",
            "text": "Getting Started\n\u00b6\n\n\nThe Virtualization SDK is a Python package on \nPyPI\n. Install it in your local development environment so that you can build and upload a plugin.\n\n\nThe SDK consists of three parts:\n\n\n\n\nThe \ndlpx.virtulization.platform\n module\n\n\nThe \ndlpx.virtualization.libs\n module\n\n\nA CLI\n\n\n\n\nThe platform and libs modules expose objects and methods needed to develop a plugin. The CLI is used to build and upload a plugin.\n\n\nRequirements\n\u00b6\n\n\n\n\nmacOS 10.14+, Ubuntu 16.04+, or Windows 10\n\n\nPython 2.7 (Python 3 is not supported)\n\n\nJava 7+\n\n\nDelphix Engine 6.0.2.0 or above\n\n\n\n\nInstallation\n\u00b6\n\n\nTo install the latest version of the SDK run:\n\n\n$ pip install dvp\n\n\n\n\n\n\n\nUse a Virtual Environment\n\n\nWe highly recommended that you develop plugins inside of a virtual environment. To learn more about virtual environments, refer to \nVirtualenv's documentation\n.\n\n\nThe virtual environment needs to use Python 2.7. This is configured when creating the virtualenv:\n\n\n$ virtualenv -p /path/to/python2.7/binary ENV\n\n\n\n\nTo install a specific version of the SDK run:\n\n\n$ pip install dvp==<version>\n\n\n\n\n\nTo upgrade an existing installation of the SDK run:\n\n\n$ pip install dvp --upgrade\n\n\n\n\n\n\n\nAPI Build Version\n\n\nThe version of the SDK defines the version of the Virtualization Platform API your plugin will be built against.\n\n\n\n\nBasic Usage\n\u00b6\n\n\nOur \nCLI reference\n describes commands, provides examples, and a help section.\n\n\nTo build your plugin:\n\n\n$ dvp build -c <plugin_config> -a <artifact_file>\n\n\n\n\n\nThis will generate an upload artifact at \n<artifact_file>\n. That file can then be uploaded with:\n\n\n$ dvp upload -e <delphix_engine_address> -u <delphix_admin_user> -a <artifact_file>\n\n\n\n\n\nYou will be prompted for the Delphix Engine user's password.\n\n\nYou can also use a \nCLI Configuration File\n to set default values for \nCLI\n command options.\n\n\nQuestions?\n\u00b6\n\n\nIf you have questions, bugs or feature requests reach out to us via the \nVirtualization SDK GitHub repository\n.",
            "title": "Getting Started"
        },
        {
            "location": "/Getting_Started/#getting-started",
            "text": "The Virtualization SDK is a Python package on  PyPI . Install it in your local development environment so that you can build and upload a plugin.  The SDK consists of three parts:   The  dlpx.virtulization.platform  module  The  dlpx.virtualization.libs  module  A CLI   The platform and libs modules expose objects and methods needed to develop a plugin. The CLI is used to build and upload a plugin.",
            "title": "Getting Started"
        },
        {
            "location": "/Getting_Started/#requirements",
            "text": "macOS 10.14+, Ubuntu 16.04+, or Windows 10  Python 2.7 (Python 3 is not supported)  Java 7+  Delphix Engine 6.0.2.0 or above",
            "title": "Requirements"
        },
        {
            "location": "/Getting_Started/#installation",
            "text": "To install the latest version of the SDK run:  $ pip install dvp   Use a Virtual Environment  We highly recommended that you develop plugins inside of a virtual environment. To learn more about virtual environments, refer to  Virtualenv's documentation .  The virtual environment needs to use Python 2.7. This is configured when creating the virtualenv:  $ virtualenv -p /path/to/python2.7/binary ENV   To install a specific version of the SDK run:  $ pip install dvp==<version>  To upgrade an existing installation of the SDK run:  $ pip install dvp --upgrade   API Build Version  The version of the SDK defines the version of the Virtualization Platform API your plugin will be built against.",
            "title": "Installation"
        },
        {
            "location": "/Getting_Started/#basic-usage",
            "text": "Our  CLI reference  describes commands, provides examples, and a help section.  To build your plugin:  $ dvp build -c <plugin_config> -a <artifact_file>  This will generate an upload artifact at  <artifact_file> . That file can then be uploaded with:  $ dvp upload -e <delphix_engine_address> -u <delphix_admin_user> -a <artifact_file>  You will be prompted for the Delphix Engine user's password.  You can also use a  CLI Configuration File  to set default values for  CLI  command options.",
            "title": "Basic Usage"
        },
        {
            "location": "/Getting_Started/#questions",
            "text": "If you have questions, bugs or feature requests reach out to us via the  Virtualization SDK GitHub repository .",
            "title": "Questions?"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/",
            "text": "Overview\n\u00b6\n\n\nIn the following few pages, we will walk through an example of making a simple, working plugin.\n\n\nOur plugin will virtualize simple directory trees on Unix systems. The actual contents of these directories could be anything: configuration files, documents, image libraries, etc. Our plugin will not care about the contents and will treat it as a directory tree full of files.\n\n\nData Flow in the Delphix Engine\n\u00b6\n\n\nHere we will briefly overview how data moves through the Delphix Engine.\n\n\nIngestion\n\u00b6\n\n\nIt all begins with Delphix ingesting data\u2014copying some data from what we call a \nsource environment\n  onto the Delphix Engine.\n\n\nPlugins can use either of two basic strategies to do this copying:\n\n\n\n\ndirect linking\n, where the Delphix Engine pulls data directly from the source environment.\n\n\nstaged linking\n, where the plugin is responsible for pulling data from the source environment.\n\n\n\n\nOur plugin will use the staged linking strategy.\n\n\nWith staged linking, Delphix exposes and mounts storage to a \nstaging environment\n.  This would be an NFS share for Unix environments and iSCSI disks for Windows environments. You can use either the source environment or a different environment for staging. We will write our plugin to handle both approaches.\n\n\nOnce Delphix mounts the storage share onto the staging environment, the plugin needs to arrange for the relevant data to be copied from the source environment onto the storage share, which is backed by Delphix Engine storage.\n\n\nWhen this initial copy is complete, Delphix will take a snapshot of the backing storage.\n\n\nThis same basic operation will be repeated when Delphix mounts an NFS share: The plugin copies data onto it, then Delphix snapshots the result.\n\n\nProvisioning\n\u00b6\n\n\nProvisioning\n is when you take a Delphix Engine snapshot and create a virtual dataset from it.\n\n\nFirst the snapshot is cloned onto the Delphix Engine, then this newly-cloned data is mounted as a virtual dataset onto a \ntarget environment\n. While this new virtual dataset gets updated by its end users, the original snapshot is persistent. You can use it in a few ways:\n\n\n\n\nProvision other virtual datasets from it\n\n\nRewind the virtual dataset back to the state it represents\n\n\nCreate a physical database from it in what we call V2P: Virtual to Physical\n\n\n\n\nParts of a Plugin\n\u00b6\n\n\nA plugin consists of three main parts. We will cover them briefly here, and then fill in more details later in the tutorial.\n\n\nPlugin Config\n\u00b6\n\n\nPlugin config is where the plugin describes itself to the Delphix Engine. What is the plugin called? What version of the plugin is being used? What type(s) of environments does the plugin work with? What features does the plugin offer?...\n\n\nPlugin Operations\n\u00b6\n\n\nThe plugin will need to provide operations. These are Python functions, each of which implements one small piece of functionality. This is how the plugin customizes Delphix behavior to work with the kind of dataset you\u2019re building the plugin for. One operation will handle setting up a newly-configured virtual dataset. Another will handle copying data from a source environment, and so on.\n\n\nLater we\u2019ll provide examples for our first plugin. See \nPlugin Operations\n for full details on the operations that are available, which are required, and what each one is required to do.\n\n\nSchemas\n\u00b6\n\n\nAs part of normal operations, plugins need to generate and access certain pieces of information in order to do their job. For example, plugins that work with Postgres might need to know which port number to connect to, or which credentials to use.\n\n\nDefining your plugin\u2019s schemas will enable it to give the Delphix Engine the details it needs to run the operations we\u2019ve built into it. Different datasets can have very different needs. The \nschemas\n you provide for your plugin will tell Delphix how to operate with your dataset.\n\n\nPrerequisites\n\u00b6\n\n\nTo complete the tutorial that follows, make sure you check off the things on this list:\n\n\n\n\nDownload the SDK and get it working\n\n\nA running Delphix Engine version 6.0.2.0 or above.\n\n\nAdd at least one Unix host\u2014but preferably three\u2014to the Delphix Engine as remote environments.\n\n\nHave a tool at hand for editing text files\u2014mostly Python and JSON. A simple text editor would work fine, or you can use a full-fledged IDE.",
            "title": "Overview"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#overview",
            "text": "In the following few pages, we will walk through an example of making a simple, working plugin.  Our plugin will virtualize simple directory trees on Unix systems. The actual contents of these directories could be anything: configuration files, documents, image libraries, etc. Our plugin will not care about the contents and will treat it as a directory tree full of files.",
            "title": "Overview"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#data-flow-in-the-delphix-engine",
            "text": "Here we will briefly overview how data moves through the Delphix Engine.",
            "title": "Data Flow in the Delphix Engine"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#ingestion",
            "text": "It all begins with Delphix ingesting data\u2014copying some data from what we call a  source environment   onto the Delphix Engine.  Plugins can use either of two basic strategies to do this copying:   direct linking , where the Delphix Engine pulls data directly from the source environment.  staged linking , where the plugin is responsible for pulling data from the source environment.   Our plugin will use the staged linking strategy.  With staged linking, Delphix exposes and mounts storage to a  staging environment .  This would be an NFS share for Unix environments and iSCSI disks for Windows environments. You can use either the source environment or a different environment for staging. We will write our plugin to handle both approaches.  Once Delphix mounts the storage share onto the staging environment, the plugin needs to arrange for the relevant data to be copied from the source environment onto the storage share, which is backed by Delphix Engine storage.  When this initial copy is complete, Delphix will take a snapshot of the backing storage.  This same basic operation will be repeated when Delphix mounts an NFS share: The plugin copies data onto it, then Delphix snapshots the result.",
            "title": "Ingestion"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#provisioning",
            "text": "Provisioning  is when you take a Delphix Engine snapshot and create a virtual dataset from it.  First the snapshot is cloned onto the Delphix Engine, then this newly-cloned data is mounted as a virtual dataset onto a  target environment . While this new virtual dataset gets updated by its end users, the original snapshot is persistent. You can use it in a few ways:   Provision other virtual datasets from it  Rewind the virtual dataset back to the state it represents  Create a physical database from it in what we call V2P: Virtual to Physical",
            "title": "Provisioning"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#parts-of-a-plugin",
            "text": "A plugin consists of three main parts. We will cover them briefly here, and then fill in more details later in the tutorial.",
            "title": "Parts of a Plugin"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#plugin-config",
            "text": "Plugin config is where the plugin describes itself to the Delphix Engine. What is the plugin called? What version of the plugin is being used? What type(s) of environments does the plugin work with? What features does the plugin offer?...",
            "title": "Plugin Config"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#plugin-operations",
            "text": "The plugin will need to provide operations. These are Python functions, each of which implements one small piece of functionality. This is how the plugin customizes Delphix behavior to work with the kind of dataset you\u2019re building the plugin for. One operation will handle setting up a newly-configured virtual dataset. Another will handle copying data from a source environment, and so on.  Later we\u2019ll provide examples for our first plugin. See  Plugin Operations  for full details on the operations that are available, which are required, and what each one is required to do.",
            "title": "Plugin Operations"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#schemas",
            "text": "As part of normal operations, plugins need to generate and access certain pieces of information in order to do their job. For example, plugins that work with Postgres might need to know which port number to connect to, or which credentials to use.  Defining your plugin\u2019s schemas will enable it to give the Delphix Engine the details it needs to run the operations we\u2019ve built into it. Different datasets can have very different needs. The  schemas  you provide for your plugin will tell Delphix how to operate with your dataset.",
            "title": "Schemas"
        },
        {
            "location": "/Building_Your_First_Plugin/Overview/#prerequisites",
            "text": "To complete the tutorial that follows, make sure you check off the things on this list:   Download the SDK and get it working  A running Delphix Engine version 6.0.2.0 or above.  Add at least one Unix host\u2014but preferably three\u2014to the Delphix Engine as remote environments.  Have a tool at hand for editing text files\u2014mostly Python and JSON. A simple text editor would work fine, or you can use a full-fledged IDE.",
            "title": "Prerequisites"
        },
        {
            "location": "/Building_Your_First_Plugin/Initial_Setup/",
            "text": "Initial Setup\n\u00b6\n\n\nBefore we begin to start writing plugin code, we will need to do some setup work. We will be using the \ndvp\n tool, which is described in the \nGetting Started\n section.\n\n\nThe quoted examples in this section assume you're working on a Unix-like system.\n\n\nSanity check\n\u00b6\n\n\nFirst a reminder that it's highly recommended that you develop your plugin in a \nvirtual environment\n.\n\n\nNext, make sure you have a Delphix Engine ready to use, as described in the \nPrerequisites\n section on the previous page.\n\n\nFinally, let's quickly make sure that \ndvp\n is working! Type \ndvp -h\n and you should see something like the following:\n\n\n(venv)$ dvp -h\nUsage: dvp [OPTIONS] COMMAND [ARGS]...\n\n  The tools of the Delphix Virtualization SDK that help develop, build, and\n  upload a plugin.\n\nOptions:\n  --version      Show the version and exit.\n  -v, --verbose  Enable verbose mode. Can be repeated up to three times for\n                 increased verbosity.\n  -q, --quiet    Enable quiet mode. Can be repeated up to three times for\n                 increased suppression.\n  -h, --help     Show this message and exit.\n\nCommands:\n  build          Build the plugin code and generate upload artifact file...\n  download-logs  Download plugin logs from a target Delphix Engine to a...\n  init           Create a plugin in the root directory.\n  upload         Upload the generated upload artifact (the plugin JSON\n                 file)...\n\n\n\n\n\nIf this looks good, you are ready to begin!\n\n\nIf, instead, you see something like the following, go back to \nGetting Started\n and make sure you setup everything correctly before continuing.\n\n\n(venv)$ dvp\n-bash: dvp: command not found\n\n\n\n\n\nCreating a Bare Plugin\n\u00b6\n\n\nTo start, we will create a new directory where our new plugin code will live.\n\n\n(venv)$ mkdir first_plugin\n(venv)$ cd first_plugin\n\n\n\n\n\nNow that we are in our new plugin directory, we can use the \ndvp\n tool to create a plugin for us. This plugin will be a mere skeleton -- it will not do anything useful until we modify it in the subsequent pages.\n\n\n(venv) first_plugin$ dvp init -n first_plugin -s STAGED -p WINDOWS\n\n\n\n\n\nThe \n-n\n argument here means \"plugin name.\" We are using the name \nfirst_plugin\n.\n\n\nThe \n-s\n argument tells which syncing strategy we want to use.\n\n\nThe \n-p\n argument tells which host platform our plugin supports.\n\n\nYou can type \ndvp init -h\n for more information about the options available.\n\n\nAfter running this command, you should see that files have been created for you:\n\n\n(venv) first_plugin$ ls\nplugin_config.yml   schema.json     src\n\n\n\n\n\nThese files are described below:\n\n\n\n\n\n\n\n\nFile\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nplugin_config.yml\n\n\nThe \nplugin config\n file, which provides a list of plugin properties\n\n\n\n\n\n\nschema.json\n\n\nContains \nschemas\n which provide custom datatype definitions\n\n\n\n\n\n\nsrc/plugin_runner.py\n\n\nA Python file which will eventually contain code that handles plugin \noperations\n\n\n\n\n\n\n\n\nOpen these files in your editor/IDE and take a look at them. At this point they will not have a lot of content, but we will add to them as we go through the next few pages.\n\n\nBuilding The New Plugin\n\u00b6\n\n\nThe new files we created above have to get \nbuilt\n to produce a single \nartifact\n. This is done with the \ndvp\n tool.\n\n\n(venv) first_plugin$ dvp build\n\n\n\n\n\nAfter the build, you should see that the build process has created a new file called \nartifact.json\n.\n\n\n(venv) first_plugin$ ls\nartifact.json       plugin_config.yml   schema.json     src\n\n\n\n\n\nUploading The New Plugin\n\u00b6\n\n\nNow using the \ndvp\n tool we can upload the artifact onto our Delphix Engine.\n\n\n(venv) first_plugin$ dvp upload -e engine.company.com -u admin\n\n\n\n\n\nThe \n-e\n argument specifies the engine on which to install the plugin, and the \n-u\n argument gives the Delphix Engine user.\n\n\nYou will be prompted for a password.\n\n\nOnce the upload is finished, you can verify the installation from the Manage > Toolkits screen in the Delphix Engine UI.",
            "title": "Initial Setup"
        },
        {
            "location": "/Building_Your_First_Plugin/Initial_Setup/#initial-setup",
            "text": "Before we begin to start writing plugin code, we will need to do some setup work. We will be using the  dvp  tool, which is described in the  Getting Started  section.  The quoted examples in this section assume you're working on a Unix-like system.",
            "title": "Initial Setup"
        },
        {
            "location": "/Building_Your_First_Plugin/Initial_Setup/#sanity-check",
            "text": "First a reminder that it's highly recommended that you develop your plugin in a  virtual environment .  Next, make sure you have a Delphix Engine ready to use, as described in the  Prerequisites  section on the previous page.  Finally, let's quickly make sure that  dvp  is working! Type  dvp -h  and you should see something like the following:  (venv)$ dvp -h\nUsage: dvp [OPTIONS] COMMAND [ARGS]...\n\n  The tools of the Delphix Virtualization SDK that help develop, build, and\n  upload a plugin.\n\nOptions:\n  --version      Show the version and exit.\n  -v, --verbose  Enable verbose mode. Can be repeated up to three times for\n                 increased verbosity.\n  -q, --quiet    Enable quiet mode. Can be repeated up to three times for\n                 increased suppression.\n  -h, --help     Show this message and exit.\n\nCommands:\n  build          Build the plugin code and generate upload artifact file...\n  download-logs  Download plugin logs from a target Delphix Engine to a...\n  init           Create a plugin in the root directory.\n  upload         Upload the generated upload artifact (the plugin JSON\n                 file)...  If this looks good, you are ready to begin!  If, instead, you see something like the following, go back to  Getting Started  and make sure you setup everything correctly before continuing.  (venv)$ dvp\n-bash: dvp: command not found",
            "title": "Sanity check"
        },
        {
            "location": "/Building_Your_First_Plugin/Initial_Setup/#creating-a-bare-plugin",
            "text": "To start, we will create a new directory where our new plugin code will live.  (venv)$ mkdir first_plugin\n(venv)$ cd first_plugin  Now that we are in our new plugin directory, we can use the  dvp  tool to create a plugin for us. This plugin will be a mere skeleton -- it will not do anything useful until we modify it in the subsequent pages.  (venv) first_plugin$ dvp init -n first_plugin -s STAGED -p WINDOWS  The  -n  argument here means \"plugin name.\" We are using the name  first_plugin .  The  -s  argument tells which syncing strategy we want to use.  The  -p  argument tells which host platform our plugin supports.  You can type  dvp init -h  for more information about the options available.  After running this command, you should see that files have been created for you:  (venv) first_plugin$ ls\nplugin_config.yml   schema.json     src  These files are described below:     File  Description      plugin_config.yml  The  plugin config  file, which provides a list of plugin properties    schema.json  Contains  schemas  which provide custom datatype definitions    src/plugin_runner.py  A Python file which will eventually contain code that handles plugin  operations     Open these files in your editor/IDE and take a look at them. At this point they will not have a lot of content, but we will add to them as we go through the next few pages.",
            "title": "Creating a Bare Plugin"
        },
        {
            "location": "/Building_Your_First_Plugin/Initial_Setup/#building-the-new-plugin",
            "text": "The new files we created above have to get  built  to produce a single  artifact . This is done with the  dvp  tool.  (venv) first_plugin$ dvp build  After the build, you should see that the build process has created a new file called  artifact.json .  (venv) first_plugin$ ls\nartifact.json       plugin_config.yml   schema.json     src",
            "title": "Building The New Plugin"
        },
        {
            "location": "/Building_Your_First_Plugin/Initial_Setup/#uploading-the-new-plugin",
            "text": "Now using the  dvp  tool we can upload the artifact onto our Delphix Engine.  (venv) first_plugin$ dvp upload -e engine.company.com -u admin  The  -e  argument specifies the engine on which to install the plugin, and the  -u  argument gives the Delphix Engine user.  You will be prompted for a password.  Once the upload is finished, you can verify the installation from the Manage > Toolkits screen in the Delphix Engine UI.",
            "title": "Uploading The New Plugin"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/",
            "text": "Discovery\n\u00b6\n\n\nWhat is Discovery?\n\u00b6\n\n\nIn order to ingest data from a source environment, the Delphix Engine first needs to learn information about the data: Where does it live? How can it be accessed? What is it called?\n\n\nDiscovery\n is the process by which the Delphix Engine learns about remote data. Discovery can be either:\n\n\n\n\nautomatic\n \u2014 where the plugin finds the remote data on its own\n\n\nmanual\n \u2014 where the user tells us about the remote data\n\n\n\n\nFor our first plugin, we will be using a mix of these two techniques.\n\n\nSource Configs and Repositories\n\u00b6\n\n\nWhat are Source Configs and Repositories?\n\u00b6\n\n\nA \nsource config\n is a collection of information that Delphix uses to represent a dataset. Different plugins will have different ideas about what a \"dataset\" is (an entire database? a set of config files? an application?). For our first plugin, it is simply a directory tree on the filesystem of the remote environment.\n\n\nA \nrepository\n represents what you might call \"data dependencies\" -- anything installed on the remote host that the dataset depends on. For example, if you are working with a Postgres database, then your repository will represent an installation of a particular version of the Postgres DBMS. In this plugin, we do not have any special dependencies, except for the simple existence of the unix system on which the directory lives.\n\n\nWe will be using automatic discovery for our repositories, and manual discovery for our source configs. This is the default configuration that is created by \ndvp init\n, so there is nothing further we need to do here.\n\n\nDefining Your Data Formats\n\u00b6\n\n\nBecause each plugin will have different ideas about what a repository or source config represents, different plugins will have different sets of information that they need to collect and store.\n\n\nDelphix needs to know the format of this information. How many pieces of information are collected? What are they called? Are they strings? Numbers?\n\n\nFor our first plugin, we do not need a lot of information. We use no special information about our repositories (except some way for the user to identify them). For source configs, all we need to know is the path to the directory from which we will be ingesting data.\n\n\nThe plugin needs to describe all of this to the Delphix Engine, and it does so using \nschemas\n.  Recall that when we ran \ndvp init\n, a file full of bare-bones schemas was created. As we build up our first toolkit, we will be augmenting these schemas to serve our needs.\n\n\nRepository Schema\n\u00b6\n\n\nOpen up the \nschema.json\n file in your editor/IDE and locate \nrepositoryDefinition\n, it should look like this:\n\n\n{\n\n    \n\"repositoryDefinition\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"object\"\n,\n\n        \n\"properties\"\n:\n \n{\n\n            \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n        \n},\n\n        \n\"nameField\"\n:\n \n\"name\"\n,\n\n        \n\"identityFields\"\n:\n \n[\n\"name\"\n]\n\n    \n}\n\n\n}\n\n\n\n\n\n\nSince we do not have any special dependencies, we can just leave it as-is.\n\n\nFor detailed information about exactly how repository schemas work, see \nthe reference page\n.\n\n\nIn brief, what we are doing here is saying that each of our repositories will have a single property called \nname\n, which will be used both as a unique identifier and as the user-visible name of the repository.\n\n\nSource Config Schema\n\u00b6\n\n\nFor source configs, the bare-bones schema is not going to be good enough. Recall that for us, a source config represents a directory tree on a remote environment.\n\n\nLocate the \nsourceConfigDefinition\n inside the \nschema.json\n file and modify the definition so it looks like this:\n\n\n\"sourceConfigDefinition\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"required\"\n:\n \n[\n\"name\"\n,\n \n\"path\"\n],\n\n    \n\"additionalProperties\"\n:\n \nfalse\n,\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"name\"\n:\n \n{\n\n          \n\"type\"\n:\n \n\"string\"\n,\n\n          \n\"prettyName\"\n:\n \n\"Dataset Name\"\n,\n\n          \n\"description\"\n:\n \n\"User-visible name for this dataset\"\n\n        \n},\n\n        \n\"path\"\n:\n \n{\n\n          \n\"type\"\n:\n \n\"string\"\n,\n\n          \n\"format\"\n:\n \n\"unixpath\"\n,\n\n          \n\"prettyName\"\n:\n \n\"Path\"\n,\n\n          \n\"description\"\n:\n \n\"Full path to data location on the remote environment\"\n\n        \n}\n\n    \n},\n\n    \n\"nameField\"\n:\n \n\"name\"\n,\n\n    \n\"identityFields\"\n:\n \n[\n\"path\"\n]\n\n\n}\n,\n\n\n\n\n\n\nNow, we have two properties, a property \nname\n serving as the user-visible name of the source config and \npath\n which tells us where the data lives on the remote host. Note  we are using \npath\n as the unique identifier.\n\n\nBecause we are using manual discovery, the end user is going to be responsible for filling in values for \nname\n and \npath\n. So, we have added some things to our schema that we did not need for repositories.\n\n\nThe \nprettyName\n and \ndescription\n entries will be used by the UI to tell the user what these fields mean.\n\n\nBecause we set \nadditionalProperties\n to \nfalse\n, this will prevent users from supplying properties other than \nname\n and \npath\n.\n\n\nFinally, we have specified that the \npath\n property must be a well-formatted Unix path. This allows the UI to enforce that the format is correct before the user is allowed to proceed. (Note this only enforces the format, and does not actually check to see if the path really exists on some remote environment!)\n\n\nRefer to the reference page for \nSchemas\n for more details about these entries, and for other things that you can do in these schemas.\n\n\nImplementing Discovery in Your Plugin\n\u00b6\n\n\nAbout Python Code\n\u00b6\n\n\nAs described in the overview section, plugins customize the behavior of the Delphix Engine by providing Python code. Each customizable piece of behavior is called a \"plugin operation\". The plugin provides separate Python functions for each of the operations that it wants to customize.\n\n\nRight now, we are concerned with discovery. There are two customizable operations related to automatic discovery, one for repositories and one for source configs. In both cases, the job of the Python method is to automatically collect whatever information the schemas (see above) require, and to return that information to the Delphix Engine. The Delphix Engine will run these customized operations whenever a new environment is added, or when an existing environment is rediscovered.\n\n\nA Look at the Generated Code\n\u00b6\n\n\nRecall that the \ndvp init\n command we ran created a file called \nsrc/plugin_runner.py\n. Open this file in your editor/IDE. You will see that this file already contains a bunch of Python code. Let's take a look at the first three blocks of code in this file.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nMount\n,\n \nMountSpecification\n,\n \nPlugin\n\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \n(\n\n    \nRepositoryDefinition\n,\n\n    \nSourceConfigDefinition\n,\n\n    \nSnapshotDefinition\n,\n\n\n)\n\n\n\n\n\n\nThese \nimport\n lines make certain functionality available to our Python code. Some of this functionality will\nbe used just below, as we implement discovery. Others will be used later on, as we implement\ningestion and provisioning. Later, you'll add more \nimport\ns to unlock more functionality.\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n\n\n\nThis line creates a Python object which allows us to define our plugin types. We have the ability to do this because of the \nimport Plugin\n statement above.\n\n\nThis object is stored in a variable we have elected to call \nplugin\n. We are free to call this variable anything we want, so long as we also change the \nentryPoint\n line in the \nplugin_config.yml\n file. For this example, we will just leave it as \nplugin\n.\n\n\n#\n\n\n# Below is an example of the repository discovery operation.\n\n\n#\n\n\n# NOTE: The decorators are defined on the 'plugin' object created above.\n\n\n#\n\n\n# Mark the function below as the operation that does repository discovery.\n\n\n@plugin.discovery.repository\n()\n\n\ndef\n \nrepository_discovery\n(\nsource_connection\n):\n\n    \n#\n\n    \n# This is an object generated from the repositoryDefinition schema.\n\n    \n# In order to use it locally you must run the 'build -g' command provided\n\n    \n# by the SDK tools from the plugin's root directory.\n\n    \n#\n\n\n    \nreturn\n \n[\nRepositoryDefinition\n(\nname\n=\n'1e87dc30-3cdb-4f0a-9634-07ce017d20d1'\n)]\n\n\n\n\n\n\nThis is our first \nplugin operation\n. In this case, it's defining what will happen when the Delphix Engine wants to discover repositories on an environment.  Let's take a look at this code line-by-line\n\n\n@plugin.discovery.repository\n()\n\n\ndef\n \nrepository_discovery\n(\nsource_connection\n):\n\n\n\n\n\n\nThis begins the definition of a function called \nrepository_discovery\n.\n\n\nWe are using a Python \ndecorator\n which signals to the Delphix Engine that this is the function which should be called when it is time to do repository discovery. The actual name of the function doesn't matter here. Note that we are using our \nplugin\n variable here as part of the decorator.\n\n\nThe Delphix Engine will pass us information about the source environment in an argument called \nsource_connection\n.\n\n\n\n\nWarning\n\n\nThe name of this input argument matters. That is, you'll always need to have an argument called\n\nsource_connection\n here. Each plugin operation has its own set of required argument names. For\ndetails on which arguments apply to which operations, see the \nreference section\n.\n\n\n\n\n    \nreturn\n \n[\nRepositoryDefinition\n(\nname\n=\n'1e87dc30-3cdb-4f0a-9634-07ce017d20d1'\n)]\n\n\n\n\n\n\nThis creates and returns a Python object that corresponds to the format defined by our repository schema. Because out repository has exactly one string property called \nname\n, therefore this Python object has one property called \nname\n.\n\n\nNotice that the code generator has filled in the value of \nname\n with a random string. This results in a plugin operation that works, but which will not be very helpful for the user. We'll change this later.\n\n\nThe rest of the file contains more plugin operations, and we'll be modifying them later.\n\n\nRepository Discovery\n\u00b6\n\n\nNow, we need to modify the provided \nrepository discovery\n operation. This operation will examine a remote environment, find any repositories, and return information about them to the Delphix Engine.\n\n\nAs a reminder, our only external dependency on the remote environment is simply the existence of a filesystem. Since every Unix host has a filesystem, that means we will have exactly one repository per remote environment. Therefore, our repository discovery operation can be very simple.\n\n\nIn fact, as we saw above, the default-generated \nrepository_discovery\n function does almost exactly what we want -- it returns one single repository for any Unix host that it is asked to work with. The only problem with it is that it uses\nunhelpful name.  That's really easy to change!\n\n\nReplace or modify \nrepository_discovery\n so it looks like this:\n\n\n@plugin.discovery.repository\n()\n\n\ndef\n \nrepository_discovery\n(\nsource_connection\n):\n\n    \nrepository\n \n=\n \nRepositoryDefinition\n(\n'Repository for our First Plugin'\n)\n\n    \nreturn\n \n[\nrepository\n]\n\n\n\n\n\n\n\n\nTip\n\n\nBe careful to always use consistent indentation in Python code!\n\n\n\n\nSource Config Discovery\n\u00b6\n\n\nFor source configs, we will rely solely on manual discovery. Therefore, the user will tell us which directories they want to ingest from. We still have to define a source config discovery operation -- it just won't need to do much.\n\n\nThe job of this operation is to return only source configs associated with the given \nrepository\n. This function will be called once per repository. In our case, that means it will only be called once.\n\n\nBecause we want to supply \nno\n automatically-discovered source configs, this function should simply returns an empty list.\n\n\nIn fact, \ndvp init\n has already generated a function for us that does exactly this.\n\n\n@plugin.discovery.source_config\n()\n\n\ndef\n \nsource_config_discovery\n(\nsource_connection\n,\n \nrepository\n):\n\n    \nreturn\n \n[]\n\n\n\n\n\n\nIf we wanted to do automatic discovery of source configs, we'd modify this function. But, for our purposes now, the existing code is fine and we don't need to change anything.\n\n\nHow to Run Discovery in the Delphix Engine\n\u00b6\n\n\nLet us make sure discovery works!\n\n\n\n\n\n\nRun the \ndvp build\n commands, as before. This will build the plugin, with all of the new changes, and create an artifact.\n\n\n\n\n\n\nRun \ndvp upload -e <engine> -u <user>\n, as before. This will get all the new changes onto the Delphix Engine.\n\n\n\n\n\n\nOnce the new plugin is uploaded, add a remote unix environment to your engine. To do this, go to \nManage > Environments\n, chose \nAdd Environment\n from the menu, answer the questions, and \nSubmit\n. (If you already have an environment set up, you can just refresh it instead).\n\n\n\n\n\n\nTo keep an eye on this discovery process, you may need to open the \nActions\n tab on the UI. If any errors happen, they will be reported here.\n\n\n\n\nAfter the automatic discovery process completes, go to the \nDatabases\n tab. You will see an entry for \nRepository For Our First Plugin\n. This is the repository you created in your Python code.\n\n\n\n\n\n\nNotice that it says \nNo databases found on installation\n. This is because we chose not to do automatic source config discovery.\n\n\nHowever, because we have allowed manual source config discovery, you can add your own entries by clicking the plus sign (\nAdd Database\n). Complete the information in the Add Database dialog and click Add.\n\n\n\n\nThis should all look familiar. It is precisely what we defined in our source config schema. As expected, there are two entries, one for our \nname\n property, and one for \npath\n.\n\n\nFor example, in the above screenshot, we are specifying that we want to sync the \n/bin\n directory\nfrom the remote host, and we want to call it \nBinaries\n. You can pick any directory and name that\nyou want.\n\n\nOnce you have added one or more source configs, you will be able to sync. This is covered on the next page.\n\n\n\n\nWarning\n\n\nOnce you have automatically or manually created source configs, you will not be allowed to modify your plugin's source config schema. We will cover how to deal with this later in the \nupgrade section\n. For now, if you need to change your plugin's source config schema:\n\n\n\n\nYou will have to delete any source configs you have manually added.\n\n\nDelete the plugin and its corresponding objects (dSources, Virtual Sources, etc) if the source configs were manually discovered.",
            "title": "Discovery"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#discovery",
            "text": "",
            "title": "Discovery"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#what-is-discovery",
            "text": "In order to ingest data from a source environment, the Delphix Engine first needs to learn information about the data: Where does it live? How can it be accessed? What is it called?  Discovery  is the process by which the Delphix Engine learns about remote data. Discovery can be either:   automatic  \u2014 where the plugin finds the remote data on its own  manual  \u2014 where the user tells us about the remote data   For our first plugin, we will be using a mix of these two techniques.",
            "title": "What is Discovery?"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#source-configs-and-repositories",
            "text": "",
            "title": "Source Configs and Repositories"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#what-are-source-configs-and-repositories",
            "text": "A  source config  is a collection of information that Delphix uses to represent a dataset. Different plugins will have different ideas about what a \"dataset\" is (an entire database? a set of config files? an application?). For our first plugin, it is simply a directory tree on the filesystem of the remote environment.  A  repository  represents what you might call \"data dependencies\" -- anything installed on the remote host that the dataset depends on. For example, if you are working with a Postgres database, then your repository will represent an installation of a particular version of the Postgres DBMS. In this plugin, we do not have any special dependencies, except for the simple existence of the unix system on which the directory lives.  We will be using automatic discovery for our repositories, and manual discovery for our source configs. This is the default configuration that is created by  dvp init , so there is nothing further we need to do here.",
            "title": "What are Source Configs and Repositories?"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#defining-your-data-formats",
            "text": "Because each plugin will have different ideas about what a repository or source config represents, different plugins will have different sets of information that they need to collect and store.  Delphix needs to know the format of this information. How many pieces of information are collected? What are they called? Are they strings? Numbers?  For our first plugin, we do not need a lot of information. We use no special information about our repositories (except some way for the user to identify them). For source configs, all we need to know is the path to the directory from which we will be ingesting data.  The plugin needs to describe all of this to the Delphix Engine, and it does so using  schemas .  Recall that when we ran  dvp init , a file full of bare-bones schemas was created. As we build up our first toolkit, we will be augmenting these schemas to serve our needs.",
            "title": "Defining Your Data Formats"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#repository-schema",
            "text": "Open up the  schema.json  file in your editor/IDE and locate  repositoryDefinition , it should look like this:  { \n     \"repositoryDefinition\" :   { \n         \"type\" :   \"object\" , \n         \"properties\" :   { \n             \"name\" :   {   \"type\" :   \"string\"   } \n         }, \n         \"nameField\" :   \"name\" , \n         \"identityFields\" :   [ \"name\" ] \n     }  }   Since we do not have any special dependencies, we can just leave it as-is.  For detailed information about exactly how repository schemas work, see  the reference page .  In brief, what we are doing here is saying that each of our repositories will have a single property called  name , which will be used both as a unique identifier and as the user-visible name of the repository.",
            "title": "Repository Schema"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#source-config-schema",
            "text": "For source configs, the bare-bones schema is not going to be good enough. Recall that for us, a source config represents a directory tree on a remote environment.  Locate the  sourceConfigDefinition  inside the  schema.json  file and modify the definition so it looks like this:  \"sourceConfigDefinition\" :   { \n     \"type\" :   \"object\" , \n     \"required\" :   [ \"name\" ,   \"path\" ], \n     \"additionalProperties\" :   false , \n     \"properties\" :   { \n         \"name\" :   { \n           \"type\" :   \"string\" , \n           \"prettyName\" :   \"Dataset Name\" , \n           \"description\" :   \"User-visible name for this dataset\" \n         }, \n         \"path\" :   { \n           \"type\" :   \"string\" , \n           \"format\" :   \"unixpath\" , \n           \"prettyName\" :   \"Path\" , \n           \"description\" :   \"Full path to data location on the remote environment\" \n         } \n     }, \n     \"nameField\" :   \"name\" , \n     \"identityFields\" :   [ \"path\" ]  } ,   Now, we have two properties, a property  name  serving as the user-visible name of the source config and  path  which tells us where the data lives on the remote host. Note  we are using  path  as the unique identifier.  Because we are using manual discovery, the end user is going to be responsible for filling in values for  name  and  path . So, we have added some things to our schema that we did not need for repositories.  The  prettyName  and  description  entries will be used by the UI to tell the user what these fields mean.  Because we set  additionalProperties  to  false , this will prevent users from supplying properties other than  name  and  path .  Finally, we have specified that the  path  property must be a well-formatted Unix path. This allows the UI to enforce that the format is correct before the user is allowed to proceed. (Note this only enforces the format, and does not actually check to see if the path really exists on some remote environment!)  Refer to the reference page for  Schemas  for more details about these entries, and for other things that you can do in these schemas.",
            "title": "Source Config Schema"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#implementing-discovery-in-your-plugin",
            "text": "",
            "title": "Implementing Discovery in Your Plugin"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#about-python-code",
            "text": "As described in the overview section, plugins customize the behavior of the Delphix Engine by providing Python code. Each customizable piece of behavior is called a \"plugin operation\". The plugin provides separate Python functions for each of the operations that it wants to customize.  Right now, we are concerned with discovery. There are two customizable operations related to automatic discovery, one for repositories and one for source configs. In both cases, the job of the Python method is to automatically collect whatever information the schemas (see above) require, and to return that information to the Delphix Engine. The Delphix Engine will run these customized operations whenever a new environment is added, or when an existing environment is rediscovered.",
            "title": "About Python Code"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#a-look-at-the-generated-code",
            "text": "Recall that the  dvp init  command we ran created a file called  src/plugin_runner.py . Open this file in your editor/IDE. You will see that this file already contains a bunch of Python code. Let's take a look at the first three blocks of code in this file.  from   dlpx.virtualization.platform   import   Mount ,   MountSpecification ,   Plugin  from   generated.definitions   import   ( \n     RepositoryDefinition , \n     SourceConfigDefinition , \n     SnapshotDefinition ,  )   These  import  lines make certain functionality available to our Python code. Some of this functionality will\nbe used just below, as we implement discovery. Others will be used later on, as we implement\ningestion and provisioning. Later, you'll add more  import s to unlock more functionality.  plugin   =   Plugin ()   This line creates a Python object which allows us to define our plugin types. We have the ability to do this because of the  import Plugin  statement above.  This object is stored in a variable we have elected to call  plugin . We are free to call this variable anything we want, so long as we also change the  entryPoint  line in the  plugin_config.yml  file. For this example, we will just leave it as  plugin .  #  # Below is an example of the repository discovery operation.  #  # NOTE: The decorators are defined on the 'plugin' object created above.  #  # Mark the function below as the operation that does repository discovery.  @plugin.discovery.repository ()  def   repository_discovery ( source_connection ): \n     # \n     # This is an object generated from the repositoryDefinition schema. \n     # In order to use it locally you must run the 'build -g' command provided \n     # by the SDK tools from the plugin's root directory. \n     # \n\n     return   [ RepositoryDefinition ( name = '1e87dc30-3cdb-4f0a-9634-07ce017d20d1' )]   This is our first  plugin operation . In this case, it's defining what will happen when the Delphix Engine wants to discover repositories on an environment.  Let's take a look at this code line-by-line  @plugin.discovery.repository ()  def   repository_discovery ( source_connection ):   This begins the definition of a function called  repository_discovery .  We are using a Python  decorator  which signals to the Delphix Engine that this is the function which should be called when it is time to do repository discovery. The actual name of the function doesn't matter here. Note that we are using our  plugin  variable here as part of the decorator.  The Delphix Engine will pass us information about the source environment in an argument called  source_connection .   Warning  The name of this input argument matters. That is, you'll always need to have an argument called source_connection  here. Each plugin operation has its own set of required argument names. For\ndetails on which arguments apply to which operations, see the  reference section .        return   [ RepositoryDefinition ( name = '1e87dc30-3cdb-4f0a-9634-07ce017d20d1' )]   This creates and returns a Python object that corresponds to the format defined by our repository schema. Because out repository has exactly one string property called  name , therefore this Python object has one property called  name .  Notice that the code generator has filled in the value of  name  with a random string. This results in a plugin operation that works, but which will not be very helpful for the user. We'll change this later.  The rest of the file contains more plugin operations, and we'll be modifying them later.",
            "title": "A Look at the Generated Code"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#repository-discovery",
            "text": "Now, we need to modify the provided  repository discovery  operation. This operation will examine a remote environment, find any repositories, and return information about them to the Delphix Engine.  As a reminder, our only external dependency on the remote environment is simply the existence of a filesystem. Since every Unix host has a filesystem, that means we will have exactly one repository per remote environment. Therefore, our repository discovery operation can be very simple.  In fact, as we saw above, the default-generated  repository_discovery  function does almost exactly what we want -- it returns one single repository for any Unix host that it is asked to work with. The only problem with it is that it uses\nunhelpful name.  That's really easy to change!  Replace or modify  repository_discovery  so it looks like this:  @plugin.discovery.repository ()  def   repository_discovery ( source_connection ): \n     repository   =   RepositoryDefinition ( 'Repository for our First Plugin' ) \n     return   [ repository ]    Tip  Be careful to always use consistent indentation in Python code!",
            "title": "Repository Discovery"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#source-config-discovery",
            "text": "For source configs, we will rely solely on manual discovery. Therefore, the user will tell us which directories they want to ingest from. We still have to define a source config discovery operation -- it just won't need to do much.  The job of this operation is to return only source configs associated with the given  repository . This function will be called once per repository. In our case, that means it will only be called once.  Because we want to supply  no  automatically-discovered source configs, this function should simply returns an empty list.  In fact,  dvp init  has already generated a function for us that does exactly this.  @plugin.discovery.source_config ()  def   source_config_discovery ( source_connection ,   repository ): \n     return   []   If we wanted to do automatic discovery of source configs, we'd modify this function. But, for our purposes now, the existing code is fine and we don't need to change anything.",
            "title": "Source Config Discovery"
        },
        {
            "location": "/Building_Your_First_Plugin/Discovery/#how-to-run-discovery-in-the-delphix-engine",
            "text": "Let us make sure discovery works!    Run the  dvp build  commands, as before. This will build the plugin, with all of the new changes, and create an artifact.    Run  dvp upload -e <engine> -u <user> , as before. This will get all the new changes onto the Delphix Engine.    Once the new plugin is uploaded, add a remote unix environment to your engine. To do this, go to  Manage > Environments , chose  Add Environment  from the menu, answer the questions, and  Submit . (If you already have an environment set up, you can just refresh it instead).    To keep an eye on this discovery process, you may need to open the  Actions  tab on the UI. If any errors happen, they will be reported here.   After the automatic discovery process completes, go to the  Databases  tab. You will see an entry for  Repository For Our First Plugin . This is the repository you created in your Python code.    Notice that it says  No databases found on installation . This is because we chose not to do automatic source config discovery.  However, because we have allowed manual source config discovery, you can add your own entries by clicking the plus sign ( Add Database ). Complete the information in the Add Database dialog and click Add.   This should all look familiar. It is precisely what we defined in our source config schema. As expected, there are two entries, one for our  name  property, and one for  path .  For example, in the above screenshot, we are specifying that we want to sync the  /bin  directory\nfrom the remote host, and we want to call it  Binaries . You can pick any directory and name that\nyou want.  Once you have added one or more source configs, you will be able to sync. This is covered on the next page.   Warning  Once you have automatically or manually created source configs, you will not be allowed to modify your plugin's source config schema. We will cover how to deal with this later in the  upgrade section . For now, if you need to change your plugin's source config schema:   You will have to delete any source configs you have manually added.  Delete the plugin and its corresponding objects (dSources, Virtual Sources, etc) if the source configs were manually discovered.",
            "title": "How to Run Discovery in the Delphix Engine"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/",
            "text": "Data Ingestion\n\u00b6\n\n\nHow Does Delphix Ingest Data?\n\u00b6\n\n\nAs \npreviously\n discussed, the Delphix Engine uses the \ndiscovery\n process to learn about datasets that live on a \nsource environment\n. In this section we will learn how the Delphix Engine uses a two-step process to ingest a dataset.\n\n\nLinking\n\u00b6\n\n\nThe first step is called \nlinking\n. This is simply the creation of a new dataset on the Delphix Engine, which is associated with the dataset on the source environment. This new linked dataset is called a \ndSource\n.\n\n\nSyncing\n\u00b6\n\n\nImmediately after linking, the new dSource is \nsynced\n for the first time. Syncing is a process by which data from the source environment is copied onto the Delphix Engine. Subsequent syncs may then be periodically performed in order to keep the dSource up-to-date.\n\n\nThe details of how this is done varies significantly from plugin to plugin. For example, some plugins will simply copy files from the filesystem. Other plugins might contact a DBMS and instruct it to send backup or replication streams. There are many possibilities here, but they all break down into two main strategies that the plugin author can choose from: direct and staging.\n\n\nWith the \ndirect\n strategy, the plugin is not in charge\nof the data copying. Instead the Delphix Engine directly pulls raw data from the source environment.\nThe plugin merely provides the location of the data. This is a very simple strategy, and is also\nquite limiting.\n\n\nFor our first plugin, we will be using the more flexible \nstaging\n strategy. With this strategy, the Delphix Engine uses NFS for Unix environments (or iSCSI on Windows environments) to mount storage onto a \nstaging environment\n. Our plugin will then be in full control of how to get data from the source environment onto this storage mount.\n\n\nWith the staging strategy, there are two types of syncs: sync and resync. A \nsync\n is used to ingestion incremental changes while a \nresync\n is used to re-ingest all the data for the dSource. For databases, this could mean re-ingesting from a full database backup to reset the dSource. A \nsync\n and a \nresync\n execute the same plugin operations and are differentiated by a boolean flag in the \nsnapshot_parameters\n argument passed into \nlinked.pre_snapshot\n and \nlinked.post_snapshot\n.\n\n\nA regular \nsync\n is the default and is executed as part of policy driven syncs. A \nresync\n is only executed during initial ingestion or if the Delphix user manually starts one. The customer can manually trigger a \nresync\n via the UI by selecting the dSource, going to more options and selecting \nResynchronize dSource\n. \n\n\n\n\nGotcha\n\n\nAlthough it is not common, it is entirely possible that the staging environment is the same as the source environment. Be careful not to assume otherwise in your plugins.\n\n\n\n\nOur Syncing Strategy\n\u00b6\n\n\nFor our purposes here in this intro plugin, we will use a simple strategy. We won't do anything with the resync snapshot parameter and simply copy files from the filesystem on the source environment onto the NFS mount on the staging environment. We will do this by running the Unix tool \nrsync\n from our staging environment, and rely on passwordless SSH to connect to the source environment.\n\n\n\n\nInfo\n\n\nThis plugin is assuming that \nrsync\n is installed on the staging host, and that the staging\nhost user is able to SSH into the source host without having to type in a password. A more\nfull-featured plugin would test these assumptions, usually as part of discovery.\n\n\n\n\nIn the special case mentioned above, where the staging environment is the same as the source environment, we could likely do something more efficient. However, for simplicity's sake, we won't do that here.\n\n\nDefining Your Linked Source Data Format\n\u00b6\n\n\nIn order to be able to successfully do the copying required, plugins might need to get some information from the end-user of your plugin. In our case, we need to tell \nrsync\n how to access the files. This means we need to know the source environment's IP address (or domain name), the username we need to connect as, and finally the location where the files live.\n\n\nAgain, we will be using a JSON schema to define the data format. The user will be presented with a UI that lets them provide all the information our schema specifies.\n\n\nOpen up \nschema.json\n in your editor/IDE. Locate the \nLinkedSourceDefinition\n and replace it with the following schema:\n\n\n\"linkedSourceDefinition\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"additionalProperties\"\n:\n \nfalse\n,\n\n    \n\"required\"\n:\n \n[\n\"sourceAddress\"\n,\n \n\"username\"\n,\n \n\"mountLocation\"\n],\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"sourceAddress\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"string\"\n,\n\n            \n\"prettyName\"\n:\n \n\"Host from which to copy\"\n,\n\n            \n\"description\"\n:\n \n\"IP or FQDN of host from which to copy\"\n\n        \n},\n\n        \n\"username\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"string\"\n,\n\n            \n\"prettyName\"\n:\n \n\"Username on Source Host\"\n,\n\n            \n\"description\"\n:\n \n\"Username for making SSH connection to source host\"\n\n        \n},\n\n        \n\"mountLocation\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"string\"\n,\n\n            \n\"format\"\n:\n \n\"unixpath\"\n,\n\n            \n\"prettyName\"\n:\n \n\"Mount Location on Staging Host\"\n,\n\n            \n\"description\"\n:\n \n\"Where to mount storage onto the staging host while syncing\"\n\n        \n}\n\n    \n}\n\n\n}\n,\n\n\n\n\n\n\n\n\nInfo\n\n\nAs will be explained later, this schema will be used to generate Python code.\nAll names in the autogenerated Python code will use \nlower_case_with_underscores\n as attribute names as per Python variable naming conventions.\nThat is, if we were to use \nmountLocation\n as the schema property name, it would be called\n\nmount_location\n in the generated Python code.\n\n\n\n\nWith this schema, the user will be required to provide the source username, the source's IP address, and the staging mount location as part of the linking process.\n\n\nImplementing Syncing in Your Plugin\n\u00b6\n\n\nThere are three things we must do to implement syncing. First, we need to tell the Delphix Engine\nwhere to mount storage onto the staging environment. Next we need to actually do the work of copying\ndata onto that mounted storage. Finally, we need to generate any snapshot-related data.\n\n\nMount Specification\n\u00b6\n\n\nBefore syncing can begin, the Delphix Engine needs to mount some storage onto the staging host.\nSince different plugins can have different requirements about where exactly this mount lives, it is\nup to the plugin to specify this location. As mentioned above, our simple plugin will get this\nlocation from the user.\n\n\nOpen up the \nplugin_runner.py\n file and find the \nlinked_mount_specification\n function (which was generated by \ndvp init\n). Replace it with the following code:\n\n\n@plugin.linked.mount_specification()\ndef linked_mount_specification(staged_source, repository):\n    mount_location = staged_source.parameters.mount_location\n    mount = Mount(staged_source.staged_connection.environment, mount_location)\n    return MountSpecification([mount])\n\n\n\n\n\nLet's take this line-by-line to see what's going on here.\n\n\n@plugin.linked.mount_specification()\n\n\n\n\n\nThis \ndecorator\n announces that the following function\nis the code that handles the \nmount_specification\n operation. This is what allows the Delphix\nEngine to know which function to call when it's time to learn where to mount. Every operation\ndefinition will begin with a similar decorator.\n\n\ndef linked_mount_specification(staged_source, repository):\n\n\n\n\n\nThis begins a Python function definition. We chose to call it \nlinked_mount_specification\n, but we\ncould have chosen any name at all. This function accepts two arguments, one giving information about\nthe linked source, and one giving information about the associated repository.\n\n\n    mount_location = staged_source.parameters.mount_location\n\n\n\n\n\nThe \nstaged_source\n input argument contains an attribute called \nparameters\n. This in turn contains\nall of the properties defined by the \nlinkedSourceDefinition\n schema. So, in our case, that means\nit will contain attributes called \nsource_address\n, \nusername\n, and \nmount_location\n. Note how any attribute defined in \ncamelCase\n in the schema is converted to \nvariable_with_underscores\n. This line\nsimply retrieves the user-provided mount location and saves it in a local variable.\n\n\n    mount = Mount(staged_source.staged_connection.environment, mount_location)\n\n\n\n\n\nThis line constructs a new object from the \nMount class\n. This class\nholds details about how Delphix Engine storage is mounted onto remote environments. Here, we\ncreate a mount object that says to mount onto the staging environment, at the location specified\nby the user.\n\n\n    return MountSpecification([mount])\n\n\n\n\n\nOn the line just before this one, we created an object that describes a \nsingle\n mount. Now, we\nmust return a full \nmount specification\n. In general,\na mount specification is a collection of mounts. But, in our case, we just have one single mount.\nTherefore, we use an array with only one item it in -- namely, the one single mount object we\ncreated just above.\n\n\nData Copying\n\u00b6\n\n\nAs explained \nhere\n, the Delphix Engine will always run the plugin's \npreSnapshot\n operation just before taking a snapshot of the dsource. That means our \npreSnapshot\n operation has to get the NFS share into the desired state. For us, that means that's the time to do our data copy.\n\n\nUnlike the previous operations we've seen so far, the pre-snapshot operation will not be autogenerated by \ndvp init\n.\nSo, we will need to add one ourselves.  Open up the \nplugin_runner.py\n file.\n\n\nFirst, we'll add a new import line near the top of the file, so that we can use Delphix's platform libraries and raise user visible errors (explained below).\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\nfrom\n \ndlpx.virtualization.platform.exceptions\n \nimport\n \nUserError\n\n\n\n\n\n\nNext, we'll add a new function:\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \ncopy_data_from_source\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n,\n \nsnapshot_parameters\n):\n\n    \nstage_mount_path\n \n=\n \nstaged_source\n.\nmount\n.\nmount_path\n\n    \ndata_location\n \n=\n \n\"{}@{}:{}\"\n.\nformat\n(\nstaged_source\n.\nparameters\n.\nusername\n,\n\n        \nstaged_source\n.\nparameters\n.\nsource_address\n,\n\n        \nsource_config\n.\npath\n)\n\n\n    \nrsync_command\n \n=\n \n\"rsync -r {} {}\"\n.\nformat\n(\ndata_location\n,\n \nstage_mount_path\n)\n\n\n    \nresult\n \n=\n \nlibs\n.\nrun_bash\n(\nstaged_source\n.\nstaged_connection\n,\n \nrsync_command\n)\n\n\n    \nif\n \nresult\n.\nexit_code\n \n!=\n \n0\n:\n\n        \nraise\n \nUserError\n(\n\n            \n\"Could not copy files.\"\n,\n\n            \n\"Ensure that passwordless SSH works for {}.\"\n.\nformat\n(\nstaged_source\n.\nparameters\n.\nsource_address\n),\n\n            \nresult\n.\nstderr\n)\n\n\n\n\n\n\nLet's walk through this function and see what's going on\n\n\n    \nstage_mount_path\n \n=\n \nstaged_source\n.\nmount\n.\nmount_path\n\n\n\n\n\n\nThe \nstaged_source\n argument contains information about the current mount location. Here we save that\nto a local variable for convenience.\n\n\n    \ndata_location\n \n=\n \n\"{}@{}:{}\"\n.\nformat\n(\nstaged_source\n.\nparameters\n.\nusername\n,\n\n        \nstaged_source\n.\nparameters\n.\nsource_address\n,\n\n        \nsource_config\n.\npath\n)\n\n\n\n\n\n\nThis code creates a Python string that represents the location of the data that we want to ingest.\nThis is in the form \n<user>@<host>:<path>\n. For example \njdoe@sourcehost.mycompany.com:/bin\n. As\nbefore with \nmountLocation\n, we have defined our schemas such that these three pieces of information\nwere provided by the user. Here we're just putting them into a format that \nrsync\n will understand.\n\n\n    \nrsync_command\n \n=\n \n\"rsync -r {} {}\"\n.\nformat\n(\ndata_location\n,\n \nstage_mount_path\n)\n\n\n\n\n\n\nThis line is the actual Bash command that we'll be running on the staging host. This will look something like \nrsync -r user@host:/source/path /staging/mount/path\n.\n\n\n    \nresult\n \n=\n \nlibs\n.\nrun_bash\n(\nstaged_source\n.\nstaged_connection\n,\n \nrsync_command\n)\n\n\n\n\n\n\nThis is an example of a \nplatform library\n function, where we ask the Virtualization Platform\nto do some work on our behalf. In this case, we're asking the platform to run our Bash command on the\nstaging environment. For full details on the \nrun_bash\n platform library function and others, see this \nreference\n.\n\n\n    \nif\n \nresult\n.\nexit_code\n \n!=\n \n0\n:\n\n        \nraise\n \nUserError\n(\n\n            \n\"Could not copy files.\"\n,\n\n            \n\"Ensure that passwordless SSH works for {}.\"\n.\nformat\n(\nstaged_source\n.\nparameters\n.\nsource_address\n),\n\n            \nresult\n.\nstderr\n)\n\n\n\n\n\n\nFinally, we check to see if our Bash command actually worked okay. If not, we raise an error\nmessage, and describe one possible problem for the user to investigate. For more details on raising user visible errors, see this \nreference\n.\n\n\nSaving Snapshot Data\n\u00b6\n\n\nWhenever the Delphix Engine takes a \nsnapshot\n of a dSource or VDB,\nthe plugin has the chance to save any information it likes alongside that snapshot. Later, if the\nsnapshot is ever used to provision a new VDB, the plugin can use the previously-saved information\nto help get the new VDB ready for use.\n\n\nThe format of this data is controlled by the plugin's \nsnapshotDefinition\n schema. In our case, we\ndon't have any data we need to save. So, there's not much to do here. We will not modify the blank\nschema that was created by \ndvp init\n.\n\n\nWe do still need to provide python function for the engine to call, but we don't have to do much.\nIn fact, the default implementation that was generated by \ndvp init\n will work just fine for our purposes:\n\n\n@plugin.linked.post_snapshot\n()\n\n\ndef\n \nlinked_post_snapshot\n(\nstaged_source\n,\n\n                         \nrepository\n,\n\n                         \nsource_config\n,\n\n                         \nsnapshot_parameters\n):\n\n    \nreturn\n \nSnapshotDefinition\n()\n\n\n\n\n\n\nThe only thing this code is doing is creating a new object using our (empty) snapshot\ndefinition, and returning that new empty object.\n\n\nHow to Link and Sync in the Delphix Engine\n\u00b6\n\n\nLet's try it out and make sure this works!\n\n\nPrerequisites\n\n\n\n\n\n\nYou should already have a repository and source config set up from the previous page.\n\n\n\n\n\n\nYou can optionally set up a new staging environment. Or, you can simply re-use your source\n    environment for staging.\n\n\n\n\n\n\nProcedure\n\n\n\n\nNote\n\n\nRecall that, for simplicity's sake, this plugin requires that passwordless SSH is set up between\nyour staging and source environments. You may want to verify this before continuing.\n\n\n\n\n\n\n\n\nAs before, use \ndvp build\n and \ndvp upload\n to get your latest plugin changes installed onto\nthe Delphix Engine.\n\n\n\n\n\n\nGo to \nManage > Environments\n, select your \nsource\n environment, and then go to the \nDatabases\n tab. Find \nRepository for our First Plugin\n, and your source config underneath it.\n\n\n\n\n\n\nFrom your source config click \nAdd dSource\n. This will begin the linking process. The first\nscreen you see should ask for the properties that you recently added to your \nlinkedSourceDefinition\n. \n\n\n\n\n\n\nWalk through the remainder of the screens and hit \nSubmit\n. This will kick off the initial link and first sync.\n\n\n\n\n\n\nYou can confirm that your new dSource was added successfully by going to \nManage > Datasets\n.\n\n\n\n\n\n\nAfter you have finished entering this information, the initial sync process will begin. This is what will call your pre-snapshot operation, thus copying data.\n\n\n\n\nGotcha\n\n\nManually creating a dSource sets your plugin\u2019s linked source schema in stone, and you will have to recreate the dSource in order to modify your schema. We will cover how to deal with this correctly later, in the \nupgrade section\n. For now, if you need to change your plugin's linked source schema, you will have to first delete any dSources you have manually added.",
            "title": "Data Ingestion"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#data-ingestion",
            "text": "",
            "title": "Data Ingestion"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#how-does-delphix-ingest-data",
            "text": "As  previously  discussed, the Delphix Engine uses the  discovery  process to learn about datasets that live on a  source environment . In this section we will learn how the Delphix Engine uses a two-step process to ingest a dataset.",
            "title": "How Does Delphix Ingest Data?"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#linking",
            "text": "The first step is called  linking . This is simply the creation of a new dataset on the Delphix Engine, which is associated with the dataset on the source environment. This new linked dataset is called a  dSource .",
            "title": "Linking"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#syncing",
            "text": "Immediately after linking, the new dSource is  synced  for the first time. Syncing is a process by which data from the source environment is copied onto the Delphix Engine. Subsequent syncs may then be periodically performed in order to keep the dSource up-to-date.  The details of how this is done varies significantly from plugin to plugin. For example, some plugins will simply copy files from the filesystem. Other plugins might contact a DBMS and instruct it to send backup or replication streams. There are many possibilities here, but they all break down into two main strategies that the plugin author can choose from: direct and staging.  With the  direct  strategy, the plugin is not in charge\nof the data copying. Instead the Delphix Engine directly pulls raw data from the source environment.\nThe plugin merely provides the location of the data. This is a very simple strategy, and is also\nquite limiting.  For our first plugin, we will be using the more flexible  staging  strategy. With this strategy, the Delphix Engine uses NFS for Unix environments (or iSCSI on Windows environments) to mount storage onto a  staging environment . Our plugin will then be in full control of how to get data from the source environment onto this storage mount.  With the staging strategy, there are two types of syncs: sync and resync. A  sync  is used to ingestion incremental changes while a  resync  is used to re-ingest all the data for the dSource. For databases, this could mean re-ingesting from a full database backup to reset the dSource. A  sync  and a  resync  execute the same plugin operations and are differentiated by a boolean flag in the  snapshot_parameters  argument passed into  linked.pre_snapshot  and  linked.post_snapshot .  A regular  sync  is the default and is executed as part of policy driven syncs. A  resync  is only executed during initial ingestion or if the Delphix user manually starts one. The customer can manually trigger a  resync  via the UI by selecting the dSource, going to more options and selecting  Resynchronize dSource .    Gotcha  Although it is not common, it is entirely possible that the staging environment is the same as the source environment. Be careful not to assume otherwise in your plugins.",
            "title": "Syncing"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#our-syncing-strategy",
            "text": "For our purposes here in this intro plugin, we will use a simple strategy. We won't do anything with the resync snapshot parameter and simply copy files from the filesystem on the source environment onto the NFS mount on the staging environment. We will do this by running the Unix tool  rsync  from our staging environment, and rely on passwordless SSH to connect to the source environment.   Info  This plugin is assuming that  rsync  is installed on the staging host, and that the staging\nhost user is able to SSH into the source host without having to type in a password. A more\nfull-featured plugin would test these assumptions, usually as part of discovery.   In the special case mentioned above, where the staging environment is the same as the source environment, we could likely do something more efficient. However, for simplicity's sake, we won't do that here.",
            "title": "Our Syncing Strategy"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#defining-your-linked-source-data-format",
            "text": "In order to be able to successfully do the copying required, plugins might need to get some information from the end-user of your plugin. In our case, we need to tell  rsync  how to access the files. This means we need to know the source environment's IP address (or domain name), the username we need to connect as, and finally the location where the files live.  Again, we will be using a JSON schema to define the data format. The user will be presented with a UI that lets them provide all the information our schema specifies.  Open up  schema.json  in your editor/IDE. Locate the  LinkedSourceDefinition  and replace it with the following schema:  \"linkedSourceDefinition\" :   { \n     \"type\" :   \"object\" , \n     \"additionalProperties\" :   false , \n     \"required\" :   [ \"sourceAddress\" ,   \"username\" ,   \"mountLocation\" ], \n     \"properties\" :   { \n         \"sourceAddress\" :   { \n             \"type\" :   \"string\" , \n             \"prettyName\" :   \"Host from which to copy\" , \n             \"description\" :   \"IP or FQDN of host from which to copy\" \n         }, \n         \"username\" :   { \n             \"type\" :   \"string\" , \n             \"prettyName\" :   \"Username on Source Host\" , \n             \"description\" :   \"Username for making SSH connection to source host\" \n         }, \n         \"mountLocation\" :   { \n             \"type\" :   \"string\" , \n             \"format\" :   \"unixpath\" , \n             \"prettyName\" :   \"Mount Location on Staging Host\" , \n             \"description\" :   \"Where to mount storage onto the staging host while syncing\" \n         } \n     }  } ,    Info  As will be explained later, this schema will be used to generate Python code.\nAll names in the autogenerated Python code will use  lower_case_with_underscores  as attribute names as per Python variable naming conventions.\nThat is, if we were to use  mountLocation  as the schema property name, it would be called mount_location  in the generated Python code.   With this schema, the user will be required to provide the source username, the source's IP address, and the staging mount location as part of the linking process.",
            "title": "Defining Your Linked Source Data Format"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#implementing-syncing-in-your-plugin",
            "text": "There are three things we must do to implement syncing. First, we need to tell the Delphix Engine\nwhere to mount storage onto the staging environment. Next we need to actually do the work of copying\ndata onto that mounted storage. Finally, we need to generate any snapshot-related data.",
            "title": "Implementing Syncing in Your Plugin"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#mount-specification",
            "text": "Before syncing can begin, the Delphix Engine needs to mount some storage onto the staging host.\nSince different plugins can have different requirements about where exactly this mount lives, it is\nup to the plugin to specify this location. As mentioned above, our simple plugin will get this\nlocation from the user.  Open up the  plugin_runner.py  file and find the  linked_mount_specification  function (which was generated by  dvp init ). Replace it with the following code:  @plugin.linked.mount_specification()\ndef linked_mount_specification(staged_source, repository):\n    mount_location = staged_source.parameters.mount_location\n    mount = Mount(staged_source.staged_connection.environment, mount_location)\n    return MountSpecification([mount])  Let's take this line-by-line to see what's going on here.  @plugin.linked.mount_specification()  This  decorator  announces that the following function\nis the code that handles the  mount_specification  operation. This is what allows the Delphix\nEngine to know which function to call when it's time to learn where to mount. Every operation\ndefinition will begin with a similar decorator.  def linked_mount_specification(staged_source, repository):  This begins a Python function definition. We chose to call it  linked_mount_specification , but we\ncould have chosen any name at all. This function accepts two arguments, one giving information about\nthe linked source, and one giving information about the associated repository.      mount_location = staged_source.parameters.mount_location  The  staged_source  input argument contains an attribute called  parameters . This in turn contains\nall of the properties defined by the  linkedSourceDefinition  schema. So, in our case, that means\nit will contain attributes called  source_address ,  username , and  mount_location . Note how any attribute defined in  camelCase  in the schema is converted to  variable_with_underscores . This line\nsimply retrieves the user-provided mount location and saves it in a local variable.      mount = Mount(staged_source.staged_connection.environment, mount_location)  This line constructs a new object from the  Mount class . This class\nholds details about how Delphix Engine storage is mounted onto remote environments. Here, we\ncreate a mount object that says to mount onto the staging environment, at the location specified\nby the user.      return MountSpecification([mount])  On the line just before this one, we created an object that describes a  single  mount. Now, we\nmust return a full  mount specification . In general,\na mount specification is a collection of mounts. But, in our case, we just have one single mount.\nTherefore, we use an array with only one item it in -- namely, the one single mount object we\ncreated just above.",
            "title": "Mount Specification"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#data-copying",
            "text": "As explained  here , the Delphix Engine will always run the plugin's  preSnapshot  operation just before taking a snapshot of the dsource. That means our  preSnapshot  operation has to get the NFS share into the desired state. For us, that means that's the time to do our data copy.  Unlike the previous operations we've seen so far, the pre-snapshot operation will not be autogenerated by  dvp init .\nSo, we will need to add one ourselves.  Open up the  plugin_runner.py  file.  First, we'll add a new import line near the top of the file, so that we can use Delphix's platform libraries and raise user visible errors (explained below).  from   dlpx.virtualization   import   libs  from   dlpx.virtualization.platform.exceptions   import   UserError   Next, we'll add a new function:  @plugin.linked.pre_snapshot ()  def   copy_data_from_source ( staged_source ,   repository ,   source_config ,   snapshot_parameters ): \n     stage_mount_path   =   staged_source . mount . mount_path \n     data_location   =   \"{}@{}:{}\" . format ( staged_source . parameters . username , \n         staged_source . parameters . source_address , \n         source_config . path ) \n\n     rsync_command   =   \"rsync -r {} {}\" . format ( data_location ,   stage_mount_path ) \n\n     result   =   libs . run_bash ( staged_source . staged_connection ,   rsync_command ) \n\n     if   result . exit_code   !=   0 : \n         raise   UserError ( \n             \"Could not copy files.\" , \n             \"Ensure that passwordless SSH works for {}.\" . format ( staged_source . parameters . source_address ), \n             result . stderr )   Let's walk through this function and see what's going on       stage_mount_path   =   staged_source . mount . mount_path   The  staged_source  argument contains information about the current mount location. Here we save that\nto a local variable for convenience.       data_location   =   \"{}@{}:{}\" . format ( staged_source . parameters . username , \n         staged_source . parameters . source_address , \n         source_config . path )   This code creates a Python string that represents the location of the data that we want to ingest.\nThis is in the form  <user>@<host>:<path> . For example  jdoe@sourcehost.mycompany.com:/bin . As\nbefore with  mountLocation , we have defined our schemas such that these three pieces of information\nwere provided by the user. Here we're just putting them into a format that  rsync  will understand.       rsync_command   =   \"rsync -r {} {}\" . format ( data_location ,   stage_mount_path )   This line is the actual Bash command that we'll be running on the staging host. This will look something like  rsync -r user@host:/source/path /staging/mount/path .       result   =   libs . run_bash ( staged_source . staged_connection ,   rsync_command )   This is an example of a  platform library  function, where we ask the Virtualization Platform\nto do some work on our behalf. In this case, we're asking the platform to run our Bash command on the\nstaging environment. For full details on the  run_bash  platform library function and others, see this  reference .       if   result . exit_code   !=   0 : \n         raise   UserError ( \n             \"Could not copy files.\" , \n             \"Ensure that passwordless SSH works for {}.\" . format ( staged_source . parameters . source_address ), \n             result . stderr )   Finally, we check to see if our Bash command actually worked okay. If not, we raise an error\nmessage, and describe one possible problem for the user to investigate. For more details on raising user visible errors, see this  reference .",
            "title": "Data Copying"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#saving-snapshot-data",
            "text": "Whenever the Delphix Engine takes a  snapshot  of a dSource or VDB,\nthe plugin has the chance to save any information it likes alongside that snapshot. Later, if the\nsnapshot is ever used to provision a new VDB, the plugin can use the previously-saved information\nto help get the new VDB ready for use.  The format of this data is controlled by the plugin's  snapshotDefinition  schema. In our case, we\ndon't have any data we need to save. So, there's not much to do here. We will not modify the blank\nschema that was created by  dvp init .  We do still need to provide python function for the engine to call, but we don't have to do much.\nIn fact, the default implementation that was generated by  dvp init  will work just fine for our purposes:  @plugin.linked.post_snapshot ()  def   linked_post_snapshot ( staged_source , \n                          repository , \n                          source_config , \n                          snapshot_parameters ): \n     return   SnapshotDefinition ()   The only thing this code is doing is creating a new object using our (empty) snapshot\ndefinition, and returning that new empty object.",
            "title": "Saving Snapshot Data"
        },
        {
            "location": "/Building_Your_First_Plugin/Data_Ingestion/#how-to-link-and-sync-in-the-delphix-engine",
            "text": "Let's try it out and make sure this works!  Prerequisites    You should already have a repository and source config set up from the previous page.    You can optionally set up a new staging environment. Or, you can simply re-use your source\n    environment for staging.    Procedure   Note  Recall that, for simplicity's sake, this plugin requires that passwordless SSH is set up between\nyour staging and source environments. You may want to verify this before continuing.     As before, use  dvp build  and  dvp upload  to get your latest plugin changes installed onto\nthe Delphix Engine.    Go to  Manage > Environments , select your  source  environment, and then go to the  Databases  tab. Find  Repository for our First Plugin , and your source config underneath it.    From your source config click  Add dSource . This will begin the linking process. The first\nscreen you see should ask for the properties that you recently added to your  linkedSourceDefinition .     Walk through the remainder of the screens and hit  Submit . This will kick off the initial link and first sync.    You can confirm that your new dSource was added successfully by going to  Manage > Datasets .    After you have finished entering this information, the initial sync process will begin. This is what will call your pre-snapshot operation, thus copying data.   Gotcha  Manually creating a dSource sets your plugin\u2019s linked source schema in stone, and you will have to recreate the dSource in order to modify your schema. We will cover how to deal with this correctly later, in the  upgrade section . For now, if you need to change your plugin's linked source schema, you will have to first delete any dSources you have manually added.",
            "title": "How to Link and Sync in the Delphix Engine"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/",
            "text": "Provisioning\n\u00b6\n\n\nWhat is Provisioning?\n\u00b6\n\n\nOnce Delphix has a \nsnapshot\n of a dataset (for example of a dSource), it is possible to quickly clone that snapshot to create a new \nvirtual dataset\n. This new virtual dataset will be made available for use on a \ntarget environment\n. This process is called \nprovisioning\n.\n\n\nOur Provisioning Strategy\n\u00b6\n\n\nFor many plugins, there is a lot of work that needs to be done before a newly-provisioned virtual dataset can be made useful. For example, it might need to be registered with a running DBMS. Or, maybe some data inside the dataset needs to be changed so it behaves properly on the target environment.\n\n\nIn our case, however, there is very little to do. All we really require is that the files in the virtual dataset are accessible at some path on the target environment. Since the Delphix Engine takes care of mounting the data, we only need to worry about controlling \nwhere\n that data is mounted.\n\n\nDefining our Provision-Related Data Formats\n\u00b6\n\n\nWe have already seen four custom data formats: for repositories, source configs, snapshots and\nlinked sources. The final one is used for \nvirtual sources\n.\n\n\nRecall that, for our plugin, a VDB is just a directory full of files. There is no special\nprocedure needed to enable it, no DBMS to coordinate with, etc. All we need to do is make the files\navailable on the target environment.\n\n\nSo, the only question for the user is \"Where should these files live?\"\n\n\nOpen up \nschema.json\n, locate the \nvirtualSourceDefintion\n section, and change it to look like this:\n\n\n\"virtualSourceDefinition\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"additionalProperties\"\n \n:\n \nfalse\n,\n\n    \n\"required\"\n:\n \n[\n\"mountLocation\"\n],\n\n    \n\"properties\"\n \n:\n \n{\n\n        \n\"mountLocation\"\n:\n \n{\n\n            \n\"type\"\n:\n \n\"string\"\n,\n\n            \n\"format\"\n:\n \n\"unixpath\"\n,\n\n            \n\"prettyName\"\n:\n \n\"Mount Location on Target Host\"\n,\n\n            \n\"description\"\n:\n \n\"Where to mount VDB onto the target host\"\n\n        \n}\n\n    \n}\n\n\n}\n,\n\n\n\n\n\n\nThis should look familiar from the source config schema that we did earlier. We only have one\nproperty, and it represents the mount location on the target environment.\n\n\nImplementing Provisioning\n\u00b6\n\n\nThere are numerous ways for a plugin to customize the provisioning process.\nFor our example plugin, we just need to do a few things:\n\n\n\n\nTell Delphix where to mount the virtual dataset.\n\n\nCreate a \nsourceConfig\n to represent each newly-provisioned virtual dataset.\n\n\nModify an existing \nsourceConfig\n, if necessary, when the virtual dataset is refreshed or rewound.\n\n\nConstruct snapshot-related data any time a snapshot is taken of the virtual dataset.\n\n\n\n\nControlling Mounting\n\u00b6\n\n\nAs we saw previously with linked sources, we need to tell Delphix where to mount the dataset. Open\nup \nplugin_runner.py\n and find the \nplugin.virtual.mount_specification\n decorator. Change that function so that\nit looks like this:\n\n\n@plugin.virtual.mount_specification\n()\n\n\ndef\n \nvdb_mount_spec\n(\nvirtual_source\n,\n \nrepository\n):\n\n    \nmount_location\n \n=\n \nvirtual_source\n.\nparameters\n.\nmount_location\n\n    \nmount\n \n=\n \nMount\n(\nvirtual_source\n.\nconnection\n.\nenvironment\n,\n \nmount_location\n)\n\n    \nreturn\n \nMountSpecification\n([\nmount\n])\n\n\n\n\n\n\nAs we did with linked sources, we just look up what the user told us, and then package that up\nand return it to Delphix.\n\n\nCreating a Source Config for a new VDB\n\u00b6\n\n\nJust like we saw earlier with \nlinked datasets\n, each virtual dataset will need its own source config so that the Delphix Engine can interact with it. Our plugin is in charge of creating that source config at provision time\n\n\nAs a reminder, here is what our schema looks like for source configs:\n\n\n\"sourceConfigDefinition\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"required\"\n:\n \n[\n\"name\"\n,\n \n\"path\"\n],\n\n    \n\"additionalProperties\"\n:\n \nfalse\n,\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"name\"\n:\n \n{\n\n          \n\"type\"\n:\n \n\"string\"\n,\n\n          \n\"prettyName\"\n:\n \n\"Dataset Name\"\n,\n\n          \n\"description\"\n:\n \n\"User-visible name for this dataset\"\n\n        \n},\n\n        \n\"path\"\n:\n \n{\n\n          \n\"type\"\n:\n \n\"string\"\n,\n\n          \n\"format\"\n:\n \n\"unixpath\"\n,\n\n          \n\"prettyName\"\n:\n \n\"Path\"\n,\n\n          \n\"description\"\n:\n \n\"Full path to data location on the remote environment\"\n\n        \n}\n\n    \n},\n\n    \n\"nameField\"\n:\n \n\"name\"\n,\n\n    \n\"identityFields\"\n:\n \n[\n\"path\"\n]\n\n\n}\n,\n\n\n\n\n\n\nThus, for each newly-cloned virtual dataset, we create a new source config object with a name and a\npath. This is done by the \nconfigure\n plugin operation.\n\n\nIn addition to generating a new source config, the configure operation is also tasked with getting\nthe newly-cloned dataset ready for use on the target environment. What this means exactly will vary\nfrom plugin to plugin. For our simple plugin, the dataset does not require any setup work, and so we\nonly have to worry about the source config.\n\n\nFind the \nplugin.virtual.configure\n decorator and change the function to look like this:\n\n\n@plugin.virtual.configure\n()\n\n\ndef\n \nconfigure_new_vdb\n(\nvirtual_source\n,\n \nsnapshot\n,\n \nrepository\n):\n\n    \nmount_location\n \n=\n \nvirtual_source\n.\nparameters\n.\nmount_location\n\n    \nname\n \n=\n \n\"VDB mounted at {}\"\n.\nformat\n(\nmount_location\n)\n\n    \nreturn\n \nSourceConfigDefinition\n(\npath\n=\nmount_location\n,\n \nname\n=\nname\n)\n\n\n\n\n\n\nModifying a Source Config after Rewind or Refresh\n\u00b6\n\n\nJust as a new VDB might need to be configured, a refreshed or rewound VDB might need to be\n\"reconfigured\" to handle the new post-refresh (or post-rewind) state of the VDB. So, just as there\nis a \nconfigure\n operation, there is also a \nreconfigure\n operation.\n\n\nThe main difference between the two is that \nconfigure\n must \ncreate\n a source config, but\n\nreconfigure\n needs to \nmodify\n a pre-existing source config.\n\n\nIn our simple plugin, there is no special work to do at reconfigure time, and there is no reason\nto modify anything about the source config. We just need to write a \nreconfigure\n operation that\nreturns the existing source config without making any changes. Find the \nplugin.virtual.reconfigure\n decorator and modify the function as follows.\n\n\n@plugin.virtual.reconfigure\n()\n\n\ndef\n \nreconfigure_existing_vdb\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n,\n \nsnapshot\n):\n\n    \nreturn\n \nsource_config\n\n\n\n\n\n\nSaving Snapshot Data\n\u00b6\n\n\nAs with our linked sources, we don't actually have anything we need to save when VDB snapshots are\ntaken. And, again, \ndvp init\n has created a post-snapshot operation that will work just fine for us without modification:\n\n\n@plugin.virtual.post_snapshot\n()\n\n\ndef\n \nvirtual_post_snapshot\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n    \nreturn\n \nSnapshotDefinition\n()\n\n\n\n\n\n\nHow To Provision in the Delphix Engine\n\u00b6\n\n\nFinally, let us try it out to make sure provisioning works!\n\n\n\n\nAgain, use \ndvp build\n and \ndvp upload\n to get your new changes onto your Delphix Engine.\n\n\nClick \nManage > Datasets\n.\n\n\nSelect the dSource you created in the last page. You should see at least one snapshot, and maybe more than one if you have manually taken a snapshot, or if you have a snapshot policy in place. Select one of these snapshots and click the \nProvision vFiles\n icon.\n\n\nThis will open the Provision VDB wizard. Complete the steps and select \nSubmit\n.\n  During VDB provisioning one of the things you will have to do is to provide the data required by your virtual source schema. In our case, that means you will be asked to provide a value for \nmountLocation\n. You will also be asked to choose a target environment on which the new VDB will live. After the wizard finishes, you will see a job appear in the \nActions\n tab on the right-hand side of the screen. When that job completes, your new VDB should be ready.\n\n\nTo ensure everything has worked correctly, log into to your target environment. From there, you can examine the directory you specified as the \nmountLocation\n. What you should see is a copy of the directory that you linked to with your dSource.",
            "title": "Provisioning"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#provisioning",
            "text": "",
            "title": "Provisioning"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#what-is-provisioning",
            "text": "Once Delphix has a  snapshot  of a dataset (for example of a dSource), it is possible to quickly clone that snapshot to create a new  virtual dataset . This new virtual dataset will be made available for use on a  target environment . This process is called  provisioning .",
            "title": "What is Provisioning?"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#our-provisioning-strategy",
            "text": "For many plugins, there is a lot of work that needs to be done before a newly-provisioned virtual dataset can be made useful. For example, it might need to be registered with a running DBMS. Or, maybe some data inside the dataset needs to be changed so it behaves properly on the target environment.  In our case, however, there is very little to do. All we really require is that the files in the virtual dataset are accessible at some path on the target environment. Since the Delphix Engine takes care of mounting the data, we only need to worry about controlling  where  that data is mounted.",
            "title": "Our Provisioning Strategy"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#defining-our-provision-related-data-formats",
            "text": "We have already seen four custom data formats: for repositories, source configs, snapshots and\nlinked sources. The final one is used for  virtual sources .  Recall that, for our plugin, a VDB is just a directory full of files. There is no special\nprocedure needed to enable it, no DBMS to coordinate with, etc. All we need to do is make the files\navailable on the target environment.  So, the only question for the user is \"Where should these files live?\"  Open up  schema.json , locate the  virtualSourceDefintion  section, and change it to look like this:  \"virtualSourceDefinition\" :   { \n     \"type\" :   \"object\" , \n     \"additionalProperties\"   :   false , \n     \"required\" :   [ \"mountLocation\" ], \n     \"properties\"   :   { \n         \"mountLocation\" :   { \n             \"type\" :   \"string\" , \n             \"format\" :   \"unixpath\" , \n             \"prettyName\" :   \"Mount Location on Target Host\" , \n             \"description\" :   \"Where to mount VDB onto the target host\" \n         } \n     }  } ,   This should look familiar from the source config schema that we did earlier. We only have one\nproperty, and it represents the mount location on the target environment.",
            "title": "Defining our Provision-Related Data Formats"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#implementing-provisioning",
            "text": "There are numerous ways for a plugin to customize the provisioning process.\nFor our example plugin, we just need to do a few things:   Tell Delphix where to mount the virtual dataset.  Create a  sourceConfig  to represent each newly-provisioned virtual dataset.  Modify an existing  sourceConfig , if necessary, when the virtual dataset is refreshed or rewound.  Construct snapshot-related data any time a snapshot is taken of the virtual dataset.",
            "title": "Implementing Provisioning"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#controlling-mounting",
            "text": "As we saw previously with linked sources, we need to tell Delphix where to mount the dataset. Open\nup  plugin_runner.py  and find the  plugin.virtual.mount_specification  decorator. Change that function so that\nit looks like this:  @plugin.virtual.mount_specification ()  def   vdb_mount_spec ( virtual_source ,   repository ): \n     mount_location   =   virtual_source . parameters . mount_location \n     mount   =   Mount ( virtual_source . connection . environment ,   mount_location ) \n     return   MountSpecification ([ mount ])   As we did with linked sources, we just look up what the user told us, and then package that up\nand return it to Delphix.",
            "title": "Controlling Mounting"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#creating-a-source-config-for-a-new-vdb",
            "text": "Just like we saw earlier with  linked datasets , each virtual dataset will need its own source config so that the Delphix Engine can interact with it. Our plugin is in charge of creating that source config at provision time  As a reminder, here is what our schema looks like for source configs:  \"sourceConfigDefinition\" :   { \n     \"type\" :   \"object\" , \n     \"required\" :   [ \"name\" ,   \"path\" ], \n     \"additionalProperties\" :   false , \n     \"properties\" :   { \n         \"name\" :   { \n           \"type\" :   \"string\" , \n           \"prettyName\" :   \"Dataset Name\" , \n           \"description\" :   \"User-visible name for this dataset\" \n         }, \n         \"path\" :   { \n           \"type\" :   \"string\" , \n           \"format\" :   \"unixpath\" , \n           \"prettyName\" :   \"Path\" , \n           \"description\" :   \"Full path to data location on the remote environment\" \n         } \n     }, \n     \"nameField\" :   \"name\" , \n     \"identityFields\" :   [ \"path\" ]  } ,   Thus, for each newly-cloned virtual dataset, we create a new source config object with a name and a\npath. This is done by the  configure  plugin operation.  In addition to generating a new source config, the configure operation is also tasked with getting\nthe newly-cloned dataset ready for use on the target environment. What this means exactly will vary\nfrom plugin to plugin. For our simple plugin, the dataset does not require any setup work, and so we\nonly have to worry about the source config.  Find the  plugin.virtual.configure  decorator and change the function to look like this:  @plugin.virtual.configure ()  def   configure_new_vdb ( virtual_source ,   snapshot ,   repository ): \n     mount_location   =   virtual_source . parameters . mount_location \n     name   =   \"VDB mounted at {}\" . format ( mount_location ) \n     return   SourceConfigDefinition ( path = mount_location ,   name = name )",
            "title": "Creating a Source Config for a new VDB"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#modifying-a-source-config-after-rewind-or-refresh",
            "text": "Just as a new VDB might need to be configured, a refreshed or rewound VDB might need to be\n\"reconfigured\" to handle the new post-refresh (or post-rewind) state of the VDB. So, just as there\nis a  configure  operation, there is also a  reconfigure  operation.  The main difference between the two is that  configure  must  create  a source config, but reconfigure  needs to  modify  a pre-existing source config.  In our simple plugin, there is no special work to do at reconfigure time, and there is no reason\nto modify anything about the source config. We just need to write a  reconfigure  operation that\nreturns the existing source config without making any changes. Find the  plugin.virtual.reconfigure  decorator and modify the function as follows.  @plugin.virtual.reconfigure ()  def   reconfigure_existing_vdb ( virtual_source ,   repository ,   source_config ,   snapshot ): \n     return   source_config",
            "title": "Modifying a Source Config after Rewind or Refresh"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#saving-snapshot-data",
            "text": "As with our linked sources, we don't actually have anything we need to save when VDB snapshots are\ntaken. And, again,  dvp init  has created a post-snapshot operation that will work just fine for us without modification:  @plugin.virtual.post_snapshot ()  def   virtual_post_snapshot ( virtual_source ,   repository ,   source_config ): \n     return   SnapshotDefinition ()",
            "title": "Saving Snapshot Data"
        },
        {
            "location": "/Building_Your_First_Plugin/Provisioning/#how-to-provision-in-the-delphix-engine",
            "text": "Finally, let us try it out to make sure provisioning works!   Again, use  dvp build  and  dvp upload  to get your new changes onto your Delphix Engine.  Click  Manage > Datasets .  Select the dSource you created in the last page. You should see at least one snapshot, and maybe more than one if you have manually taken a snapshot, or if you have a snapshot policy in place. Select one of these snapshots and click the  Provision vFiles  icon.  This will open the Provision VDB wizard. Complete the steps and select  Submit .\n  During VDB provisioning one of the things you will have to do is to provide the data required by your virtual source schema. In our case, that means you will be asked to provide a value for  mountLocation . You will also be asked to choose a target environment on which the new VDB will live. After the wizard finishes, you will see a job appear in the  Actions  tab on the right-hand side of the screen. When that job completes, your new VDB should be ready.  To ensure everything has worked correctly, log into to your target environment. From there, you can examine the directory you specified as the  mountLocation . What you should see is a copy of the directory that you linked to with your dSource.",
            "title": "How To Provision in the Delphix Engine"
        },
        {
            "location": "/Versioning_And_Upgrade/Overview/",
            "text": "Overview\n\u00b6\n\n\nOnce you start writing and releasing your plugin, you\u2019ll reach a point when bug fixes or new features may require schema changes. The plugin upgrade process enables objects that have been created with a prior schema to be migrated to the newly defined schema. When this happens, a new version of the plugin must be created. The following few pages will walk through how versions need to change between upgrades and what needs to be written in the plugin to make sure upgrade is successful.\n\n\nPlugin Versioning\n\u00b6\n\n\nLike any other piece of software, plugins change over time. Every so often, there will be a new release. To keep track of the different releases, each plugin release has its own versioning information. Depending on what changes are included in a particular release, there are different rules and recommendations for how the versioning information should be changed. More information on versioning is located \nhere\n.\n\n\nUpgrade\n\u00b6\n\n\nUpgrade is the process by which an older version of a plugin is replaced by a newer version. Depending on what has changed between the two versions, this process may also include modifying pre-existing plugin defined objects so they conform to the new schema expected by the new version of the plugin. Information on the upgrade process can be found \nhere\n.",
            "title": "Overview"
        },
        {
            "location": "/Versioning_And_Upgrade/Overview/#overview",
            "text": "Once you start writing and releasing your plugin, you\u2019ll reach a point when bug fixes or new features may require schema changes. The plugin upgrade process enables objects that have been created with a prior schema to be migrated to the newly defined schema. When this happens, a new version of the plugin must be created. The following few pages will walk through how versions need to change between upgrades and what needs to be written in the plugin to make sure upgrade is successful.",
            "title": "Overview"
        },
        {
            "location": "/Versioning_And_Upgrade/Overview/#plugin-versioning",
            "text": "Like any other piece of software, plugins change over time. Every so often, there will be a new release. To keep track of the different releases, each plugin release has its own versioning information. Depending on what changes are included in a particular release, there are different rules and recommendations for how the versioning information should be changed. More information on versioning is located  here .",
            "title": "Plugin Versioning"
        },
        {
            "location": "/Versioning_And_Upgrade/Overview/#upgrade",
            "text": "Upgrade is the process by which an older version of a plugin is replaced by a newer version. Depending on what has changed between the two versions, this process may also include modifying pre-existing plugin defined objects so they conform to the new schema expected by the new version of the plugin. Information on the upgrade process can be found  here .",
            "title": "Upgrade"
        },
        {
            "location": "/Versioning_And_Upgrade/Versioning/",
            "text": "Versioning\n\u00b6\n\n\nAlmost all software products are periodically updated to include new features and bug fixes. Plugins are no exception -- a plugin's code will very likely be different two years from now.\n\n\nTo deal with this, plugins use \nversioning\n. This just means that a plugin communicates (to the user, and to the Delphix Engine) exactly what code is in use.\n\n\nVersioning Information\n\u00b6\n\n\nThere are three different pieces of version-related information, each used for different purposes.\n\n\nExternal Version\n\u00b6\n\n\nThis field is intended only for use by the end user. The Delphix Engine does not use this field, and therefore imposes no restrictions on its content. This is a free-form string which the plugin can use in any way it feels like.\n\n\nExamples might be \"5.3.0\", \"2012B\", \"MyPlugin Millennium Edition, Service Pack 3\", \"Playful Platypus\" or \"Salton City\".\n\n\nThe external version is specified using the \nexternalVersion\n property in your \nplugin config\n file.\n\n\n\n\nTip\n\n\nUse an external version that makes it easier for end users to determine newer vs older plugins.\n\n\n\n\nBuild Number\n\u00b6\n\n\nUnlike \"external version\", this field is intended to convey information to the Delphix Engine. This is a string of integers, separated by periods. Examples would be \"5.3.0\", \"7\", \"5.3.0.0.0.157\".\n\n\nThe Delphix Engine uses the build number to guard against end users trying to \"downgrade\" their plugin to an older, incompatible version. So, if a user has build number \"3.4.1\" installed, then they may not install a version with a build number like \"2.x.y\", \"3.3.y\" or \"3.4.0\".\n\n\nThe build number is specified using the \nbuildNumber\n property in your \nplugin config\n file.\n\n\nThis field is required to be a string. You might need to enclose your build number in quotes in order to prevent YAML from interpreting the field as a number. Examples:\n\n\n\n\n\n\n\n\nbuildNumber\n\n\nAllowed\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\n1\n\n\nNo\n\n\nYAML will interpret this as an integer.\n\n\n\n\n\n\n1.2\n\n\nNo\n\n\nYAML will interpret this as a floating-point number.\n\n\n\n\n\n\n\"1\"\n\n\nYes\n\n\nThe quotes mean this is a string.\n\n\n\n\n\n\n\"1.2\"\n\n\nYes\n\n\nThe quotes mean this is a string.\n\n\n\n\n\n\n1.2.3\n\n\nYes\n\n\nYAML treats this as a string, since it cannot be a number.\n\n\n\n\n\n\n\n\nBuild Number Format Rules\n\u00b6\n\n\nYour build number must be a string, conforming to these rules:\n\n\n\n\nThe string must be composed of a sequence of non-negative integers, not all zero, separated by periods.\n\n\nTrailing zeros are ignored. So, \"1.0.0\" is treated the same as \"1\".\n\n\nBuild numbers are sortable numerically, with earlier numbers having more significance than later numbers. So, \"2.0\" comes after \"1.99999\", and \"1.10\" comes after \"1.2\".\n\n\nThe Delphix Engine will never allow installation of plugin with a build number that is ordered before the the already-installed build number.\n\n\n\n\n\n\nTip\n\n\nYou can upload a plugin with the same \nbuildNumber\n as the installed plugin. However this should only be done while a plugin is being developed. Plugin releases for end users should never have the same \nbuildNumber\n\n\n\n\nPlease also see the \nApp-Style vs. Enterprise-Style section\n below. We generally recommend using a single integer build number for app-style development. Build numbers need to have multiple parts if you are doing enterprise-style development.\n\n\nRelease Strategies\n\u00b6\n\n\nThere are two main strategies for releasing software:\n\n\n\"App-style\" Release Strategy\n\u00b6\n\n\nHere, all users are expected to use the latest available version of the software. Most consumer software works this way today -- websites, phone apps, etc. An app-style strategy is much simpler, but also more limiting:\n\n\n\n\nAt any time, there is only one branch under active development.\n\n\nCustomers that want bugfixes must upgrade to the latest version.\n\n\nThe plugin's build number can be a simple integer that is incremented with each new release.\n\n\n\n\n\"Enterprise-style\" Release Strategy\n\u00b6\n\n\nHere, you might distinguish \"major\" releases of your software from \"minor\" releases. You might expect some customers to continue to use older major releases for a long time, even after a new major release comes out. This strategy is often used for software like operating systems and DBMSs, where upgrading can cause significant disruption. An enterprise-style strategy is more flexible, but also more cumbersome:\n\n\n\n\nThere may be multiple branches under active development at any time. Typically one branch for every \"major release\" that is still being supported. This requires careful coordination to make sure that each new code change ends up on the correct branch (or branches).\n\n\nIt is possible to supply bugfix-only minor releases (often called \"patch releases\") which build atop older major releases. Customers do not need to move to the new major version in order to get these bugfixes.\n\n\nThe plugin's build number needs to be composed of multiple integers.\n\n\n\n\nIf you are using this strategy read more \nhere\n about how to deal with backports and hotfixes.\n\n\nYou may use whichever of these strategies works best for you. The SDK and the Delphix Engine support either strategy. You can even change your mind later and switch to the other strategy.\n\n\nRecommendations\n\u00b6\n\n\n\n\nBuild your plugin with the newest Virtualization SDK version available.\n\n\nOnly publish one artifact built for a given official version of the plugin.\n\n\nThe official release of a plugin should not use the same build number as a development build.\n\n\nUse an \nexternal version\n that helps easily identify newer plugins.\n\n\nPublish a plugin version compatibility matrix which lists out the plugin version, the Virtualization SDK it was built with and the Delphix Engine version(s) it supports.",
            "title": "Versioning"
        },
        {
            "location": "/Versioning_And_Upgrade/Versioning/#versioning",
            "text": "Almost all software products are periodically updated to include new features and bug fixes. Plugins are no exception -- a plugin's code will very likely be different two years from now.  To deal with this, plugins use  versioning . This just means that a plugin communicates (to the user, and to the Delphix Engine) exactly what code is in use.",
            "title": "Versioning"
        },
        {
            "location": "/Versioning_And_Upgrade/Versioning/#versioning-information",
            "text": "There are three different pieces of version-related information, each used for different purposes.",
            "title": "Versioning Information"
        },
        {
            "location": "/Versioning_And_Upgrade/Versioning/#external-version",
            "text": "This field is intended only for use by the end user. The Delphix Engine does not use this field, and therefore imposes no restrictions on its content. This is a free-form string which the plugin can use in any way it feels like.  Examples might be \"5.3.0\", \"2012B\", \"MyPlugin Millennium Edition, Service Pack 3\", \"Playful Platypus\" or \"Salton City\".  The external version is specified using the  externalVersion  property in your  plugin config  file.   Tip  Use an external version that makes it easier for end users to determine newer vs older plugins.",
            "title": "External Version"
        },
        {
            "location": "/Versioning_And_Upgrade/Versioning/#build-number",
            "text": "Unlike \"external version\", this field is intended to convey information to the Delphix Engine. This is a string of integers, separated by periods. Examples would be \"5.3.0\", \"7\", \"5.3.0.0.0.157\".  The Delphix Engine uses the build number to guard against end users trying to \"downgrade\" their plugin to an older, incompatible version. So, if a user has build number \"3.4.1\" installed, then they may not install a version with a build number like \"2.x.y\", \"3.3.y\" or \"3.4.0\".  The build number is specified using the  buildNumber  property in your  plugin config  file.  This field is required to be a string. You might need to enclose your build number in quotes in order to prevent YAML from interpreting the field as a number. Examples:     buildNumber  Allowed  Details      1  No  YAML will interpret this as an integer.    1.2  No  YAML will interpret this as a floating-point number.    \"1\"  Yes  The quotes mean this is a string.    \"1.2\"  Yes  The quotes mean this is a string.    1.2.3  Yes  YAML treats this as a string, since it cannot be a number.",
            "title": "Build Number"
        },
        {
            "location": "/Versioning_And_Upgrade/Versioning/#build-number-format-rules",
            "text": "Your build number must be a string, conforming to these rules:   The string must be composed of a sequence of non-negative integers, not all zero, separated by periods.  Trailing zeros are ignored. So, \"1.0.0\" is treated the same as \"1\".  Build numbers are sortable numerically, with earlier numbers having more significance than later numbers. So, \"2.0\" comes after \"1.99999\", and \"1.10\" comes after \"1.2\".  The Delphix Engine will never allow installation of plugin with a build number that is ordered before the the already-installed build number.    Tip  You can upload a plugin with the same  buildNumber  as the installed plugin. However this should only be done while a plugin is being developed. Plugin releases for end users should never have the same  buildNumber   Please also see the  App-Style vs. Enterprise-Style section  below. We generally recommend using a single integer build number for app-style development. Build numbers need to have multiple parts if you are doing enterprise-style development.",
            "title": "Build Number Format Rules"
        },
        {
            "location": "/Versioning_And_Upgrade/Versioning/#release-strategies",
            "text": "There are two main strategies for releasing software:",
            "title": "Release Strategies"
        },
        {
            "location": "/Versioning_And_Upgrade/Versioning/#app-style-release-strategy",
            "text": "Here, all users are expected to use the latest available version of the software. Most consumer software works this way today -- websites, phone apps, etc. An app-style strategy is much simpler, but also more limiting:   At any time, there is only one branch under active development.  Customers that want bugfixes must upgrade to the latest version.  The plugin's build number can be a simple integer that is incremented with each new release.",
            "title": "\"App-style\" Release Strategy"
        },
        {
            "location": "/Versioning_And_Upgrade/Versioning/#enterprise-style-release-strategy",
            "text": "Here, you might distinguish \"major\" releases of your software from \"minor\" releases. You might expect some customers to continue to use older major releases for a long time, even after a new major release comes out. This strategy is often used for software like operating systems and DBMSs, where upgrading can cause significant disruption. An enterprise-style strategy is more flexible, but also more cumbersome:   There may be multiple branches under active development at any time. Typically one branch for every \"major release\" that is still being supported. This requires careful coordination to make sure that each new code change ends up on the correct branch (or branches).  It is possible to supply bugfix-only minor releases (often called \"patch releases\") which build atop older major releases. Customers do not need to move to the new major version in order to get these bugfixes.  The plugin's build number needs to be composed of multiple integers.   If you are using this strategy read more  here  about how to deal with backports and hotfixes.  You may use whichever of these strategies works best for you. The SDK and the Delphix Engine support either strategy. You can even change your mind later and switch to the other strategy.",
            "title": "\"Enterprise-style\" Release Strategy"
        },
        {
            "location": "/Versioning_And_Upgrade/Versioning/#recommendations",
            "text": "Build your plugin with the newest Virtualization SDK version available.  Only publish one artifact built for a given official version of the plugin.  The official release of a plugin should not use the same build number as a development build.  Use an  external version  that helps easily identify newer plugins.  Publish a plugin version compatibility matrix which lists out the plugin version, the Virtualization SDK it was built with and the Delphix Engine version(s) it supports.",
            "title": "Recommendations"
        },
        {
            "location": "/Versioning_And_Upgrade/Upgrade/",
            "text": "Upgrade\n\u00b6\n\n\nUpgrade is the process of moving from an older version of a plugin to a newer version.\nUpgrading is not as simple as just replacing the installed plugin with a newer one.  The main complication comes when the new plugin version makes changes to its \nschemas\n.\n\n\nConsider the case of a plugin that works with collections of text files -- the user points it to a directory tree containing text files, and the plugin syncs the files from there.\n\n\nThe first release of such a plugin might have no link-related user options. So the plugin's linked source schema might define no properties at all:\n\n\n\"linkedSourceDefinition\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"additionalProperties\"\n \n:\n \nfalse\n,\n\n    \n\"properties\"\n \n:\n \n{\n\n    \n}\n\n\n}\n\n\n\n\n\n\nAnd, the syncing code is very simple:\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \nlinked_pre_snapshot\n(\ndirect_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n    \nlibs\n.\nrun_sync\n(\n\n        \nremote_connection\n \n=\n \ndirect_source\n.\nconnection\n,\n\n        \nsource_directory\n \n=\n \nsource_config\n.\npath\n\n        \n)\n\n\n\n\n\n\nBut, later, some users request a new feature -- they want to avoid syncing any backup or hidden files. So, a new plugin version is released. This time, there is a new boolean property in the linked source schema where users can elect to skip these files, if desired.\n\n\n\"linkedSourceDefinition\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"additionalProperties\"\n \n:\n \nfalse\n,\n\n    \n\"required\"\n:\n \n[\n\"skipHiddenAndBackup\"\n],\n\n    \n\"properties\"\n \n:\n \n{\n\n      \n\"skipHiddenAndBackup\"\n:\n \n{\n \n\"type\"\n:\n \n\"boolean\"\n \n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nThe plugin code that handles the syncing can now pay attention to this new boolean property:\n\n\n_HIDDEN_AND_BACKUP_SPECS\n \n=\n \n[\n\n    \n\"*.bak\"\n,\n\n    \n\"*~\"\n,\n  \n# Backup files from certain editors\n\n    \n\".*\"\n  \n# Unix-style hidden files\n\n\n]\n\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \nlinked_pre_snapshot\n(\ndirect_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n    \nexclude_spec\n \n=\n \n_HIDDEN_AND_BACKUP_SPECS\n \nif\n \ndirect_source\n.\nparameters\n.\nskip_hidden_and_backup\n \nelse\n \n[]\n\n\n    \nlibs\n.\nrun_sync\n(\n\n        \nremote_connection\n \n=\n \ndirect_source\n.\nconnection\n,\n\n        \nsource_directory\n \n=\n \nsource_config\n.\npath\n,\n\n        \nexclude_paths\n \n=\n \nexclude_spec\n\n        \n)\n\n\n\n\n\n\nSuppose a user has an engine with linked sources created by the older version of this plugin. That is, the existing linked sources have no \nskipHiddenAndBackup\n property.\n\n\nIf the user installs the new version of the plugin, we have a problem! The above \npre_snapshot\n code from the new plugin will attempt to access the \nskip_hidden_and_backup\n property, which we've just seen will not exist!\n\n\nThe solution to this problem is to use \ndata migrations\n, explained below.\n\n\n\n\nZero dSource and VDB downtime during plugin upgrade\n\n\ndSources and VDBs do not need to be disabled before a plugin upgrade is initiated. End users can continue to access data from existing VDBs during a plugin upgrade. However, while a particular plugin is in the process of being upgraded, no administrative Delphix Engine operations like: VDB Refresh, VDB Provision, dSource Disable/Enable etc will be allowed on the objects associated with that plugin. Objects associated with other plugins will not be affected.\n\n\n\n\nData Migrations\n\u00b6\n\n\nWhat is a Data Migration?\n\u00b6\n\n\nWhenever a new version of a plugin is installed on a Delphix Engine, the engine needs to migrate pre-existing data from its old format (as specified by the schemas in the old version of the plugin), to its new format (as specified by the schemas in the new version of the plugin).\n\n\nA \ndata migration\n is a function that is responsible for doing this conversion. It is provided by the plugin.\n\n\nThus, when the new plugin version is installed, the engine will call all applicable data migrations provided by the new plugin. This ensures that all data is always in the format expected by the new plugin.\n\n\nA Simple Example\n\u00b6\n\n\nLet's go back to the above example of the plugin that adds a new boolean option to allow users to avoid syncing backup and hidden files. Here is a data migration that the new plugin can provide to handle the data format change:\n\n\n@plugin.upgrade.linked_source\n(\n\"2019.11.20\"\n)\n\n\ndef\n \nadd_skip_option\n(\nold_linked_source\n):\n\n    \nreturn\n \n{\n\n      \n\"skipHiddenAndBackup\"\n:\n \nfalse\n\n    \n}\n\n\n\n\n\n\nThe exact rules for data migrations are covered in detail \nbelow\n. Here, we'll just walk through this code line by line and make some observations.\n\n\n@plugin.upgrade.linked_source\n(\n\"2019.11.20\"\n)\n\n\n\n\n\n\nThe above line is a \ndecorator\n that identifies the following function as a data migration. This particular migration will handle linked sources. It is given an ID of \n2019.11.20\n -- this controls when this migration is run in relation to other data migrations.\n\n\ndef\n \nadd_skip_option\n(\nold_linked_source\n):\n\n\n\n\n\n\nNote that the data migration takes an argument representing the old-format data. In this simple example, we know that there are no properties in the old-format data, so we can just ignore it.\n\n\n    \nreturn\n \n{\n\n      \n\"skipHiddenAndBackup\"\n:\n \nfalse\n\n    \n}\n\n\n\n\n\n\nHere, we are returning a Python dictionary representing the new format of the data. In this example, the dictionary has only one field: \nskipHiddenAndBackup\n. Because the old version of the plugin had no ability to skip files, we default this property to \nfalse\n to match the new schema.\n\n\nRules for Data Migrations\n\u00b6\n\n\nAs shown above, the a data migration receives old-format input and produces new-format output. The rules and recommendations for data migrations follow:\n\n\nRules\n\u00b6\n\n\n\n\n\n\nInput and output are Python dictionaries, with properties named exactly as specified in the schemas. Note that this differs from other plugin operations, where the inputs are defined with autogenerated Python \nclasses\n, and whose properties use Python-style naming.\n\n\n\n\n\n\nEach data migration must be tagged with an ID string. This string must consist of one or more positive integers separated by periods.\n\n\n\n\n\n\nData migration IDs must be numerically unique. Note that \n\"1.2\"\n, \n\"01.02\"\n, and \"\n1.2.0.0.0\"\n are all considered to be identical.\n\n\n\n\n\n\nOnce released, a data migration must never be deleted. An attempted upgrade will fail if the already-installed plugin version has a data migration that does not appear in the to-be-installed version.\n\n\n\n\n\n\nAt upgrade time, the engine will find the set of new migrations provided by the new version that are not already part of the already-installed version. Each of these migrations will then be run, in the order specified below.\n\n\n\n\n\n\nAfter running all applicable migrations, the engine will confirm that the resultant data conforms to the new version's schemas. If not, the upgrade will fail.\n\n\n\n\n\n\nNote that there is no requirement or guarantee that the input or output of any particular data migration will conform to a schema. We only guarantee that the input to the \nfirst\n data migration conforms to the schema of the already-installed plugin version. And, we only require that the output of the \nfinal\n data migration conforms to the schema of the new plugin version.\n\n\n\n\n\n\nData migrations are run in the order specified by their IDs. The ordering is numerical, not lexicographical. Thus \n\"1\"\n would run before \n\"2\"\n, which would run before \n\"10\"\n.\n\n\n\n\n\n\nData migrations have no access to \nPlatform Libraries\n or remote hosts. For example: If a data migration attempts to use \nrun_bash\n the upgrade will fail.\n\n\n\n\n\n\nNote that the above rules imply that at least one data migration is required any time a schema change is made that would invalidate any data produced using a previous version of the plugin. For example: adding a \n\"required\"\n property to the new schema.\n\n\n\n\n\n\nRecommendations\n\u00b6\n\n\n\n\n\n\nWe recommend using a \"Year.Month.Date\" format like \n\"2019.11.04\"\n for migration IDs. You can use trailing integers as necessary (e.g. use \n\"2019.11.04.5\"\n if you need something to be run between \n\"2019.11.04\"\n and \n\"2019.11.05\"\n).\n\n\n\n\n\n\nEven though they follow similar naming rules, migration IDs are not the same thing as plugin versions. We do not recommend using your plugin version in your migration IDs.\n\n\n\n\n\n\nWe recommend using small, single-purpose data migrations. That is, if you end up making four schema changes over the course of developing a new plugin version, we recommend writing four different data migrations, one for each change.\n\n\n\n\n\n\nData Migration Example\n\u00b6\n\n\nHere is a very simple data migration.\n\n\n@plugin.upgrade.repository\n(\n\"2019.12.15\"\n)\n\n\ndef\n \nadd_new_flag_to_repo\n(\nold_repository\n):\n\n  \nnew_repository\n \n=\n \ndict\n(\nold_repository\n)\n\n  \nnew_repository\n[\n\"useNewFeature\"\n]\n \n=\n \nFalse\n\n  \nreturn\n \nnew_repository\n\n\n\n\n\n\nDebugging Data Migration Problems\n\u00b6\n\n\nDuring the process of upgrading to a new version, the Delphix Engine will run all applicable data migrations, and then ensure that the resulting object matches the new schema. But, what if there is a bug, and the resulting object does \nnot\n match the schema?\n\n\nSecurity Concerns Prevent Detailed Error Messages\n\u00b6\n\n\nOne problem here is that the Delphix Engine is limited in the information that it can provide in the error message. Ideally, the engine would say exactly what was wrong with the object (e.g.: \"The field \nport\n has the value \n15\n, but the schema says it has to have a value between \n256\n and \n1024\n\").\n\n\nBut, the Delphix Engine cannot do this for security reasons. Ordinarily, the Delphix Engine knows which fields contain sensitive information, and can redact such fields from error messages. But, the only reason the Delphix Engine has that knowledge is because the schema provides that information. If an object does\n\nnot\n conform to the schema, then the Delphix Engine can't know what is sensitive and what isn't.\n\n\nTherefore, the error message here might lack the detail necessary to debug the problem.\n\n\nOne Solution: Temporary Logging\n\u00b6\n\n\nDuring development of a new plugin version, you may find yourself trying to find and fix such a bug.\nOne technique is to use temporary logging.\n\n\nFor example, while you are trying to locate and fix the bug, you could put a log statement at the very end of each of your data migrations, like so:\n\n\n  logger.debug(\"Migration 2010.03.01 returning {}\".format(new_object))\n  return new_object\n\n\n\n\n\nSee the \nLogging\n section for more information about logging works.\n\n\nFrom the logs, you'll be able to see exactly what each migration is returning. From there, hopefully the problem will become apparent. As a supplemental tool, consider pasting these results (along with your schema) into an online JSON validator for more information.\n\n\n\n\nWarning\n\n\nIt is \nvery important\n that you only use logging as a temporary debugging strategy. \nSuch logging must be removed before you release the plugin to end users\n. If this logging ends up in your end product, it could cause a serious security concern. Please see our \nsensitive data best practices\n for more information.\n\n\n\n\nWhen Data Migrations Are Insufficient\n\u00b6\n\n\nNew versions of plugins often require some modification of data that was written using an older version of the same plugin. Data migrations handle this modification. Unfortunately, data migrations cannot always fully handle all possible upgrade scenarios by themselves.\n\n\nFor example, a new plugin version might want to add a new required field to one of its schemas. But, the correct value for this new field might not be knowable while the upgrade is underway -- perhaps it must be entered by the user, or perhaps it would require automatic discovery to be rerun.\n\n\nSuch a situation will require some user intervention after the upgrade.\n\n\nIn all cases, of course you will want to \nclearly document\n to your users that there will extra work required so they can make sure they known what they are getting into before they decide to upgrade.\n\n\n\n\nTip\n\n\nIt should also be said that you should try to avoid cases like this.  As much as possible, try to make your post-upgrade plugin function with no user intervention. Only resort to user intervention as a last resort.\n\n\n\n\nThe recommended strategy here is to arrange for the affected objects to be in an \"invalid\" state, and for your plugin code to detect this state, and throw errors when the objects are used.\n\n\nFor such a situation, we recommend the following process:\n\n\n\n\nMake your schema changes so that the affected property can be set in such a way that plugin code can identify it as being invalid. Typically this is done by allowing for some \"sentinel\" value. This may require you to have a less-strict schema definition than you might otherwise want.\n\n\nIn your data migrations, make sure the affected properties are indeed marked invalid.\n\n\nIn any plugin code that needs to use these properties, first check them for validity. If they are invalid, then raise an error that explains the situation to the user, and tells them what steps they need to take.\n\n\n\n\nFollowing are two examples of schema changes that need extra user intervention after upgrade. One will require a rediscovery, and the other will require the user to enter information.\n\n\nAutodiscovery Example\n\u00b6\n\n\nSuppose that a new plugin version adds a new required field to its repository schema. This new field specifies a full path to a database installation. The following listing shows what we'd ideally like the new repository schema to look like (\ninstallationPath\n is the new required property)\n\n\n\"repositoryDefinition\": {\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": { \"type\": \"string\" },\n        \"installationPath\": { \"type\": \"string\", \"format\": \"unixpath\"}\n    },\n    \"required\": [\"name\", \"installationPath\"],\n    \"nameField\": \"name\",\n    \"identityFields\": [\"name\"]\n}\n\n\n\n\n\nThe new plugin's autodiscovery code will know how to find this full path. Therefore, any repositories that are discovered (or rediscovered) after the upgrade will have this path filled in correctly.\n\n\nBut, there may be repositories that were discovered before the upgrade. The data migrations will have to ensure that \nsome\n value is provided for this new field. However, a data migration will not be able to determine what the correct final value is.\n\n\nOne way to handle this is to modify the schema to allow a special value to indicate that the object needs to be rediscovered. In this example, we'll change the schema from the ideal version above, removing the \nunixpath\n constraint on this string:\n\n\n\"installationPath\": { \"type\": \"string\" }\n\n\n\n\n\nNow, our data migration can set this property to some special sentinel value that will never be mistaken for an actual installation path.\n\n\n_REDISCOVERY_TOKEN = \"###_REPOSITORY_NEEDS_REDISCOVERY_###\"\n\n@plugin.upgrade.repository(\"2020.02.04.01\")\ndef repo_path(old_repository):\n    # We need to add in a repository path, but there is no way for us to know\n    # what the correct path is here, so we cannot set this to anything useful.\n    # Instead, we'll set a special sentinel value that will indicate that the\n    # repository is unusable until the remote host is rediscovered.\n    old_repository[\"installationPath\"] = _REDISCOVERY_TOKEN\n    return old_repository\n\n\n\n\n\nNow, wherever the plugin needs to use this path, we'll need to check for this sentinel value, and error out if we find it.  For example, we might need a valid path during the \nconfigure\n operation:\n\n\n@plugin.virtual.configure()\ndef configure(virtual_source, snapshot, repository):\n    if repository.installation_path == _REDISCOVERY_TOKEN:\n        # We cannot use this repository as/is -- it must be rediscovered.\n        msg = 'Unable to use repository \"{}\" because it has not been updated ' \\\n        'since upgrade. Please re-run discovery and try again'\n        raise UserError(msg.format(repository.name))\n\n    # ... actual configure code goes here\n\n\n\n\n\nManual Entry\n\u00b6\n\n\nAbove, we looked at an example where the plugin could handle filling in new values for a new field at discovery time, so the user was simply asked to rediscover.\n\n\nSometimes, though, users themselves will have to be the ones to supply new values.\n\n\nSuppose that a new plugin version wants to add a required field to the \nvirtualSource\n object. This new property will tell which port the database should be accessible on. Ideally, we might want our new field to look like this:\n\n\n\"port\": {\"type\": \"integer\", \"minimum\": 1024, \"maximum\": 65535}\n\n\n\n\n\nAgain, however, the data migration will not know which value is correct here. This is something the user must decide. Still, the data migration must provide \nsome\n value. As before, we'll change the schema a bit from what would be ideal:\n\n\n\"port\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 65535}\n\n\n\n\n\nNow, our data migration can use the value \n0\n as code for \"this VDB needs user intervention\".\n\n\n@plugin.upgrade.virtual_source(\"2020.02.04.02\")\ndef add_dummy_port(old_virtual_source):\n    # Set the \"port\" property to 0 to act as a placeholder.\n    old_virtual_source[\"port\"] = 0\n    return old_virtual_source\n\n\n\n\n\nAs with the previous example, our plugin code will need to look for this special value, and raise an error so that the user knows what to do. This example shows the \nVirtual Source Reconfigure\n operation, but of course, similar code will be needed anywhere else that the new \nport\n property is required.\n\n\n@plugin.virtual.reconfigure()\ndef virtual_reconfigure(virtual_source, repository, source_config, snapshot):\n    if virtual_source.parameters.port == 0:\n        raise UserError('VDB \"{}\" cannot function properly. Please choose a ' \\\n        'port number for this VDB to use.'.format(virtual_source.parameters.name))\n\n    # ... actual reconfigure code goes here",
            "title": "Upgrade"
        },
        {
            "location": "/Versioning_And_Upgrade/Upgrade/#upgrade",
            "text": "Upgrade is the process of moving from an older version of a plugin to a newer version.\nUpgrading is not as simple as just replacing the installed plugin with a newer one.  The main complication comes when the new plugin version makes changes to its  schemas .  Consider the case of a plugin that works with collections of text files -- the user points it to a directory tree containing text files, and the plugin syncs the files from there.  The first release of such a plugin might have no link-related user options. So the plugin's linked source schema might define no properties at all:  \"linkedSourceDefinition\" :   { \n     \"type\" :   \"object\" , \n     \"additionalProperties\"   :   false , \n     \"properties\"   :   { \n     }  }   And, the syncing code is very simple:  @plugin.linked.pre_snapshot ()  def   linked_pre_snapshot ( direct_source ,   repository ,   source_config ): \n     libs . run_sync ( \n         remote_connection   =   direct_source . connection , \n         source_directory   =   source_config . path \n         )   But, later, some users request a new feature -- they want to avoid syncing any backup or hidden files. So, a new plugin version is released. This time, there is a new boolean property in the linked source schema where users can elect to skip these files, if desired.  \"linkedSourceDefinition\" :   { \n     \"type\" :   \"object\" , \n     \"additionalProperties\"   :   false , \n     \"required\" :   [ \"skipHiddenAndBackup\" ], \n     \"properties\"   :   { \n       \"skipHiddenAndBackup\" :   {   \"type\" :   \"boolean\"   } \n     }  }   The plugin code that handles the syncing can now pay attention to this new boolean property:  _HIDDEN_AND_BACKUP_SPECS   =   [ \n     \"*.bak\" , \n     \"*~\" ,    # Backup files from certain editors \n     \".*\"    # Unix-style hidden files  ]  @plugin.linked.pre_snapshot ()  def   linked_pre_snapshot ( direct_source ,   repository ,   source_config ): \n     exclude_spec   =   _HIDDEN_AND_BACKUP_SPECS   if   direct_source . parameters . skip_hidden_and_backup   else   [] \n\n     libs . run_sync ( \n         remote_connection   =   direct_source . connection , \n         source_directory   =   source_config . path , \n         exclude_paths   =   exclude_spec \n         )   Suppose a user has an engine with linked sources created by the older version of this plugin. That is, the existing linked sources have no  skipHiddenAndBackup  property.  If the user installs the new version of the plugin, we have a problem! The above  pre_snapshot  code from the new plugin will attempt to access the  skip_hidden_and_backup  property, which we've just seen will not exist!  The solution to this problem is to use  data migrations , explained below.   Zero dSource and VDB downtime during plugin upgrade  dSources and VDBs do not need to be disabled before a plugin upgrade is initiated. End users can continue to access data from existing VDBs during a plugin upgrade. However, while a particular plugin is in the process of being upgraded, no administrative Delphix Engine operations like: VDB Refresh, VDB Provision, dSource Disable/Enable etc will be allowed on the objects associated with that plugin. Objects associated with other plugins will not be affected.",
            "title": "Upgrade"
        },
        {
            "location": "/Versioning_And_Upgrade/Upgrade/#data-migrations",
            "text": "",
            "title": "Data Migrations"
        },
        {
            "location": "/Versioning_And_Upgrade/Upgrade/#what-is-a-data-migration",
            "text": "Whenever a new version of a plugin is installed on a Delphix Engine, the engine needs to migrate pre-existing data from its old format (as specified by the schemas in the old version of the plugin), to its new format (as specified by the schemas in the new version of the plugin).  A  data migration  is a function that is responsible for doing this conversion. It is provided by the plugin.  Thus, when the new plugin version is installed, the engine will call all applicable data migrations provided by the new plugin. This ensures that all data is always in the format expected by the new plugin.",
            "title": "What is a Data Migration?"
        },
        {
            "location": "/Versioning_And_Upgrade/Upgrade/#a-simple-example",
            "text": "Let's go back to the above example of the plugin that adds a new boolean option to allow users to avoid syncing backup and hidden files. Here is a data migration that the new plugin can provide to handle the data format change:  @plugin.upgrade.linked_source ( \"2019.11.20\" )  def   add_skip_option ( old_linked_source ): \n     return   { \n       \"skipHiddenAndBackup\" :   false \n     }   The exact rules for data migrations are covered in detail  below . Here, we'll just walk through this code line by line and make some observations.  @plugin.upgrade.linked_source ( \"2019.11.20\" )   The above line is a  decorator  that identifies the following function as a data migration. This particular migration will handle linked sources. It is given an ID of  2019.11.20  -- this controls when this migration is run in relation to other data migrations.  def   add_skip_option ( old_linked_source ):   Note that the data migration takes an argument representing the old-format data. In this simple example, we know that there are no properties in the old-format data, so we can just ignore it.       return   { \n       \"skipHiddenAndBackup\" :   false \n     }   Here, we are returning a Python dictionary representing the new format of the data. In this example, the dictionary has only one field:  skipHiddenAndBackup . Because the old version of the plugin had no ability to skip files, we default this property to  false  to match the new schema.",
            "title": "A Simple Example"
        },
        {
            "location": "/Versioning_And_Upgrade/Upgrade/#rules-for-data-migrations",
            "text": "As shown above, the a data migration receives old-format input and produces new-format output. The rules and recommendations for data migrations follow:",
            "title": "Rules for Data Migrations"
        },
        {
            "location": "/Versioning_And_Upgrade/Upgrade/#rules",
            "text": "Input and output are Python dictionaries, with properties named exactly as specified in the schemas. Note that this differs from other plugin operations, where the inputs are defined with autogenerated Python  classes , and whose properties use Python-style naming.    Each data migration must be tagged with an ID string. This string must consist of one or more positive integers separated by periods.    Data migration IDs must be numerically unique. Note that  \"1.2\" ,  \"01.02\" , and \" 1.2.0.0.0\"  are all considered to be identical.    Once released, a data migration must never be deleted. An attempted upgrade will fail if the already-installed plugin version has a data migration that does not appear in the to-be-installed version.    At upgrade time, the engine will find the set of new migrations provided by the new version that are not already part of the already-installed version. Each of these migrations will then be run, in the order specified below.    After running all applicable migrations, the engine will confirm that the resultant data conforms to the new version's schemas. If not, the upgrade will fail.    Note that there is no requirement or guarantee that the input or output of any particular data migration will conform to a schema. We only guarantee that the input to the  first  data migration conforms to the schema of the already-installed plugin version. And, we only require that the output of the  final  data migration conforms to the schema of the new plugin version.    Data migrations are run in the order specified by their IDs. The ordering is numerical, not lexicographical. Thus  \"1\"  would run before  \"2\" , which would run before  \"10\" .    Data migrations have no access to  Platform Libraries  or remote hosts. For example: If a data migration attempts to use  run_bash  the upgrade will fail.    Note that the above rules imply that at least one data migration is required any time a schema change is made that would invalidate any data produced using a previous version of the plugin. For example: adding a  \"required\"  property to the new schema.",
            "title": "Rules"
        },
        {
            "location": "/Versioning_And_Upgrade/Upgrade/#recommendations",
            "text": "We recommend using a \"Year.Month.Date\" format like  \"2019.11.04\"  for migration IDs. You can use trailing integers as necessary (e.g. use  \"2019.11.04.5\"  if you need something to be run between  \"2019.11.04\"  and  \"2019.11.05\" ).    Even though they follow similar naming rules, migration IDs are not the same thing as plugin versions. We do not recommend using your plugin version in your migration IDs.    We recommend using small, single-purpose data migrations. That is, if you end up making four schema changes over the course of developing a new plugin version, we recommend writing four different data migrations, one for each change.",
            "title": "Recommendations"
        },
        {
            "location": "/Versioning_And_Upgrade/Upgrade/#data-migration-example",
            "text": "Here is a very simple data migration.  @plugin.upgrade.repository ( \"2019.12.15\" )  def   add_new_flag_to_repo ( old_repository ): \n   new_repository   =   dict ( old_repository ) \n   new_repository [ \"useNewFeature\" ]   =   False \n   return   new_repository",
            "title": "Data Migration Example"
        },
        {
            "location": "/Versioning_And_Upgrade/Upgrade/#debugging-data-migration-problems",
            "text": "During the process of upgrading to a new version, the Delphix Engine will run all applicable data migrations, and then ensure that the resulting object matches the new schema. But, what if there is a bug, and the resulting object does  not  match the schema?",
            "title": "Debugging Data Migration Problems"
        },
        {
            "location": "/Versioning_And_Upgrade/Upgrade/#security-concerns-prevent-detailed-error-messages",
            "text": "One problem here is that the Delphix Engine is limited in the information that it can provide in the error message. Ideally, the engine would say exactly what was wrong with the object (e.g.: \"The field  port  has the value  15 , but the schema says it has to have a value between  256  and  1024 \").  But, the Delphix Engine cannot do this for security reasons. Ordinarily, the Delphix Engine knows which fields contain sensitive information, and can redact such fields from error messages. But, the only reason the Delphix Engine has that knowledge is because the schema provides that information. If an object does not  conform to the schema, then the Delphix Engine can't know what is sensitive and what isn't.  Therefore, the error message here might lack the detail necessary to debug the problem.",
            "title": "Security Concerns Prevent Detailed Error Messages"
        },
        {
            "location": "/Versioning_And_Upgrade/Upgrade/#one-solution-temporary-logging",
            "text": "During development of a new plugin version, you may find yourself trying to find and fix such a bug.\nOne technique is to use temporary logging.  For example, while you are trying to locate and fix the bug, you could put a log statement at the very end of each of your data migrations, like so:    logger.debug(\"Migration 2010.03.01 returning {}\".format(new_object))\n  return new_object  See the  Logging  section for more information about logging works.  From the logs, you'll be able to see exactly what each migration is returning. From there, hopefully the problem will become apparent. As a supplemental tool, consider pasting these results (along with your schema) into an online JSON validator for more information.   Warning  It is  very important  that you only use logging as a temporary debugging strategy.  Such logging must be removed before you release the plugin to end users . If this logging ends up in your end product, it could cause a serious security concern. Please see our  sensitive data best practices  for more information.",
            "title": "One Solution: Temporary Logging"
        },
        {
            "location": "/Versioning_And_Upgrade/Upgrade/#when-data-migrations-are-insufficient",
            "text": "New versions of plugins often require some modification of data that was written using an older version of the same plugin. Data migrations handle this modification. Unfortunately, data migrations cannot always fully handle all possible upgrade scenarios by themselves.  For example, a new plugin version might want to add a new required field to one of its schemas. But, the correct value for this new field might not be knowable while the upgrade is underway -- perhaps it must be entered by the user, or perhaps it would require automatic discovery to be rerun.  Such a situation will require some user intervention after the upgrade.  In all cases, of course you will want to  clearly document  to your users that there will extra work required so they can make sure they known what they are getting into before they decide to upgrade.   Tip  It should also be said that you should try to avoid cases like this.  As much as possible, try to make your post-upgrade plugin function with no user intervention. Only resort to user intervention as a last resort.   The recommended strategy here is to arrange for the affected objects to be in an \"invalid\" state, and for your plugin code to detect this state, and throw errors when the objects are used.  For such a situation, we recommend the following process:   Make your schema changes so that the affected property can be set in such a way that plugin code can identify it as being invalid. Typically this is done by allowing for some \"sentinel\" value. This may require you to have a less-strict schema definition than you might otherwise want.  In your data migrations, make sure the affected properties are indeed marked invalid.  In any plugin code that needs to use these properties, first check them for validity. If they are invalid, then raise an error that explains the situation to the user, and tells them what steps they need to take.   Following are two examples of schema changes that need extra user intervention after upgrade. One will require a rediscovery, and the other will require the user to enter information.",
            "title": "When Data Migrations Are Insufficient"
        },
        {
            "location": "/Versioning_And_Upgrade/Upgrade/#autodiscovery-example",
            "text": "Suppose that a new plugin version adds a new required field to its repository schema. This new field specifies a full path to a database installation. The following listing shows what we'd ideally like the new repository schema to look like ( installationPath  is the new required property)  \"repositoryDefinition\": {\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": { \"type\": \"string\" },\n        \"installationPath\": { \"type\": \"string\", \"format\": \"unixpath\"}\n    },\n    \"required\": [\"name\", \"installationPath\"],\n    \"nameField\": \"name\",\n    \"identityFields\": [\"name\"]\n}  The new plugin's autodiscovery code will know how to find this full path. Therefore, any repositories that are discovered (or rediscovered) after the upgrade will have this path filled in correctly.  But, there may be repositories that were discovered before the upgrade. The data migrations will have to ensure that  some  value is provided for this new field. However, a data migration will not be able to determine what the correct final value is.  One way to handle this is to modify the schema to allow a special value to indicate that the object needs to be rediscovered. In this example, we'll change the schema from the ideal version above, removing the  unixpath  constraint on this string:  \"installationPath\": { \"type\": \"string\" }  Now, our data migration can set this property to some special sentinel value that will never be mistaken for an actual installation path.  _REDISCOVERY_TOKEN = \"###_REPOSITORY_NEEDS_REDISCOVERY_###\"\n\n@plugin.upgrade.repository(\"2020.02.04.01\")\ndef repo_path(old_repository):\n    # We need to add in a repository path, but there is no way for us to know\n    # what the correct path is here, so we cannot set this to anything useful.\n    # Instead, we'll set a special sentinel value that will indicate that the\n    # repository is unusable until the remote host is rediscovered.\n    old_repository[\"installationPath\"] = _REDISCOVERY_TOKEN\n    return old_repository  Now, wherever the plugin needs to use this path, we'll need to check for this sentinel value, and error out if we find it.  For example, we might need a valid path during the  configure  operation:  @plugin.virtual.configure()\ndef configure(virtual_source, snapshot, repository):\n    if repository.installation_path == _REDISCOVERY_TOKEN:\n        # We cannot use this repository as/is -- it must be rediscovered.\n        msg = 'Unable to use repository \"{}\" because it has not been updated ' \\\n        'since upgrade. Please re-run discovery and try again'\n        raise UserError(msg.format(repository.name))\n\n    # ... actual configure code goes here",
            "title": "Autodiscovery Example"
        },
        {
            "location": "/Versioning_And_Upgrade/Upgrade/#manual-entry",
            "text": "Above, we looked at an example where the plugin could handle filling in new values for a new field at discovery time, so the user was simply asked to rediscover.  Sometimes, though, users themselves will have to be the ones to supply new values.  Suppose that a new plugin version wants to add a required field to the  virtualSource  object. This new property will tell which port the database should be accessible on. Ideally, we might want our new field to look like this:  \"port\": {\"type\": \"integer\", \"minimum\": 1024, \"maximum\": 65535}  Again, however, the data migration will not know which value is correct here. This is something the user must decide. Still, the data migration must provide  some  value. As before, we'll change the schema a bit from what would be ideal:  \"port\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 65535}  Now, our data migration can use the value  0  as code for \"this VDB needs user intervention\".  @plugin.upgrade.virtual_source(\"2020.02.04.02\")\ndef add_dummy_port(old_virtual_source):\n    # Set the \"port\" property to 0 to act as a placeholder.\n    old_virtual_source[\"port\"] = 0\n    return old_virtual_source  As with the previous example, our plugin code will need to look for this special value, and raise an error so that the user knows what to do. This example shows the  Virtual Source Reconfigure  operation, but of course, similar code will be needed anywhere else that the new  port  property is required.  @plugin.virtual.reconfigure()\ndef virtual_reconfigure(virtual_source, repository, source_config, snapshot):\n    if virtual_source.parameters.port == 0:\n        raise UserError('VDB \"{}\" cannot function properly. Please choose a ' \\\n        'port number for this VDB to use.'.format(virtual_source.parameters.name))\n\n    # ... actual reconfigure code goes here",
            "title": "Manual Entry"
        },
        {
            "location": "/Versioning_And_Upgrade/Compatibility/",
            "text": "Compatibility\n\u00b6\n\n\nBefore we allow a newly-uploaded plugin to replace an already-installed plugin, we have to make sure that it will not cause any problems.\n\n\nFor example:\n\n\n\n\nThe newly-uploaded plugin must be able to accept any existing data that has been written using the already-installed plugin.\n\n\nThe user should not unexpectedly lose any features or bug fixes that are present in the already-installed plugin.\n\n\n\n\nThese restrictions are enforced by the Delphix Engine, and sometimes, the plugin itself.\n\n\nDelphix Engine Rules\n\u00b6\n\n\nThe Delphix Engine will enforce these rules before a newly-uploded plugin is allowed to be installed:\n\n\n\n\nThe \nbuild number\n may only move forward, not backwards.\n\n\nAll \ndata migration IDs\n that are present in the already-installed plugin must also be present on the newly-uploaded plugin. The newly-uploaded plugin may add more data migrations, of course.",
            "title": "Compatibility"
        },
        {
            "location": "/Versioning_And_Upgrade/Compatibility/#compatibility",
            "text": "Before we allow a newly-uploaded plugin to replace an already-installed plugin, we have to make sure that it will not cause any problems.  For example:   The newly-uploaded plugin must be able to accept any existing data that has been written using the already-installed plugin.  The user should not unexpectedly lose any features or bug fixes that are present in the already-installed plugin.   These restrictions are enforced by the Delphix Engine, and sometimes, the plugin itself.",
            "title": "Compatibility"
        },
        {
            "location": "/Versioning_And_Upgrade/Compatibility/#delphix-engine-rules",
            "text": "The Delphix Engine will enforce these rules before a newly-uploded plugin is allowed to be installed:   The  build number  may only move forward, not backwards.  All  data migration IDs  that are present in the already-installed plugin must also be present on the newly-uploaded plugin. The newly-uploaded plugin may add more data migrations, of course.",
            "title": "Delphix Engine Rules"
        },
        {
            "location": "/Versioning_And_Upgrade/Backports_And_Hotfixes/",
            "text": "Backports and Hotfixes\n\u00b6\n\n\nIf your plugin uses an \n\"enterprise-style\"\n release strategy, then you'll probably want to occasionally provide new \"minor\" or \"patch\" versions that build atop older versions.\n\n\nCode changes that are applied atop old releases are usually called \"backports\". Sometimes, they are also called \"hotfixes\", if the change is specifically created for a single user.\n\n\nThese releases present a problem: although they are built atop an older code branch, they are still newer than some releases from a newer code branch. Below, we'll walk through how we prevent users from \"upgrading\" to a new-branch release that would be incompatible with an installed old-branch release.\n\n\nMotivating Example\n\u00b6\n\n\nLet's take a look at an example of a possible timeline of releases.\n\n\n\n\nFebruary\n: The initial version of a plugin is released, with build number \"1.0\". This is a simple plugin that uses a simple strategy for syncing dSources.\n\n\nApril\n: A new version is released, with build number \"1.1\". This adds some bugfixes and adds some small optimizations to improve the performance of syncing.\n\n\nAugust\n: A new version is released, with build number \"2.0\". This uses a completely new syncing strategy that is far more sophisticated and efficient.\n\n\n\n\nLet's assume that not all users will want to upgrade to the 2.0 release immediately. So, even months later, you expect to have a significant number of users still on version 1.0 or 1.1.\n\n\nLater, in October, a bug is found which impacts all releases. This bug is important enough that you want to fix it for \nall\n of your end users (not just the ones using 2.0).\n\n\nHere are the behaviors we need:\n\n\n\n\nOur 2.0 end users should be able to get the new bugfix without giving up any of the major new features that were part of 2.0.\n\n\nOur 1.0 and 1.1 end users should be able to get the new bugfix without also needing to accept all the major new features that were part of 2.0.\n\n\nOnce an end user has received the bugfix, it should be impossible to lose the bugfix in an upgrade.\n\n\n\n\nStrategy\n\u00b6\n\n\nYou can include a \ndata migration\n along with your bugfix. If your bugfix involves a schema change, you will have to do this anyways. If not, you can still include a data migration that simply does nothing. If a user with the bugfix attempts to \"upgrade\" to 2.0, the Delphix Engine will prevent it, because the 2.0 releases does not include this migration.\n\n\nYou would typically follow these steps:\n\n\n\n\nFix the bug by applying a code change atop the 2.0 code.\n\n\nInclude the new data migration in your 2.1 release.\n\n\nSeparately, apply the same bugfix atop the 1.1 code. Note: depending on how code changed between 1.1 and 2.0, this 1.1-based bugfix might not contain the exact same code as we used with 2.0.\n\n\nMake another new release of the plugin, this time with build number \"1.2\". This release includes the 1.1-based bugfix. It also should include the new data migration.\n\n\n\n\nThis meets our requirements:\n\n\n\n\nOur 2.0 end users can install version 2.1. This gives them the bugfix, and keeps all the features from 2.0.\n\n\nOur 1.0 and 1.1 end users can install version 1.2. This gives them the bugfix without any of the 2.0 features.\n\n\nIt is impossible for a 2.1 end user to lose the bugfix, because the Delphix Engine will not allow the build number to go \"backwards\". So, a 2.1 end user will not be able to install versions 2.0, 1.1, or 1.0.\n\n\nIt is also impossible for a 1.2 end user to lose the bugfix.\n\n\nThey cannot install 1.0 or 1.1 because the build number is not allowed to decrease.\n\n\nThey also cannot install 2.0. The missing data migration on 2.0 will prevent this.\n\n\n\n\n\n\n\n\nNote that a 1.2 end user can still upgrade to 2.1 at any time. This will allow them to keep the bugfix, and also take advantage of the new features that were part of 2.0.",
            "title": "Backports and Hotfixes"
        },
        {
            "location": "/Versioning_And_Upgrade/Backports_And_Hotfixes/#backports-and-hotfixes",
            "text": "If your plugin uses an  \"enterprise-style\"  release strategy, then you'll probably want to occasionally provide new \"minor\" or \"patch\" versions that build atop older versions.  Code changes that are applied atop old releases are usually called \"backports\". Sometimes, they are also called \"hotfixes\", if the change is specifically created for a single user.  These releases present a problem: although they are built atop an older code branch, they are still newer than some releases from a newer code branch. Below, we'll walk through how we prevent users from \"upgrading\" to a new-branch release that would be incompatible with an installed old-branch release.",
            "title": "Backports and Hotfixes"
        },
        {
            "location": "/Versioning_And_Upgrade/Backports_And_Hotfixes/#motivating-example",
            "text": "Let's take a look at an example of a possible timeline of releases.   February : The initial version of a plugin is released, with build number \"1.0\". This is a simple plugin that uses a simple strategy for syncing dSources.  April : A new version is released, with build number \"1.1\". This adds some bugfixes and adds some small optimizations to improve the performance of syncing.  August : A new version is released, with build number \"2.0\". This uses a completely new syncing strategy that is far more sophisticated and efficient.   Let's assume that not all users will want to upgrade to the 2.0 release immediately. So, even months later, you expect to have a significant number of users still on version 1.0 or 1.1.  Later, in October, a bug is found which impacts all releases. This bug is important enough that you want to fix it for  all  of your end users (not just the ones using 2.0).  Here are the behaviors we need:   Our 2.0 end users should be able to get the new bugfix without giving up any of the major new features that were part of 2.0.  Our 1.0 and 1.1 end users should be able to get the new bugfix without also needing to accept all the major new features that were part of 2.0.  Once an end user has received the bugfix, it should be impossible to lose the bugfix in an upgrade.",
            "title": "Motivating Example"
        },
        {
            "location": "/Versioning_And_Upgrade/Backports_And_Hotfixes/#strategy",
            "text": "You can include a  data migration  along with your bugfix. If your bugfix involves a schema change, you will have to do this anyways. If not, you can still include a data migration that simply does nothing. If a user with the bugfix attempts to \"upgrade\" to 2.0, the Delphix Engine will prevent it, because the 2.0 releases does not include this migration.  You would typically follow these steps:   Fix the bug by applying a code change atop the 2.0 code.  Include the new data migration in your 2.1 release.  Separately, apply the same bugfix atop the 1.1 code. Note: depending on how code changed between 1.1 and 2.0, this 1.1-based bugfix might not contain the exact same code as we used with 2.0.  Make another new release of the plugin, this time with build number \"1.2\". This release includes the 1.1-based bugfix. It also should include the new data migration.   This meets our requirements:   Our 2.0 end users can install version 2.1. This gives them the bugfix, and keeps all the features from 2.0.  Our 1.0 and 1.1 end users can install version 1.2. This gives them the bugfix without any of the 2.0 features.  It is impossible for a 2.1 end user to lose the bugfix, because the Delphix Engine will not allow the build number to go \"backwards\". So, a 2.1 end user will not be able to install versions 2.0, 1.1, or 1.0.  It is also impossible for a 1.2 end user to lose the bugfix.  They cannot install 1.0 or 1.1 because the build number is not allowed to decrease.  They also cannot install 2.0. The missing data migration on 2.0 will prevent this.     Note that a 1.2 end user can still upgrade to 2.1 at any time. This will allow them to keep the bugfix, and also take advantage of the new features that were part of 2.0.",
            "title": "Strategy"
        },
        {
            "location": "/Versioning_And_Upgrade/Replication/",
            "text": "Replication\n\u00b6\n\n\nA Delphix Engine (source) can be setup to replicate data objects to another Delphix Engine (target). Plugins built using the Virtualization SDK work seamlessly with Delphix Engine replication with no additional development required from plugin developers.\n\n\nOnly a single version of a plugin can be active on a Delphix Engine at a time. We discuss some basic scenarios below. For more detailed information refer to the \nDelphix Engine Documentation\n.\n\n\nReplica Provisioning\n\u00b6\n\n\nReplicated dSource or VDB snapshots can be used to provision new VDBs onto a target Delphix Engine, without failing over any of the objects. When provisioning a VDB from a replicated snapshot:\n\n\n\n\nA version of the plugin has to be installed on the target Delphix Engine.\n\n\nThe versions of the plugins installed on the source and target Delphix Engines have to be \ncompatible\n.\n\n\n\n\nOnce provisioned, the VDB on the target Delphix Engine will be associated with the version of the plugin installed on the target Delphix Engine, any required data migrations will be run as part of the provisioning process. For more details refer to the \nDelphix Engine Documentation\n.\n\n\nReplication Failover\n\u00b6\n\n\nOn failover, there are three scenarios for each plugin:\n\n\n\n\n\n\n\n\nScenario\n\n\nOutcome\n\n\n\n\n\n\n\n\n\n\nSource plugin \nnot installed\n on target Delphix Engine\n\n\nThe plugin will be failed over and marked as \nactive\n on the target Delphix Engine.\n\n\n\n\n\n\nSource plugin version \nis equal to\n the target plugin version\n\n\nThe plugin from the source will be merged with the plugin on the target Delphix Engine.\n\n\n\n\n\n\nSource plugin version \nis not equal to\n the target plugin version\n\n\nThe plugin from the source will be marked \ninactive\n on the target Delphix Engine. An \ninactive\n plugin can be subsequently activated, after failover, if it is \ncompatible\n with the existing \nactive\n plugin. Activating a plugin will do an upgrade and merge the \ninactive\n plugin, and all its associated objects, with the \nactive\n plugin. For more details refer to the \nDelphix Engine Documentation\n.",
            "title": "Replication"
        },
        {
            "location": "/Versioning_And_Upgrade/Replication/#replication",
            "text": "A Delphix Engine (source) can be setup to replicate data objects to another Delphix Engine (target). Plugins built using the Virtualization SDK work seamlessly with Delphix Engine replication with no additional development required from plugin developers.  Only a single version of a plugin can be active on a Delphix Engine at a time. We discuss some basic scenarios below. For more detailed information refer to the  Delphix Engine Documentation .",
            "title": "Replication"
        },
        {
            "location": "/Versioning_And_Upgrade/Replication/#replica-provisioning",
            "text": "Replicated dSource or VDB snapshots can be used to provision new VDBs onto a target Delphix Engine, without failing over any of the objects. When provisioning a VDB from a replicated snapshot:   A version of the plugin has to be installed on the target Delphix Engine.  The versions of the plugins installed on the source and target Delphix Engines have to be  compatible .   Once provisioned, the VDB on the target Delphix Engine will be associated with the version of the plugin installed on the target Delphix Engine, any required data migrations will be run as part of the provisioning process. For more details refer to the  Delphix Engine Documentation .",
            "title": "Replica Provisioning"
        },
        {
            "location": "/Versioning_And_Upgrade/Replication/#replication-failover",
            "text": "On failover, there are three scenarios for each plugin:     Scenario  Outcome      Source plugin  not installed  on target Delphix Engine  The plugin will be failed over and marked as  active  on the target Delphix Engine.    Source plugin version  is equal to  the target plugin version  The plugin from the source will be merged with the plugin on the target Delphix Engine.    Source plugin version  is not equal to  the target plugin version  The plugin from the source will be marked  inactive  on the target Delphix Engine. An  inactive  plugin can be subsequently activated, after failover, if it is  compatible  with the existing  active  plugin. Activating a plugin will do an upgrade and merge the  inactive  plugin, and all its associated objects, with the  active  plugin. For more details refer to the  Delphix Engine Documentation .",
            "title": "Replication Failover"
        },
        {
            "location": "/References/CLI/",
            "text": "CLI\n\u00b6\n\n\nThe CLI is installed with the SDK. To install the SDK, refer to the \nGetting Started\n section. You can also use a \nCLI Configuration File\n to set default values for CLI command options.\n\n\nHelp\n\u00b6\n\n\nEvery command has a \n-h\n flag including the CLI itself. This will print the help menu.\n\n\nExamples\n\u00b6\n\n\nGet the CLI's help menu.\n\n\n$ dvp -h\nUsage: dvp [OPTIONS] COMMAND [ARGS]...\n\n  The tools of the Delphix Virtualization SDK that help develop, build, and\n  upload a plugin.\n\nOptions:\n  --version      Show the version and exit.\n  -v, --verbose  Enable verbose mode. Can be repeated up to three times for\n                 increased verbosity.\n\n  -q, --quiet    Enable quiet mode. Can be repeated up to three times for\n                 increased suppression.\n\n  -h, --help     Show this message and exit.\n\nCommands:\n  build          Build the plugin code and generate upload artifact file...\n  download-logs  Download plugin logs from a target Delphix Engine to a...\n  init           Create a plugin in the root directory.\n  upload         Upload the generated upload artifact (the plugin JSON\n                 file)...\n\n\n\n\n\nGet the \nbuild\n command's help menu.\n\n\n$ dvp build -h\nUsage: dvp build [OPTIONS]\n\n  Build the plugin code and generate upload artifact file using the\n  configuration provided in the plugin config file.\n\nOptions:\n  -c, --plugin-config FILE    Set the path to plugin config file.This file\n                              contains the configuration required to build the\n                              plugin.  [default: plugin_config.yml]\n  -a, --upload-artifact FILE  Set the upload artifact.The upload artifact file\n                              generated by build process will be writtento\n                              this file and later used by upload command.\n                              [default: artifact.json]\n  -g, --generate-only         Only generate the Python classes from the schema\n                              definitions. Do not do a full build or create an\n                              upload artifact.  [default: False]\n  -h, --help                  Show this message and exit.\n\n\n\n\n\nVerbosity\n\u00b6\n\n\nTo change the verbosity level of the CLI you can specify up to three \n-v\n (to increase) or \n-q\n (to decrease) the amount that is printed to the console. This is an option on the CLI itself and can be used with any command.\n\n\n\n\n\n\n\n\nOption\n\n\nOutput\n\n\n\n\n\n\n\n\n\n\n-qqq\n\n\nNone\n\n\n\n\n\n\n-qq\n\n\nCritical\n\n\n\n\n\n\n-q\n\n\nError\n\n\n\n\n\n\n-v\n\n\nInfo\n\n\n\n\n\n\n-vv\n\n\nDebug\n\n\n\n\n\n\n-vvv\n\n\nAll\n\n\n\n\n\n\n\n\nExamples\n\u00b6\n\n\nPrint everything to the console.\n\n\n$ dvp -vvv build\n\n\n\n\n\nPrint nothing to the console.\n\n\n$ dvp -qqq build\n\n\n\n\n\nCommands\n\u00b6\n\n\ninit\n\u00b6\n\n\nDescription\n\u00b6\n\n\nCreate a plugin in the root directory. The plugin will be valid but have no functionality.\n\n\nOptions\n\u00b6\n\n\n\n\n\n\n\n\nOption\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\nDescription\n\n\nRequired\n\n\nDefault\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\n\n-r,\n--root-dir\nDIRECTORY\n\n\nSet the plugin root directory.\n\n\nN\n\n\nos.cwd()\n\n\n\n\n\n\n-n,\n--plugin-name\nTEXT\n\n\nSet the name of the plugin that will be used to identify it.\n\n\nN\n\n\nid\n\n\n\n\n\n\n-s,\n--ingestion-strategy\n[DIRECT|STAGED]\n\n\nSet the ingestion strategy of the plugin. A \"direct\" plugin ingests without a staging server while a \"staged\" plugin requires a staging server.\n\n\nN\n\n\nDIRECT\n\n\n\n\n\n\n-t,\n--host-type\n[UNIX|WINDOWS]\n\n\nSet the host platform supported by the plugin.\n\n\nN\n\n\nUNIX\n\n\n\n\n\n\n\n\nExamples\n\u00b6\n\n\nCreate a \nUNIX\n plugin in the current working directory with the \nDIRECT\n ingestion strategy. Here the name of the plugin will be equal to the id that is generated.\n\n\n$ dvp init\n\n\n\n\n\nCreate a \nUNIX\n plugin in the current working directory with the \nDIRECT\n ingestion strategy and use \npostgres\n as the display name.\n\n\n$ dvp init -n postgres\n\n\n\n\n\nCreate a \nUNIX\n plugin called \nmongodb\n in a custom location with the \nSTAGED\n ingestion strategy.\n\n\n$ dvp init -n mongodb -s STAGED -r /our/plugin/directory\n\n\n\n\n\nCreate a \nWINDOWS\n plugin called \nmssql\n in the current working directory with the \nDIRECT\n ingestion strategy.\n\n\n$ dvp init -n mssql -t WINDOWS\n\n\n\n\n\n\n\nbuild\n\u00b6\n\n\nDescription\n\u00b6\n\n\nBuild the plugin code and generate upload artifact file using the configuration provided in the plugin config file.\n\n\nOptions\n\u00b6\n\n\n\n\n\n\n\n\nOption \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\nDescription\n\n\nRequired\n\n\nDefault\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\n\n-c,\n--plugin-config\nFILE\n\n\nSet the path to plugin config file.This file contains the configuration required to build the plugin.\n\n\nN\n\n\nplugin_config.yml\n\n\n\n\n\n\n-a,\n--upload-artifact\nFILE\n\n\nSet the upload artifact.The upload artifact file generated by build process will be written to this file and later used by upload command.\n\n\nN\n\n\nartifact.json\n\n\n\n\n\n\n-g,\n--generate-only\n\n\nOnly generate the Python classes from the schema definitions. Do not do a full build or create an upload artifact.\n\n\nN\n\n\nFalse\n\n\n\n\n\n\n\n\nExamples\n\u00b6\n\n\nDo a full build of the plugin and write the upload artifact to \n./artifact.json\n.\n\n\nThis assumes current working directory contains a plugin config file named \nplugin_config.yml\n.\n\n\n$ dvp build\n\n\n\n\n\nDo a partial build and just generate the Python classes from the schema definitions.\n\n\nThis assumes current working directory contains ad plugin config file named \nplugin_config.yml\n.\n\n\n$ dvp build -g\n\n\n\n\n\nDo a full build of a plugin and write the artifact file to a custom location.\n\n\n$ dvp build -c config.yml -a build/artifact.json\n\n\n\n\n\n\n\nupload\n\u00b6\n\n\nDescription\n\u00b6\n\n\nUpload the generated upload artifact (the plugin JSON file) that was built to a target Delphix Engine. Note that the upload artifact should be the file created after running the build command and will fail if it's not readable or valid.\n\n\nOptions\n\u00b6\n\n\n\n\n\n\n\n\nOption \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\nDescription\n\n\nRequired\n\n\nDefault \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\n\n-e,\n--delphix-engine\nTEXT\n\n\nUpload plugin to the provided engine. This should be either the hostname or IP address.\n\n\nY\n\n\nNone\n\n\n\n\n\n\n-u,\n--user\nTEXT\n\n\nAuthenticate to the Delphix Engine with the provided user.\n\n\nY\n\n\nNone\n\n\n\n\n\n\n-a,\n--upload-artifact FILE\n\n\nPath to the upload artifact that was generated through build.\n\n\nN\n\n\nartifact.json\n\n\n\n\n\n\n--wait\n\n\nBlock and wait for the upload job to finish on the Delphix Engine.\n\n\nN\n\n\nNone\n\n\n\n\n\n\n--password\nTEXT\n\n\nAuthenticate using the provided password. If ommitted, the password will be requested through a secure prompt.\n\n\nN\n\n\nNone\n\n\n\n\n\n\n\n\nExamples\n\u00b6\n\n\nUpload artifact \nbuild/artifact.json\n to \nengine.example.com\n using the user \nadmin\n. Since the password option is ommitted, a secure password prompt is used instead.\n\n\n$ dvp upload -a build/artifact -e engine.example.com -u admin\nPassword:\n\n\n\n\n\n\n\ndownload-logs\n\u00b6\n\n\nDescription\n\u00b6\n\n\nDownload plugin logs from a Delphix Engine to a local directory.\n\n\nOptions\n\u00b6\n\n\n\n\n\n\n\n\nOption \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\nDescription\n\n\nRequired\n\n\nDefault\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\n\n\n\n\n\n\n\n\n\n-e,\n--delphix-engine\nTEXT\n\n\nDownload plugin logs from the provided Delphix engine. This should be either the hostname or IP address.\n\n\nY\n\n\nNone\n\n\n\n\n\n\n-c,\n--plugin-config FILE\n\n\nSet the path to plugin config file. This file contains the plugin name to download logs for.\n\n\nN\n\n\nplugin_config.yml\n\n\n\n\n\n\n-u,\n--user\nTEXT\n\n\nAuthenticate to the Delphix Engine with the provided user.\n\n\nY\n\n\nNone\n\n\n\n\n\n\n-d,\n--directory DIRECTORY\n\n\nSpecify the directory of where to download the plugin logs.\n\n\nN\n\n\nos.cwd()\n\n\n\n\n\n\n--password\nTEXT\n\n\nAuthenticate using the provided password. If ommitted, the password will be requested through a secure prompt.\n\n\nN\n\n\nNone\n\n\n\n\n\n\n\n\nExamples\n\u00b6\n\n\nDownload plugin logs from \nengine.example.com\n using the user \nadmin\n. Since the password option is ommitted, a secure password prompt is used instead.\n\n\n$ dvp download-logs -e engine.example.com -u admin\nPassword:",
            "title": "CLI"
        },
        {
            "location": "/References/CLI/#cli",
            "text": "The CLI is installed with the SDK. To install the SDK, refer to the  Getting Started  section. You can also use a  CLI Configuration File  to set default values for CLI command options.",
            "title": "CLI"
        },
        {
            "location": "/References/CLI/#help",
            "text": "Every command has a  -h  flag including the CLI itself. This will print the help menu.",
            "title": "Help"
        },
        {
            "location": "/References/CLI/#examples",
            "text": "Get the CLI's help menu.  $ dvp -h\nUsage: dvp [OPTIONS] COMMAND [ARGS]...\n\n  The tools of the Delphix Virtualization SDK that help develop, build, and\n  upload a plugin.\n\nOptions:\n  --version      Show the version and exit.\n  -v, --verbose  Enable verbose mode. Can be repeated up to three times for\n                 increased verbosity.\n\n  -q, --quiet    Enable quiet mode. Can be repeated up to three times for\n                 increased suppression.\n\n  -h, --help     Show this message and exit.\n\nCommands:\n  build          Build the plugin code and generate upload artifact file...\n  download-logs  Download plugin logs from a target Delphix Engine to a...\n  init           Create a plugin in the root directory.\n  upload         Upload the generated upload artifact (the plugin JSON\n                 file)...  Get the  build  command's help menu.  $ dvp build -h\nUsage: dvp build [OPTIONS]\n\n  Build the plugin code and generate upload artifact file using the\n  configuration provided in the plugin config file.\n\nOptions:\n  -c, --plugin-config FILE    Set the path to plugin config file.This file\n                              contains the configuration required to build the\n                              plugin.  [default: plugin_config.yml]\n  -a, --upload-artifact FILE  Set the upload artifact.The upload artifact file\n                              generated by build process will be writtento\n                              this file and later used by upload command.\n                              [default: artifact.json]\n  -g, --generate-only         Only generate the Python classes from the schema\n                              definitions. Do not do a full build or create an\n                              upload artifact.  [default: False]\n  -h, --help                  Show this message and exit.",
            "title": "Examples"
        },
        {
            "location": "/References/CLI/#verbosity",
            "text": "To change the verbosity level of the CLI you can specify up to three  -v  (to increase) or  -q  (to decrease) the amount that is printed to the console. This is an option on the CLI itself and can be used with any command.     Option  Output      -qqq  None    -qq  Critical    -q  Error    -v  Info    -vv  Debug    -vvv  All",
            "title": "Verbosity"
        },
        {
            "location": "/References/CLI/#examples_1",
            "text": "Print everything to the console.  $ dvp -vvv build  Print nothing to the console.  $ dvp -qqq build",
            "title": "Examples"
        },
        {
            "location": "/References/CLI/#commands",
            "text": "",
            "title": "Commands"
        },
        {
            "location": "/References/CLI/#init",
            "text": "",
            "title": "init"
        },
        {
            "location": "/References/CLI/#description",
            "text": "Create a plugin in the root directory. The plugin will be valid but have no functionality.",
            "title": "Description"
        },
        {
            "location": "/References/CLI/#options",
            "text": "Option\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  Description  Required  Default\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0      -r, --root-dir DIRECTORY  Set the plugin root directory.  N  os.cwd()    -n, --plugin-name TEXT  Set the name of the plugin that will be used to identify it.  N  id    -s, --ingestion-strategy [DIRECT|STAGED]  Set the ingestion strategy of the plugin. A \"direct\" plugin ingests without a staging server while a \"staged\" plugin requires a staging server.  N  DIRECT    -t, --host-type [UNIX|WINDOWS]  Set the host platform supported by the plugin.  N  UNIX",
            "title": "Options"
        },
        {
            "location": "/References/CLI/#examples_2",
            "text": "Create a  UNIX  plugin in the current working directory with the  DIRECT  ingestion strategy. Here the name of the plugin will be equal to the id that is generated.  $ dvp init  Create a  UNIX  plugin in the current working directory with the  DIRECT  ingestion strategy and use  postgres  as the display name.  $ dvp init -n postgres  Create a  UNIX  plugin called  mongodb  in a custom location with the  STAGED  ingestion strategy.  $ dvp init -n mongodb -s STAGED -r /our/plugin/directory  Create a  WINDOWS  plugin called  mssql  in the current working directory with the  DIRECT  ingestion strategy.  $ dvp init -n mssql -t WINDOWS",
            "title": "Examples"
        },
        {
            "location": "/References/CLI/#build",
            "text": "",
            "title": "build"
        },
        {
            "location": "/References/CLI/#description_1",
            "text": "Build the plugin code and generate upload artifact file using the configuration provided in the plugin config file.",
            "title": "Description"
        },
        {
            "location": "/References/CLI/#options_1",
            "text": "Option \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  Description  Required  Default\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0      -c, --plugin-config FILE  Set the path to plugin config file.This file contains the configuration required to build the plugin.  N  plugin_config.yml    -a, --upload-artifact FILE  Set the upload artifact.The upload artifact file generated by build process will be written to this file and later used by upload command.  N  artifact.json    -g, --generate-only  Only generate the Python classes from the schema definitions. Do not do a full build or create an upload artifact.  N  False",
            "title": "Options"
        },
        {
            "location": "/References/CLI/#examples_3",
            "text": "Do a full build of the plugin and write the upload artifact to  ./artifact.json .  This assumes current working directory contains a plugin config file named  plugin_config.yml .  $ dvp build  Do a partial build and just generate the Python classes from the schema definitions.  This assumes current working directory contains ad plugin config file named  plugin_config.yml .  $ dvp build -g  Do a full build of a plugin and write the artifact file to a custom location.  $ dvp build -c config.yml -a build/artifact.json",
            "title": "Examples"
        },
        {
            "location": "/References/CLI/#upload",
            "text": "",
            "title": "upload"
        },
        {
            "location": "/References/CLI/#description_2",
            "text": "Upload the generated upload artifact (the plugin JSON file) that was built to a target Delphix Engine. Note that the upload artifact should be the file created after running the build command and will fail if it's not readable or valid.",
            "title": "Description"
        },
        {
            "location": "/References/CLI/#options_2",
            "text": "Option \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  Description  Required  Default \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0      -e, --delphix-engine TEXT  Upload plugin to the provided engine. This should be either the hostname or IP address.  Y  None    -u, --user TEXT  Authenticate to the Delphix Engine with the provided user.  Y  None    -a, --upload-artifact FILE  Path to the upload artifact that was generated through build.  N  artifact.json    --wait  Block and wait for the upload job to finish on the Delphix Engine.  N  None    --password TEXT  Authenticate using the provided password. If ommitted, the password will be requested through a secure prompt.  N  None",
            "title": "Options"
        },
        {
            "location": "/References/CLI/#examples_4",
            "text": "Upload artifact  build/artifact.json  to  engine.example.com  using the user  admin . Since the password option is ommitted, a secure password prompt is used instead.  $ dvp upload -a build/artifact -e engine.example.com -u admin\nPassword:",
            "title": "Examples"
        },
        {
            "location": "/References/CLI/#download-logs",
            "text": "",
            "title": "download-logs"
        },
        {
            "location": "/References/CLI/#description_3",
            "text": "Download plugin logs from a Delphix Engine to a local directory.",
            "title": "Description"
        },
        {
            "location": "/References/CLI/#options_3",
            "text": "Option \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  Description  Required  Default\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0      -e, --delphix-engine TEXT  Download plugin logs from the provided Delphix engine. This should be either the hostname or IP address.  Y  None    -c, --plugin-config FILE  Set the path to plugin config file. This file contains the plugin name to download logs for.  N  plugin_config.yml    -u, --user TEXT  Authenticate to the Delphix Engine with the provided user.  Y  None    -d, --directory DIRECTORY  Specify the directory of where to download the plugin logs.  N  os.cwd()    --password TEXT  Authenticate using the provided password. If ommitted, the password will be requested through a secure prompt.  N  None",
            "title": "Options"
        },
        {
            "location": "/References/CLI/#examples_5",
            "text": "Download plugin logs from  engine.example.com  using the user  admin . Since the password option is ommitted, a secure password prompt is used instead.  $ dvp download-logs -e engine.example.com -u admin\nPassword:",
            "title": "Examples"
        },
        {
            "location": "/References/Plugin_Config/",
            "text": "Plugin Config\n\u00b6\n\n\nThe plugin config is a \nYAML\n file that marks the root of a plugin and defines metadata about the plugin and its structure. The config file is read at build time to generate the upload artifact.\n\n\nThe name of the file can be specified during the build. By default, the build looks for \nplugin_config.yml\n in the current working directory.\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField Name\n\n\nRequired\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nid\n\n\nY\n\n\nstring\n\n\nThe unique id of the plugin in a valid UUID format.\n\n\n\n\n\n\nname\n\n\nN\n\n\nstring\n\n\nThe display name of the plugin. This will be used in the UI. If it is not specified name will be equal to id.\n\n\n\n\n\n\nexternalVersion\n\n\nN\n\n\nstring\n\n\nThe plugin's \nexternal version\n. This is a freeform string. If it is not supplied, the build number is used as an external version.\n\n\n\n\n\n\nbuildNumber\n\n\nY\n\n\nstring\n\n\nThe plugin's \nbuild number\n. This string must conform to the format described \nhere\n.\n\n\n\n\n\n\nhostTypes\n\n\nY\n\n\nlist\n\n\nThe host type that the plugin supports. Either \nUNIX\n or \nWINDOWS\n.\n\n\n\n\n\n\nschemaFile\n\n\nY\n\n\nstring\n\n\nThe path to the JSON file that contains the \nplugin's schema definitions\n.\nThis path can be absolute or relative to the directory containing the plugin config file.\n\n\n\n\n\n\nsrcDir\n\n\nY\n\n\nstring\n\n\nThe path to the directory that contains the source code for the plugin. During execution of a plugin operation, this directory will be the current working directory of the Python interpreter. Any modules or resources defined outside of this directory will be inaccessible at runtime.\nThis path can be absolute or relative to the directory containing the plugin config file.\n\n\n\n\n\n\nentryPoint\n\n\nY\n\n\nstring\n\n\nA fully qualified Python symbol that points to the \ndlpx.virtualization.platform.Plugin\n object that defines the plugin.\nIt must be in the form \nimportable.module:object_name\n where \nimportable.module\n is in \nsrcDir\n.\n\n\n\n\n\n\nmanualDiscovery\n\n\nN\n\n\nboolean\n\n\nTrue if the plugin supports manual discovery of source config objects. The default value is \ntrue\n.\n\n\n\n\n\n\npluginType\n\n\nY\n\n\nenum\n\n\nThe ingestion strategy of the plugin. Can be either \nSTAGED\n or \nDIRECT\n.\n\n\n\n\n\n\nlanguage\n\n\nY\n\n\nenum\n\n\nMust be \nPYTHON27\n.\n\n\n\n\n\n\ndefaultLocale\n\n\nN\n\n\nenum\n\n\nThe locale to be used by the plugin if the Delphix user does not specify one. Plugin messages will be displayed in this locale by default. The default value is \nen-us\n.\n\n\n\n\n\n\nrootSquashEnabled\n\n\nN\n\n\nboolean\n\n\nThis dictates whether \"root squash\" is enabled on NFS mounts for the plugin (i.e. whether the \nroot\n user on remote hosts has access to the NFS mounts). Setting this to \nfalse\n allows processes usually run as \nroot\n, like Docker daemons, access to the NFS mounts. The default value is \ntrue\n. This field only applies to Unix hosts.\n\n\n\n\n\n\n\n\nExample\n\u00b6\n\n\nAssume the following basic plugin structure:\n\n\n\u251c\u2500\u2500 plugin_config.yml\n\u251c\u2500\u2500 schema.json\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 mongo_runner.py\n\n\n\n\n\nmongo_runner.py\n contains:\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\n\nmongodb\n \n=\n \nPlugin\n()\n\n\n\n\n\n\nThis is a valid plugin config for the plugin:\n\n\nid\n:\n \n7cf830f2-82f3-4d5d-a63c-7bbe50c22b32\n\n\nname\n:\n \nMongoDB\n\n\nversion\n:\n \n2.0.0\n\n\nhostTypes\n:\n\n  \n-\n \nUNIX\n\n\nentryPoint\n:\n \nmongo_runner:mongodb\n\n\nsrcDir\n:\n \nsrc/\n\n\nschemaFile\n:\n \nschema.json\n\n\npluginType\n:\n \nDIRECT\n\n\nlanguage\n:\n \nPYTHON27\n\n\nbuildNumber\n:\n \n0.1.0\n\n\n\n\n\n\nThis is a valid plugin config for the plugin with \nmanualDiscovery\n set to \nfalse\n and an \nexternalVersion\n set:\n\n\nid\n:\n \n7cf830f2-82f3-4d5d-a63c-7bbe50c22b32\n\n\nname\n:\n \nMongoDB\n\n\nhostTypes\n:\n\n  \n-\n \nUNIX\n\n\nentryPoint\n:\n \nmongo_runner:mongodb\n\n\nsrcDir\n:\n \nsrc/\n\n\nschemaFile\n:\n \nschema.json\n\n\nmanualDiscovery\n:\n \nfalse\n\n\npluginType\n:\n \nDIRECT\n\n\nlanguage\n:\n \nPYTHON27\n\n\nexternalVersion\n:\n \n\"MongoDB\n \n1.0\"\n\n\nbuildNumber\n:\n \n\"1\"",
            "title": "Plugin Config"
        },
        {
            "location": "/References/Plugin_Config/#plugin-config",
            "text": "The plugin config is a  YAML  file that marks the root of a plugin and defines metadata about the plugin and its structure. The config file is read at build time to generate the upload artifact.  The name of the file can be specified during the build. By default, the build looks for  plugin_config.yml  in the current working directory.",
            "title": "Plugin Config"
        },
        {
            "location": "/References/Plugin_Config/#fields",
            "text": "Field Name  Required  Type  Description      id  Y  string  The unique id of the plugin in a valid UUID format.    name  N  string  The display name of the plugin. This will be used in the UI. If it is not specified name will be equal to id.    externalVersion  N  string  The plugin's  external version . This is a freeform string. If it is not supplied, the build number is used as an external version.    buildNumber  Y  string  The plugin's  build number . This string must conform to the format described  here .    hostTypes  Y  list  The host type that the plugin supports. Either  UNIX  or  WINDOWS .    schemaFile  Y  string  The path to the JSON file that contains the  plugin's schema definitions . This path can be absolute or relative to the directory containing the plugin config file.    srcDir  Y  string  The path to the directory that contains the source code for the plugin. During execution of a plugin operation, this directory will be the current working directory of the Python interpreter. Any modules or resources defined outside of this directory will be inaccessible at runtime. This path can be absolute or relative to the directory containing the plugin config file.    entryPoint  Y  string  A fully qualified Python symbol that points to the  dlpx.virtualization.platform.Plugin  object that defines the plugin. It must be in the form  importable.module:object_name  where  importable.module  is in  srcDir .    manualDiscovery  N  boolean  True if the plugin supports manual discovery of source config objects. The default value is  true .    pluginType  Y  enum  The ingestion strategy of the plugin. Can be either  STAGED  or  DIRECT .    language  Y  enum  Must be  PYTHON27 .    defaultLocale  N  enum  The locale to be used by the plugin if the Delphix user does not specify one. Plugin messages will be displayed in this locale by default. The default value is  en-us .    rootSquashEnabled  N  boolean  This dictates whether \"root squash\" is enabled on NFS mounts for the plugin (i.e. whether the  root  user on remote hosts has access to the NFS mounts). Setting this to  false  allows processes usually run as  root , like Docker daemons, access to the NFS mounts. The default value is  true . This field only applies to Unix hosts.",
            "title": "Fields"
        },
        {
            "location": "/References/Plugin_Config/#example",
            "text": "Assume the following basic plugin structure:  \u251c\u2500\u2500 plugin_config.yml\n\u251c\u2500\u2500 schema.json\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 mongo_runner.py  mongo_runner.py  contains:  from   dlpx.virtualization.platform   import   Plugin  mongodb   =   Plugin ()   This is a valid plugin config for the plugin:  id :   7cf830f2-82f3-4d5d-a63c-7bbe50c22b32  name :   MongoDB  version :   2.0.0  hostTypes : \n   -   UNIX  entryPoint :   mongo_runner:mongodb  srcDir :   src/  schemaFile :   schema.json  pluginType :   DIRECT  language :   PYTHON27  buildNumber :   0.1.0   This is a valid plugin config for the plugin with  manualDiscovery  set to  false  and an  externalVersion  set:  id :   7cf830f2-82f3-4d5d-a63c-7bbe50c22b32  name :   MongoDB  hostTypes : \n   -   UNIX  entryPoint :   mongo_runner:mongodb  srcDir :   src/  schemaFile :   schema.json  manualDiscovery :   false  pluginType :   DIRECT  language :   PYTHON27  externalVersion :   \"MongoDB   1.0\"  buildNumber :   \"1\"",
            "title": "Example"
        },
        {
            "location": "/References/Decorators/",
            "text": "Decorators\n\u00b6\n\n\nThe Virtualization SDK exposes decorators to be able to annotate functions that correspond to each \nPlugin Operation\n.\nIn the example below, it first instantiates a \nPlugin()\n object, that can then be used to tag plugin operations.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\n# Initialize a plugin object\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n# Use the decorator to annotate the function that corresponds to the \"Virtual Source Start\" Plugin Operation\n\n\n@plugin.virtual_source.start\n()\n\n\ndef\n \nmy_start\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \nprint\n \n\"running start\"\n \n\n\n\n\n\n\n\nInfo\n\n\nDecorators exposed by the Virtualization SDK are inherently python function calls and needs parentheses \n()\n appended at the end.\n\n\n\n\nAssuming the name of the object, is \nplugin\n as above, the table below lists the corresponding decorators for each plugin operation.\n\n\n\n\n\n\n\n\nPlugin Operation\n\n\nDecorator\n\n\n\n\n\n\n\n\n\n\nRepository Discovey\n\n\n@plugin.discovery.repository()\n\n\n\n\n\n\nSource Config Discovey\n\n\n@plugin.discovery.source_config()\n\n\n\n\n\n\nDirect Linked Source Pre-Snapshot\n\n\n@plugin.linked.pre_snapshot()\n\n\n\n\n\n\nDirect Linked Source Post-Snapshot\n\n\n@plugin.linked.post_snapshot()\n\n\n\n\n\n\nStaged Linked Source Pre-Snapshot\n\n\n@plugin.linked.pre_snapshot()\n\n\n\n\n\n\nStaged Linked Source Post-Snapshot\n\n\n@plugin.linked.post_snapshot()\n\n\n\n\n\n\nStaged Linked Source Start-Staging\n\n\n@plugin.linked.start_staging()\n\n\n\n\n\n\nStaged Linked Source Stop-Staging\n\n\n@plugin.linked.stop_staging()\n\n\n\n\n\n\nStaged Linked Source Status\n\n\n@plugin.linked.status()\n\n\n\n\n\n\nStaged Linked Source Worker\n\n\n@plugin.linked.worker()\n\n\n\n\n\n\nStaged Linked Source Mount Specification\n\n\n@plugin.linked.mount_specification()\n\n\n\n\n\n\nVirtual Source Configure\n\n\n@plugin.virtual.configure()\n\n\n\n\n\n\nVirtual Source Unconfigure\n\n\n@plugin.virtual.unconfigure()\n\n\n\n\n\n\nVirtual Source Reconfigure\n\n\n@plugin.virtual.reconfigure()\n\n\n\n\n\n\nVirtual Source Start\n\n\n@plugin.virtual.start()\n\n\n\n\n\n\nVirtual Source Stop\n\n\n@plugin.virtual.stop()\n\n\n\n\n\n\nVirtualSource Pre-Snapshot\n\n\n@plugin.virtual.pre_snapshot()\n\n\n\n\n\n\nVirtual Source Post-Snapshot\n\n\n@plugin.virtual.post_snapshot()\n\n\n\n\n\n\nVirtual Source Mount Specification\n\n\n@plugin.virtual.mount_specification()\n\n\n\n\n\n\nVirtual Source Status\n\n\n@plugin.virtual.status()\n\n\n\n\n\n\nRepository Data Migration\n\n\n@plugin.upgrade.repository(migration_id)\n\n\n\n\n\n\nSource Config Data Migration\n\n\n@plugin.upgrade.source_config(migration_id)\n\n\n\n\n\n\nLinked Source Data Migration\n\n\n@plugin.upgrade.linked_source(migration_id)\n\n\n\n\n\n\nVirtual Source Data Migration\n\n\n@plugin.upgrade.virtual_source(migration_id)\n\n\n\n\n\n\nSnapshot Data Migration\n\n\n@plugin.upgrade.snapshot(migration_id)\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nA plugin should only implement the \ndirect\n operations or the \nstaged\n operations based on the \nplugin type",
            "title": "Decorators"
        },
        {
            "location": "/References/Decorators/#decorators",
            "text": "The Virtualization SDK exposes decorators to be able to annotate functions that correspond to each  Plugin Operation .\nIn the example below, it first instantiates a  Plugin()  object, that can then be used to tag plugin operations.  from   dlpx.virtualization.platform   import   Plugin  # Initialize a plugin object  plugin   =   Plugin ()  # Use the decorator to annotate the function that corresponds to the \"Virtual Source Start\" Plugin Operation  @plugin.virtual_source.start ()  def   my_start ( virtual_source ,   repository ,   source_config ): \n   print   \"running start\"     Info  Decorators exposed by the Virtualization SDK are inherently python function calls and needs parentheses  ()  appended at the end.   Assuming the name of the object, is  plugin  as above, the table below lists the corresponding decorators for each plugin operation.     Plugin Operation  Decorator      Repository Discovey  @plugin.discovery.repository()    Source Config Discovey  @plugin.discovery.source_config()    Direct Linked Source Pre-Snapshot  @plugin.linked.pre_snapshot()    Direct Linked Source Post-Snapshot  @plugin.linked.post_snapshot()    Staged Linked Source Pre-Snapshot  @plugin.linked.pre_snapshot()    Staged Linked Source Post-Snapshot  @plugin.linked.post_snapshot()    Staged Linked Source Start-Staging  @plugin.linked.start_staging()    Staged Linked Source Stop-Staging  @plugin.linked.stop_staging()    Staged Linked Source Status  @plugin.linked.status()    Staged Linked Source Worker  @plugin.linked.worker()    Staged Linked Source Mount Specification  @plugin.linked.mount_specification()    Virtual Source Configure  @plugin.virtual.configure()    Virtual Source Unconfigure  @plugin.virtual.unconfigure()    Virtual Source Reconfigure  @plugin.virtual.reconfigure()    Virtual Source Start  @plugin.virtual.start()    Virtual Source Stop  @plugin.virtual.stop()    VirtualSource Pre-Snapshot  @plugin.virtual.pre_snapshot()    Virtual Source Post-Snapshot  @plugin.virtual.post_snapshot()    Virtual Source Mount Specification  @plugin.virtual.mount_specification()    Virtual Source Status  @plugin.virtual.status()    Repository Data Migration  @plugin.upgrade.repository(migration_id)    Source Config Data Migration  @plugin.upgrade.source_config(migration_id)    Linked Source Data Migration  @plugin.upgrade.linked_source(migration_id)    Virtual Source Data Migration  @plugin.upgrade.virtual_source(migration_id)    Snapshot Data Migration  @plugin.upgrade.snapshot(migration_id)      Warning  A plugin should only implement the  direct  operations or the  staged  operations based on the  plugin type",
            "title": "Decorators"
        },
        {
            "location": "/References/Plugin_Operations/",
            "text": "Plugin Operations\n\u00b6\n\n\n\n\nWarning\n\n\nIf a Plugin Operation is \nRequired\n and is not present, the corresponding Delphix Engine Operation will fail when invoked. The plugin can still be built and uploaded to the Delphix Engine.\n\n\n\n\n\n\nWarning\n\n\nFor each operation, the argument names must match exactly. For example, the Repository Discovery\noperation must have a single argument named \nsource_connection\n.\n\n\n\n\n\n\n\n\n\n\nPlugin Operation\n\n\nRequired\n\n\nDecorator\n\n\nDelphix Engine Operations\n\n\n\n\n\n\n\n\n\n\nRepository\nDiscovery\n\n\nYes\n\n\ndiscovery.repository()\n\n\nEnvironment Discovery\nEnvironment Refresh\n\n\n\n\n\n\nSource Config\nDiscovery\n\n\nYes\n\n\ndiscovery.source_config()\n\n\nEnvironment Discovery\nEnvironment Refresh\n\n\n\n\n\n\nDirect Linked Source\nPre-Snapshot\n\n\nNo\n\n\nlinked.pre_snapshot()\n\n\nLinked Source Sync\n\n\n\n\n\n\nDirect Linked Source\nPost-Snapshot\n\n\nYes\n\n\nlinked.post_snapshot()\n\n\nLinked Source Sync\n\n\n\n\n\n\nStaged Linked Source\nPre-Snapshot\n\n\nNo\n\n\nlinked.pre_snapshot()\n\n\nLinked Source Sync\n\n\n\n\n\n\nStaged Linked Source\nPost-Snapshot\n\n\nYes\n\n\nlinked.post_snapshot()\n\n\nLinked Source Sync\n\n\n\n\n\n\nStaged Linked Source\nStart-Staging\n\n\nNo\n\n\nlinked.start_staging()\n\n\nLinked Source Enable\n\n\n\n\n\n\nStaged Linked Source\nStop-Staging\n\n\nNo\n\n\nlinked.stop_staging()\n\n\nLinked Source Disable\nLinked Source Delete\n\n\n\n\n\n\nStaged Linked Source\nStatus\n\n\nNo\n\n\nlinked.status()\n\n\nN/A\n\n\n\n\n\n\nStaged Linked Source\nWorker\n\n\nNo\n\n\nlinked.worker()\n\n\nN/A\n\n\n\n\n\n\nStaged Linked Source\nMount Specification\n\n\nYes\n\n\nlinked.mount_specification()\n\n\nLinked Source Sync\nLinked Source Enable\n\n\n\n\n\n\nVirtual Source\nConfigure\n\n\nYes\n\n\nvirtual.configure()\n\n\nVirtual Source Provision\nVirtual Source Refresh\n\n\n\n\n\n\nVirtual Source\nUnconfigure\n\n\nNo\n\n\nvirtual.unconfigure()\n\n\nVirtual Source Refresh\nVirtual Source Delete\n\n\n\n\n\n\nVirtual Source\nReconfigure\n\n\nYes\n\n\nvirtual.reconfigure()\n\n\nVirtual Source Rollback\nVirtual Source Enable\n\n\n\n\n\n\nVirtual Source\nStart\n\n\nNo\n\n\nvirtual.start()\n\n\nVirtual Source Start\n\n\n\n\n\n\nVirtual Source\nStop\n\n\nNo\n\n\nvirtual.stop()\n\n\nVirtual Source Stop\n\n\n\n\n\n\nVirtual Source\nPre-Snapshot\n\n\nNo\n\n\nvirtual.pre_snapshot()\n\n\nVirtual Source Snapshot\n\n\n\n\n\n\nVirtual Source\nPost-Snapshot\n\n\nYes\n\n\nvirtual.post_snapshot()\n\n\nVirtual Source Snapshot\n\n\n\n\n\n\nVirtual Source\nMount Specification\n\n\nYes\n\n\nvirtual.mount_specification()\n\n\nVirtual Source Enable\nVirtual Source Provision\nVirtual Source Refresh\nVirtual Source Rollback\nVirtual Source Start\n\n\n\n\n\n\nVirtual Source\nStatus\n\n\nNo\n\n\nvirtual.status()\n\n\nVirtual Source Enable\n\n\n\n\n\n\nRepository Data Migration\n\n\nNo\n\n\nupgrade.repository(migration_id)\n\n\nUpgrade\n\n\n\n\n\n\nSource Config Data Migration\n\n\nNo\n\n\nupgrade.source_config(migration_id)\n\n\nUpgrade\n\n\n\n\n\n\nLinked Source Data Migration\n\n\nNo\n\n\nupgrade.linked_source(migration_id)\n\n\nUpgrade\n\n\n\n\n\n\nVirtual Source Data Migration\n\n\nNo\n\n\nupgrade.virtual_source(migration_id)\n\n\nUpgrade\n\n\n\n\n\n\nSnapshot Data Migration\n\n\nNo\n\n\nupgrade.snapshot(migration_id)\n\n\nUpgrade\n\n\n\n\n\n\n\n\nRepository Discovery\n\u00b6\n\n\nDiscovers the set of \nrepositories\n for a plugin on an \nenvironment\n. For a DBMS, this can correspond to the set of binaries installed on a Unix host.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nEnvironment Refresh\n\n\nEnvironment Discovery\n\n\n\n\nSignature\n\u00b6\n\n\ndef repository_discovery(source_connection)\n\n\nDecorator\n\u00b6\n\n\ndiscovery.repository()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsource_connection\n\n\nRemoteConnection\n\n\nThe connection associated with the remote environment to run repository discovery\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nA list of \nRepositoryDefinition\n objects.\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nRepositoryDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.discovery.repository\n()\n\n\ndef\n \nrepository_discovery\n(\nsource_connection\n):\n\n  \n# Initialize the object, filling in all required fields\n\n  \nrepository\n \n=\n \nRepositoryDefinition\n(\ninstallPath\n=\n\"/usr/bin/install\"\n)\n\n  \n# Set any additional non-required properties\n\n  \nrepository\n.\nversion\n \n=\n \n\"1.2.3\"\n\n  \n# Return one single repository\n\n  \nreturn\n \n[\nrepository\n]\n\n\n\n\n\n\n\n\nThe above command assumes a \nRepository Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"required\"\n:\n \n[\n\"installPath\"\n],\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"installPath\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"version\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"installPath\"\n],\n\n  \n\"nameField\"\n:\n \n[\n\"installPath\"\n]\n    \n\n}\n\n\n\n\n\n\nSource Config Discovery\n\u00b6\n\n\nDiscovers the set of \nsource configs\n for a plugin for a \nrepository\n. For a DBMS, this can correspond to the set of unique databases running using a particular installation on a Unix host.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nEnvironment Refresh\n\n\nEnvironment Discovery\n\n\n\n\nSignature\n\u00b6\n\n\ndef source_config_discovery(source_connection, repository)\n\n\nDecorator\n\u00b6\n\n\ndiscovery.source_config()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsource_connection\n\n\nRemoteConnection\n\n\nThe connection to the remote environment the corresponds to the repository.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository to discover source configs for.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nA list of \nSourceConfigDefinition\n objects.\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nSourceConfigDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.discovery.source_config\n()\n\n\ndef\n \nsource_config_discovery\n(\nsource_connection\n,\n \nrepository\n):\n\n  \nsource_config\n \n=\n \nSourceConfigDefinition\n(\nname\n=\n\"my_name\"\n,\n \nport\n=\n1000\n)\n\n  \nreturn\n \n[\nsource_config\n]\n\n\n\n\n\n\n\n\nThe above command assumes a \nSource Config Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"required\"\n:\n \n[\n\"name\"\n],\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"port\"\n:\n \n{\n \n\"type\"\n:\n \n\"number\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"name\"\n],\n\n  \n\"nameField\"\n:\n \n[\n\"name\"\n]\n    \n\n}\n\n\n\n\n\n\nDirect Linked Source Pre-Snapshot\n\u00b6\n\n\nSets up a \ndSource\n to ingest data. Only applies when using a \nDirect Linking\n strategy.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nLinked Source Sync\n\n\n\n\nSignature\n\u00b6\n\n\ndef linked_pre_snapshot(direct_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nlinked.pre_snapshot()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndirect_source\n\n\nDirectSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nSourceConfigDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \nlinked_pre_snapshot\n(\ndirect_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \npass\n\n\n\n\n\n\nDirect Linked Source Post-Snapshot\n\u00b6\n\n\nCaptures metadata from a \ndSource\n once data has been ingested. Only applies when using a \nDirect Linking\n strategy.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nLinked Source Sync\n\n\n\n\nSignature\n\u00b6\n\n\ndef linked_post_snapshot(direct_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nlinked.post_snapshot()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ndirect_source\n\n\nDirectSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nSnapshotDefinition\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nSnapshotDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.post_snapshot\n()\n\n\ndef\n \nlinked_post_snapshot\n(\ndirect_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \nsnapshot\n \n=\n \nSnapshotDefinition\n()\n\n  \nsnapshot\n.\ntransaction_id\n \n=\n \n1000\n\n  \nreturn\n \nsnapshot\n\n\n\n\n\n\n\n\nThe above command assumes a \nSnapshot Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"transactionId\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nStaged Linked Source Pre-Snapshot\n\u00b6\n\n\nSets up a \ndSource\n to ingest data. Only applies when using a \nStaged Linking\n strategy.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nLinked Source Sync\n\n\n\n\nSignature\n\u00b6\n\n\ndef linked_pre_snapshot(staged_source, repository, source_config, snapshot_parameters)\n\n\nDecorator\n\u00b6\n\n\nlinked.pre_snapshot()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstaged_source\n\n\nStagedSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\nsnapshot_parameters\n\n\nSnapshotParametersDefinition\n\n\nThe snapshot parameters.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \nlinked_pre_snapshot\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n,\n \nsnapshot_parameters\n):\n\n  \npass\n\n\n\n\n\n\nStaged Linked Source Post-Snapshot\n\u00b6\n\n\nCaptures metadata from a \ndSource\n once data has been ingested. Only applies when using a \nStaged Linking\n strategy.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nLinked Source Sync\n\n\n\n\nSignature\n\u00b6\n\n\ndef linked_post_snapshot(staged_source, repository, source_config, snapshot_parameters)\n\n\nDecorator\n\u00b6\n\n\nlinked.post_snapshot()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstaged_source\n\n\nStagedSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\nsnapshot_parameters\n\n\nSnapshotParametersDefinition\n\n\nThe snapshot parameters.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nSnapshotDefinition\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nSnapshotDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.post_snapshot\n()\n\n\ndef\n \nlinked_post_snapshot\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n,\n \nsnapshot_parameters\n):\n\n  \nsnapshot\n \n=\n \nSnapshotDefinition\n()\n\n  \nif\n \nsnapshot_parameters\n.\nresync\n:\n\n    \nsnapshot\n.\ntransaction_id\n \n=\n \n1000\n\n  \nelse\n:\n\n    \nsnapshot\n.\ntransaction_id\n \n=\n \n10\n\n  \nreturn\n \nsnapshot\n\n\n\n\n\n\n\n\nThe above command assumes a \nSnapshot Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"transactionId\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nStaged Linked Source Start-Staging\n\u00b6\n\n\nSets up a \nStaging Source\n to ingest data. Only applies when using a \nStaged Linking\n strategy.\nRequired to implement for Delphix Engine operations:\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nLinked Source Enable\n\n\n\n\nSignature\n\u00b6\n\n\ndef start_staging(staged_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nlinked.start_staging()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstaged_source\n\n\nStagedSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.start_staging\n()\n\n\ndef\n \nstart_staging\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \npass\n\n\n\n\n\n\nStaged Linked Source Stop-Staging\n\u00b6\n\n\nQuiesces a \nStaging Source\n to pause ingestion. Only applies when using a \nStaged Linking\n strategy.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nLinked Source Disable\n\n\nLinked Source Delete\n\n\n\n\nSignature\n\u00b6\n\n\ndef stop_staging(staged_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nlinked.stop_staging()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstaged_source\n\n\nStagedSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExamples\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.stop_staging\n()\n\n\ndef\n \nstop_staging\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \npass\n\n\n\n\n\n\nStaged Linked Source Status\n\u00b6\n\n\nDetermines the status of a \nStaging Source\n to show end users whether it is healthy or not. Only applies when using a \nStaged Linking\n strategy.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\nIf not implemented, the platform assumes that the status is \nStatus.ACTIVE\n\n\nDelphix Engine Operations\n\u00b6\n\n\nN/A\n\n\nSignature\n\u00b6\n\n\ndef linked_status(staged_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nlinked.status()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstaged_source\n\n\nStagedSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nStatus\n\n\nStatus.ACTIVE\n if the plugin operation is not implemented.\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nStatus\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.status\n()\n\n\ndef\n \nlinked_status\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \nreturn\n \nStatus\n.\nACTIVE\n\n\n\n\n\n\nStaged Linked Source Worker\n\u00b6\n\n\nMonitors the status of a \nStaging Source\n on a reqular interval. It can be used to fix up any errors on staging if it is not functioning as expected. Only applies when using a \nStaged Linking\n strategy.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\nDelphix Engine Operations\n\u00b6\n\n\nN/A\n\n\nSignature\n\u00b6\n\n\ndef worker(staged_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nlinked.worker()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstaged_source\n\n\nStagedSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.worker\n()\n\n\ndef\n \nworker\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \npass\n\n\n\n\n\n\nStaged Linked Source Mount Specification\n\u00b6\n\n\nReturns configurations for the mounts associated for data in staged source. The \nownership_specification\n is optional. If not specified, the platform will default the ownership settings to the environment user used for the Delphix Operation.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nLinked Source Sync\n\n\nLinked Source Enable\n\n\n\n\nSignature\n\u00b6\n\n\ndef linked_mount_specification(staged_source, repository)\n\n\nDecorator\n\u00b6\n\n\nlinked.mount_specification()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nstaged_source\n\n\nStagedSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nMountSpecification\n\n\nExample\n\u00b6\n\n\n\n\nInfo\n\n\nownership_specification\n only applies to Unix hosts.\n\n\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nMount\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nMountSpecification\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nOwenershipSpecification\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nSnapshotDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.mount_specification\n()\n\n\ndef\n \nlinked_mount_specification\n(\nstaged_source\n,\n \nrepository\n):\n\n  \nmount\n \n=\n \nMount\n(\nstaged_source\n.\nstaged_connection\n.\nenvironment\n,\n \n\"/some/path\"\n)\n\n  \nownership_spec\n \n=\n \nOwenershipSpecification\n(\nrepository\n.\nuid\n,\n \nrepository\n.\ngid\n)\n\n\n  \nreturn\n \nMountSpecification\n([\nmount\n],\n \nownership_spec\n)\n\n\n\n\n\n\nVirtual Source Configure\n\u00b6\n\n\nConfigures the data in a particular snapshot to be usable on a target environment. For database data files, this may mean recovering from a crash consistent format or backup. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Provision\n\n\nVirtual Source Refresh\n\n\n\n\nSignature\n\u00b6\n\n\ndef configure(virtual_source, snapshot, repository)\n\n\nDecorator\n\u00b6\n\n\nvirtual.configure()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nsnapshot\n\n\nSnapshotDefinition\n\n\nThe snapshot of the data set to configure.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nSourceConfigDefinition\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nSourceConfigDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.configure\n()\n\n\ndef\n \nconfigure\n(\nvirtual_source\n,\n \nrepository\n,\n \nsnapshot\n):\n\n  \nsource_config\n \n=\n \nSourceConfigDefinition\n(\nname\n=\n\"config_name\"\n)\n\n  \nreturn\n \nsource_config\n\n\n\n\n\n\n\n\nThe above command assumes a \nSourceConfig Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"required\"\n:\n \n[\n\"name\"\n],\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"name\"\n],\n\n  \n\"nameField\"\n:\n \n[\n\"name\"\n]\n\n\n}\n\n\n\n\n\n\nVirtual Source Unconfigure\n\u00b6\n\n\nQuiesces the virtual source on a target environment. For database data files, shutting down and unregistering a database on a host.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Refresh\n\n\nVirtual Source Delete\n\n\n\n\nSignature\n\u00b6\n\n\ndef unconfigure(virtual_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nvirtual.unconfigure()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.unconfigure\n()\n\n\ndef\n \nunconfigure\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \npass\n\n\n\n\n\n\nVirtual Source Reconfigure\n\u00b6\n\n\nRe-configures the data for a virtual source to point to the data in a prior snapshot for the virtual source. For database data files, this may mean recovering from a crash consistent format or backup of a new snapshot. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Rollback\n\n\nVirtual Source Enable\n\n\n\n\nSignature\n\u00b6\n\n\ndef reconfigure(virtual_source, repository, source_config, snapshot)\n\n\nDecorator\n\u00b6\n\n\nvirtual.reconfigure()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nsnapshot\n\n\nSnapshotDefinition\n\n\nThe snapshot of the data set to configure.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nSourceConfigDefinition\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nSourceConfigDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.reconfigure\n()\n\n\ndef\n \nreconfigure\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n,\n \nsnapshot\n):\n\n  \nreturn\n \nSourceConfigDefinition\n(\nname\n=\n\"updated_config_name\"\n)\n\n\n\n\n\n\n\n\nThe above command assumes a \nSourceConfig Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"required\"\n:\n \n[\n\"name\"\n],\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"name\"\n],\n\n  \n\"nameField\"\n:\n \n[\n\"name\"\n]\n\n\n}\n\n\n\n\n\n\nVirtual Source Start\n\u00b6\n\n\nExecuted whenever the data should be placed in a \"running\" state.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Start\n\n\n\n\nSignature\n\u00b6\n\n\ndef start(virtual_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nvirtual.start()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.start\n()\n\n\ndef\n \nstart\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \npass\n\n\n\n\n\n\nVirtual Source Stop\n\u00b6\n\n\nExecuted whenever the data needs to be shut down.\nRequired to implement for Delphix Engine operations:\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Stop\n\n\n\n\nSignature\n\u00b6\n\n\ndef stop(virtual_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nvirtual.stop()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.stop\n()\n\n\ndef\n \nstop\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \npass\n\n\n\n\n\n\nVirtual Source Pre-Snapshot\n\u00b6\n\n\nPrepares the virtual source for taking a snapshot of the data.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Snapshot\n\n\n\n\nSignature\n\u00b6\n\n\ndef virtual_pre_snapshot(virtual_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nvirtual.pre_snapshot()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.pre_snapshot\n()\n\n\ndef\n \nvirtual_pre_snapshot\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \npass\n\n\n\n\n\n\nVirtual Source Post-Snapshot\n\u00b6\n\n\nCaptures metadata after a snapshot.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Snapshot\n\n\n\n\nSignature\n\u00b6\n\n\ndef virtual_post_snapshot(virtual_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nvirtual.post_snapshot()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nSnapshotDefinition\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nSnapshotDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.post_snapshot\n()\n\n\ndef\n \nvirtual_post_snapshot\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \nsnapshot\n \n=\n \nSnapshotDefinition\n()\n\n  \nsnapshot\n.\ntransaction_id\n \n=\n \n1000\n\n  \nreturn\n \nsnapshot\n\n\n\n\n\n\n\n\nThe above command assumes a \nSnapshot Schema\n defined as:\n\n\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"transactionId\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nVirtual Source Mount Specification\n\u00b6\n\n\nReturns configurations for the mounts associated for data in virtual source.\nThe \nownership_specification\n is optional. If not specified, the platform will default the ownership settings to the environment user used for the Delphix Operation.\n\n\nRequired / Optional\n\u00b6\n\n\nRequired.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Enable\n\n\nVirtual Source Provision\n\n\nVirtual Source Refresh\n\n\nVirtual Source Rollback\n\n\nVirtual Source Start\n\n\n\n\nSignature\n\u00b6\n\n\ndef virtual_mount_specification(virtual_source, repository)\n\n\nDecorator\n\u00b6\n\n\nvirtual.mount_specification()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nMountSpecification\n\n\nExample\n\u00b6\n\n\n\n\nInfo\n\n\nownership_specification\n only applies to Unix hosts.\n\n\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nMount\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nMountSpecification\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nOwenershipSpecification\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nSnapshotDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.mount_specification\n()\n\n\ndef\n \nvirtual_mount_specification\n(\nvirtual_source\n,\n \nrepository\n):\n\n  \nmount\n \n=\n \nMount\n(\nvirtual_source\n.\nconnection\n.\nenvironment\n,\n \n\"/some/path\"\n)\n\n  \nownership_spec\n \n=\n \nOwenershipSpecification\n(\nrepository\n.\nuid\n,\n \nrepository\n.\ngid\n)\n\n\n  \nreturn\n \nMountSpecification\n([\nmount\n],\n \nownership_spec\n)\n\n\n\n\n\n\nVirtual Source Status\n\u00b6\n\n\nDetermines the status of a \nVirtual Source\n to show end users whether it is healthy or not.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\nIf not implemented, the platform assumes that the status is \nStatus.ACTIVE\n.\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nVirtual Source Enable\n\n\n\n\nSignature\n\u00b6\n\n\ndef virtual_status(virtual_source, repository, source_config)\n\n\nDecorator\n\u00b6\n\n\nvirtual.status()\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nvirtual_source\n\n\nVirtualSource\n\n\nThe source associated with this operation.\n\n\n\n\n\n\nrepository\n\n\nRepositoryDefinition\n\n\nThe repository associated with this source.\n\n\n\n\n\n\nsource_config\n\n\nSourceConfigDefinition\n\n\nThe source config associated with this source.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nStatus\n\n\nStatus.ACTIVE\n if the plugin operation is not implemented.\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nStatus\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.status\n()\n\n\ndef\n \nvirtual_status\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \nreturn\n \nStatus\n.\nACTIVE\n\n\n\n\n\n\nRepository Data Migration\n\u00b6\n\n\nA Repository \nData Migration\n migrates repository data from an older \nschema\n format to an updated schema format.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\n\n\nWarning\n\n\nYou must ensure that all repository data will match your updated repository schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more repository data migrations.\n\n\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nUpgrade\n\n\n\n\nSignature\n\u00b6\n\n\ndef migrate_repository(old_repository)\n\n\nDecorator\n\u00b6\n\n\nupgrade.repository(migration_id)\n\n\nDecorator Arguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmigration_id\n\n\nString\n\n\nThe ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details \nhere\n.\n\n\n\n\n\n\n\n\nFunction Arguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nold_repository\n\n\nDictionary\n\n\nThe plugin-specific data associated with a repository, that conforms to the previous schema.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nThe function argument \nold_repository\n is a Python dictionary, where each property name appears exactly as described in the previous repository schema. This differs from non-upgrade-related operations, where the function arguments are \nautogenerated classes\n based on the schema.\n\n\n\n\nReturns\n\u00b6\n\n\nDictionary\n\nA migrated version of the \nold_repository\n input that must conform to the updated repository schema.\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.upgrade.repository\n(\n\"2019.12.15\"\n)\n\n\ndef\n \nadd_new_flag_to_repo\n(\nold_repository\n):\n\n  \nnew_repository\n \n=\n \ndict\n(\nold_repository\n)\n\n  \nnew_repository\n[\n\"useNewFeature\"\n]\n \n=\n \nFalse\n\n  \nreturn\n \nnew_repository\n\n\n\n\n\n\nSource Config Data Migration\n\u00b6\n\n\nA Source Config \nData Migration\n migrates source config data from an older \nschema\n format to an updated schema format.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\n\n\nWarning\n\n\nYou must ensure that all source config data will match your source config schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more source config data migrations.\n\n\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nUpgrade\n\n\n\n\nSignature\n\u00b6\n\n\ndef migrate_source_config(old_source_config)\n\n\nDecorator\n\u00b6\n\n\nupgrade.source_config(migration_id)\n\n\nDecorator Arguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmigration_id\n\n\nString\n\n\nThe ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details \nhere\n.\n\n\n\n\n\n\n\n\nFunction Arguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nold_source_config\n\n\nDictionary\n\n\nThe plugin-specific data associated with a source config, that conforms to the previous schema.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nThe function argument \nold_source_config\n is a Python dictionary, where each property name appears exactly as described in the previous source config schema. This differs from non-upgrade-related operations, where the function arguments are \nautogenerated classes\n based on the schema.\n\n\n\n\nReturns\n\u00b6\n\n\nDictionary\n\nA migrated version of the \nold_source_config\n input that must conform to the updated source config schema.\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.upgrade.source_config\n(\n\"2019.12.15\"\n)\n\n\ndef\n \nadd_new_flag_to_source_config\n(\nold_source_config\n):\n\n  \nnew_source_config\n \n=\n \ndict\n(\nold_source_config\n)\n\n  \nnew_source_config\n[\n\"useNewFeature\"\n]\n \n=\n \nFalse\n\n  \nreturn\n \nnew_source_config\n\n\n\n\n\n\nLinked Source Data Migration\n\u00b6\n\n\nA Linked Source \nData Migration\n migrates linked source data from an older \nschema\n format to an updated schema format.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\n\n\nWarning\n\n\nYou must ensure that all linked source data will match your linked source schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more linked source data migrations.\n\n\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nUpgrade\n\n\n\n\nSignature\n\u00b6\n\n\ndef migrate_linked_source(old_linked_source)\n\n\nDecorator\n\u00b6\n\n\nupgrade.linked_source(migration_id)\n\n\nDecorator Arguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmigration_id\n\n\nString\n\n\nThe ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details \nhere\n.\n\n\n\n\n\n\n\n\nFunction Arguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nold_linked_source\n\n\nDictionary\n\n\nThe plugin-specific data associated with a linked source, that conforms to the previous schema.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nThe function argument \nold_linked_source\n is a Python dictionary, where each property name appears exactly as described in the previous linked source schema. This differs from non-upgrade-related operations, where the function arguments are \nautogenerated classes\n based on the schema.\n\n\n\n\nReturns\n\u00b6\n\n\nDictionary\n\nA migrated version of the \nold_linked_source\n input that must conform to the updated linked source schema.\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.upgrade.linked_source\n(\n\"2019.12.15\"\n)\n\n\ndef\n \nadd_new_flag_to_dsource\n(\nold_linked_source\n):\n\n  \nnew_linked_source\n \n=\n \ndict\n(\nold_linked_source\n)\n\n  \nnew_linked_source\n[\n\"useNewFeature\"\n]\n \n=\n \nFalse\n\n  \nreturn\n \nnew_linked_source\n\n\n\n\n\n\nVirtual Source Data Migration\n\u00b6\n\n\nA Virtual Source \nData Migration\n migrates virtual source data from an older \nschema\n format to an updated schema format.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\n\n\nWarning\n\n\nYou must ensure that all virtual source data will match your virtual source schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more virtual source data migrations.\n\n\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nUpgrade\n\n\n\n\nSignature\n\u00b6\n\n\ndef migrate_virtual_source(old_virtual_source)\n\n\nDecorator\n\u00b6\n\n\nupgrade.virtual_source(migration_id)\n\n\nDecorator Arguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmigration_id\n\n\nString\n\n\nThe ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details \nhere\n.\n\n\n\n\n\n\n\n\nFunction Arguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nold_virtual_source\n\n\nDictionary\n\n\nThe plugin-specific data associated with a virtual source, that conforms to the previous schema.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nThe function argument \nold_virtual_source\n is a Python dictionary, where each property name appears exactly as described in the previous virtual source schema. This differs from non-upgrade-related operations, where the function arguments are \nautogenerated classes\n based on the schema.\n\n\n\n\nReturns\n\u00b6\n\n\nDictionary\n\nA migrated version of the \nold_virtual_source\n input that must conform to the updated virtual source schema.\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.upgrade.virtual_source\n(\n\"2019.12.15\"\n)\n\n\ndef\n \nadd_new_flag_to_vdb\n(\nold_virtual_source\n):\n\n  \nnew_virtual_source\n \n=\n \ndict\n(\nold_virtual_source\n)\n\n  \nnew_virtual_source\n[\n\"useNewFeature\"\n]\n \n=\n \nFalse\n\n  \nreturn\n \nnew_virtual_source\n\n\n\n\n\n\nSnapshot Data Migration\n\u00b6\n\n\nA Snapshot \nData Migration\n migrates snapshot data from an older \nschema\n format to an updated schema format.\n\n\nRequired / Optional\n\u00b6\n\n\nOptional.\n\n\n\n\nWarning\n\n\nYou must ensure that all snapshot data will match your snapshot schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more snapshot migrations.\n\n\n\n\nDelphix Engine Operations\n\u00b6\n\n\n\n\nUpgrade\n\n\n\n\nSignature\n\u00b6\n\n\ndef migrate_snapshot(old_snapshot)\n\n\nDecorator\n\u00b6\n\n\nupgrade.snapshot(migration_id)\n\n\nDecorator Arguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmigration_id\n\n\nString\n\n\nThe ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details \nhere\n.\n\n\n\n\n\n\n\n\nFunction Arguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nold_snapshot\n\n\nDictionary\n\n\nThe plugin-specific data associated with a snapshot, that conforms to the previous schema.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nThe function argument \nold_snapshot\n is a Python dictionary, where each property name appears exactly as described in the previous snapshot schema. This differs from non-upgrade-related operations, where the function arguments are \nautogenerated classes\n based on the schema.\n\n\n\n\nReturns\n\u00b6\n\n\nDictionary\n\nA migrated version of the \nold_snapshot\n input that must conform to the updated snapshot schema.\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.upgrade.snapshot\n(\n\"2019.12.15\"\n)\n\n\ndef\n \nadd_new_flag_to_snapshot\n(\nold_snapshot\n):\n\n  \nnew_snapshot\n \n=\n \ndict\n(\nold_snapshot\n)\n\n  \nnew_snapshot\n[\n\"useNewFeature\"\n]\n \n=\n \nFalse\n\n  \nreturn\n \nnew_snapshot",
            "title": "Plugin Operations"
        },
        {
            "location": "/References/Plugin_Operations/#plugin-operations",
            "text": "Warning  If a Plugin Operation is  Required  and is not present, the corresponding Delphix Engine Operation will fail when invoked. The plugin can still be built and uploaded to the Delphix Engine.    Warning  For each operation, the argument names must match exactly. For example, the Repository Discovery\noperation must have a single argument named  source_connection .      Plugin Operation  Required  Decorator  Delphix Engine Operations      Repository Discovery  Yes  discovery.repository()  Environment Discovery Environment Refresh    Source Config Discovery  Yes  discovery.source_config()  Environment Discovery Environment Refresh    Direct Linked Source Pre-Snapshot  No  linked.pre_snapshot()  Linked Source Sync    Direct Linked Source Post-Snapshot  Yes  linked.post_snapshot()  Linked Source Sync    Staged Linked Source Pre-Snapshot  No  linked.pre_snapshot()  Linked Source Sync    Staged Linked Source Post-Snapshot  Yes  linked.post_snapshot()  Linked Source Sync    Staged Linked Source Start-Staging  No  linked.start_staging()  Linked Source Enable    Staged Linked Source Stop-Staging  No  linked.stop_staging()  Linked Source Disable Linked Source Delete    Staged Linked Source Status  No  linked.status()  N/A    Staged Linked Source Worker  No  linked.worker()  N/A    Staged Linked Source Mount Specification  Yes  linked.mount_specification()  Linked Source Sync Linked Source Enable    Virtual Source Configure  Yes  virtual.configure()  Virtual Source Provision Virtual Source Refresh    Virtual Source Unconfigure  No  virtual.unconfigure()  Virtual Source Refresh Virtual Source Delete    Virtual Source Reconfigure  Yes  virtual.reconfigure()  Virtual Source Rollback Virtual Source Enable    Virtual Source Start  No  virtual.start()  Virtual Source Start    Virtual Source Stop  No  virtual.stop()  Virtual Source Stop    Virtual Source Pre-Snapshot  No  virtual.pre_snapshot()  Virtual Source Snapshot    Virtual Source Post-Snapshot  Yes  virtual.post_snapshot()  Virtual Source Snapshot    Virtual Source Mount Specification  Yes  virtual.mount_specification()  Virtual Source Enable Virtual Source Provision Virtual Source Refresh Virtual Source Rollback Virtual Source Start    Virtual Source Status  No  virtual.status()  Virtual Source Enable    Repository Data Migration  No  upgrade.repository(migration_id)  Upgrade    Source Config Data Migration  No  upgrade.source_config(migration_id)  Upgrade    Linked Source Data Migration  No  upgrade.linked_source(migration_id)  Upgrade    Virtual Source Data Migration  No  upgrade.virtual_source(migration_id)  Upgrade    Snapshot Data Migration  No  upgrade.snapshot(migration_id)  Upgrade",
            "title": "Plugin Operations"
        },
        {
            "location": "/References/Plugin_Operations/#repository-discovery",
            "text": "Discovers the set of  repositories  for a plugin on an  environment . For a DBMS, this can correspond to the set of binaries installed on a Unix host.",
            "title": "Repository Discovery"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations",
            "text": "Environment Refresh  Environment Discovery",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature",
            "text": "def repository_discovery(source_connection)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator",
            "text": "discovery.repository()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments",
            "text": "Argument  Type  Description      source_connection  RemoteConnection  The connection associated with the remote environment to run repository discovery",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns",
            "text": "A list of  RepositoryDefinition  objects.",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   generated.defintions   import   RepositoryDefinition  plugin   =   Plugin ()  @plugin.discovery.repository ()  def   repository_discovery ( source_connection ): \n   # Initialize the object, filling in all required fields \n   repository   =   RepositoryDefinition ( installPath = \"/usr/bin/install\" ) \n   # Set any additional non-required properties \n   repository . version   =   \"1.2.3\" \n   # Return one single repository \n   return   [ repository ]    The above command assumes a  Repository Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"required\" :   [ \"installPath\" ], \n   \"properties\" :   { \n     \"installPath\" :   {   \"type\" :   \"string\"   }, \n     \"version\" :   {   \"type\" :   \"string\"   } \n   }, \n   \"identityFields\" :   [ \"installPath\" ], \n   \"nameField\" :   [ \"installPath\" ]      }",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#source-config-discovery",
            "text": "Discovers the set of  source configs  for a plugin for a  repository . For a DBMS, this can correspond to the set of unique databases running using a particular installation on a Unix host.",
            "title": "Source Config Discovery"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_1",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_1",
            "text": "Environment Refresh  Environment Discovery",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_1",
            "text": "def source_config_discovery(source_connection, repository)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_1",
            "text": "discovery.source_config()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_1",
            "text": "Argument  Type  Description      source_connection  RemoteConnection  The connection to the remote environment the corresponds to the repository.    repository  RepositoryDefinition  The repository to discover source configs for.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_1",
            "text": "A list of  SourceConfigDefinition  objects.",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_1",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   generated.definitions   import   SourceConfigDefinition  plugin   =   Plugin ()  @plugin.discovery.source_config ()  def   source_config_discovery ( source_connection ,   repository ): \n   source_config   =   SourceConfigDefinition ( name = \"my_name\" ,   port = 1000 ) \n   return   [ source_config ]    The above command assumes a  Source Config Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"required\" :   [ \"name\" ], \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   }, \n     \"port\" :   {   \"type\" :   \"number\"   } \n   }, \n   \"identityFields\" :   [ \"name\" ], \n   \"nameField\" :   [ \"name\" ]      }",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#direct-linked-source-pre-snapshot",
            "text": "Sets up a  dSource  to ingest data. Only applies when using a  Direct Linking  strategy.",
            "title": "Direct Linked Source Pre-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_2",
            "text": "Optional",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_2",
            "text": "Linked Source Sync",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_2",
            "text": "def linked_pre_snapshot(direct_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_2",
            "text": "linked.pre_snapshot()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_2",
            "text": "Argument  Type  Description      direct_source  DirectSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_2",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_2",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   generated.definitions   import   SourceConfigDefinition  plugin   =   Plugin ()  @plugin.linked.pre_snapshot ()  def   linked_pre_snapshot ( direct_source ,   repository ,   source_config ): \n   pass",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#direct-linked-source-post-snapshot",
            "text": "Captures metadata from a  dSource  once data has been ingested. Only applies when using a  Direct Linking  strategy.",
            "title": "Direct Linked Source Post-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_3",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_3",
            "text": "Linked Source Sync",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_3",
            "text": "def linked_post_snapshot(direct_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_3",
            "text": "linked.post_snapshot()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_3",
            "text": "Argument  Type  Description      direct_source  DirectSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_3",
            "text": "SnapshotDefinition",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_3",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   generated.definitions   import   SnapshotDefinition  plugin   =   Plugin ()  @plugin.linked.post_snapshot ()  def   linked_post_snapshot ( direct_source ,   repository ,   source_config ): \n   snapshot   =   SnapshotDefinition () \n   snapshot . transaction_id   =   1000 \n   return   snapshot    The above command assumes a  Snapshot Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"transactionId\" :   {   \"type\" :   \"integer\"   } \n   }  }",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-source-pre-snapshot",
            "text": "Sets up a  dSource  to ingest data. Only applies when using a  Staged Linking  strategy.",
            "title": "Staged Linked Source Pre-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_4",
            "text": "Optional.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_4",
            "text": "Linked Source Sync",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_4",
            "text": "def linked_pre_snapshot(staged_source, repository, source_config, snapshot_parameters)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_4",
            "text": "linked.pre_snapshot()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_4",
            "text": "Argument  Type  Description      staged_source  StagedSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.    snapshot_parameters  SnapshotParametersDefinition  The snapshot parameters.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_4",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_4",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.linked.pre_snapshot ()  def   linked_pre_snapshot ( staged_source ,   repository ,   source_config ,   snapshot_parameters ): \n   pass",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-source-post-snapshot",
            "text": "Captures metadata from a  dSource  once data has been ingested. Only applies when using a  Staged Linking  strategy.",
            "title": "Staged Linked Source Post-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_5",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_5",
            "text": "Linked Source Sync",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_5",
            "text": "def linked_post_snapshot(staged_source, repository, source_config, snapshot_parameters)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_5",
            "text": "linked.post_snapshot()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_5",
            "text": "Argument  Type  Description      staged_source  StagedSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.    snapshot_parameters  SnapshotParametersDefinition  The snapshot parameters.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_5",
            "text": "SnapshotDefinition",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_5",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   generated.definitions   import   SnapshotDefinition  plugin   =   Plugin ()  @plugin.linked.post_snapshot ()  def   linked_post_snapshot ( staged_source ,   repository ,   source_config ,   snapshot_parameters ): \n   snapshot   =   SnapshotDefinition () \n   if   snapshot_parameters . resync : \n     snapshot . transaction_id   =   1000 \n   else : \n     snapshot . transaction_id   =   10 \n   return   snapshot    The above command assumes a  Snapshot Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"transactionId\" :   {   \"type\" :   \"integer\"   } \n   }  }",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-source-start-staging",
            "text": "Sets up a  Staging Source  to ingest data. Only applies when using a  Staged Linking  strategy.\nRequired to implement for Delphix Engine operations:",
            "title": "Staged Linked Source Start-Staging"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_6",
            "text": "Optional.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_6",
            "text": "Linked Source Enable",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_6",
            "text": "def start_staging(staged_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_6",
            "text": "linked.start_staging()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_6",
            "text": "Argument  Type  Description      staged_source  StagedSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_6",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_6",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.linked.start_staging ()  def   start_staging ( staged_source ,   repository ,   source_config ): \n   pass",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-source-stop-staging",
            "text": "Quiesces a  Staging Source  to pause ingestion. Only applies when using a  Staged Linking  strategy.",
            "title": "Staged Linked Source Stop-Staging"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_7",
            "text": "Optional.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_7",
            "text": "Linked Source Disable  Linked Source Delete",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_7",
            "text": "def stop_staging(staged_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_7",
            "text": "linked.stop_staging()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_7",
            "text": "Argument  Type  Description      staged_source  StagedSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_7",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#examples",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.linked.stop_staging ()  def   stop_staging ( staged_source ,   repository ,   source_config ): \n   pass",
            "title": "Examples"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-source-status",
            "text": "Determines the status of a  Staging Source  to show end users whether it is healthy or not. Only applies when using a  Staged Linking  strategy.",
            "title": "Staged Linked Source Status"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_8",
            "text": "Optional. \nIf not implemented, the platform assumes that the status is  Status.ACTIVE",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_8",
            "text": "N/A",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_8",
            "text": "def linked_status(staged_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_8",
            "text": "linked.status()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_8",
            "text": "Argument  Type  Description      staged_source  StagedSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_8",
            "text": "Status  Status.ACTIVE  if the plugin operation is not implemented.",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_7",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   dlpx.virtualization.platform   import   Status  plugin   =   Plugin ()  @plugin.linked.status ()  def   linked_status ( staged_source ,   repository ,   source_config ): \n   return   Status . ACTIVE",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-source-worker",
            "text": "Monitors the status of a  Staging Source  on a reqular interval. It can be used to fix up any errors on staging if it is not functioning as expected. Only applies when using a  Staged Linking  strategy.",
            "title": "Staged Linked Source Worker"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_9",
            "text": "Optional.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_9",
            "text": "N/A",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_9",
            "text": "def worker(staged_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_9",
            "text": "linked.worker()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_9",
            "text": "Argument  Type  Description      staged_source  StagedSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_9",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_8",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.linked.worker ()  def   worker ( staged_source ,   repository ,   source_config ): \n   pass",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#staged-linked-source-mount-specification",
            "text": "Returns configurations for the mounts associated for data in staged source. The  ownership_specification  is optional. If not specified, the platform will default the ownership settings to the environment user used for the Delphix Operation.",
            "title": "Staged Linked Source Mount Specification"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_10",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_10",
            "text": "Linked Source Sync  Linked Source Enable",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_10",
            "text": "def linked_mount_specification(staged_source, repository)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_10",
            "text": "linked.mount_specification()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_10",
            "text": "Argument  Type  Description      staged_source  StagedSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_10",
            "text": "MountSpecification",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_9",
            "text": "Info  ownership_specification  only applies to Unix hosts.   from   dlpx.virtualization.platform   import   Plugin  from   dlpx.virtualization.platform   import   Mount  from   dlpx.virtualization.platform   import   MountSpecification  from   dlpx.virtualization.platform   import   OwenershipSpecification  from   generated.definitions   import   SnapshotDefinition  plugin   =   Plugin ()  @plugin.linked.mount_specification ()  def   linked_mount_specification ( staged_source ,   repository ): \n   mount   =   Mount ( staged_source . staged_connection . environment ,   \"/some/path\" ) \n   ownership_spec   =   OwenershipSpecification ( repository . uid ,   repository . gid ) \n\n   return   MountSpecification ([ mount ],   ownership_spec )",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-configure",
            "text": "Configures the data in a particular snapshot to be usable on a target environment. For database data files, this may mean recovering from a crash consistent format or backup. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks.",
            "title": "Virtual Source Configure"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_11",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_11",
            "text": "Virtual Source Provision  Virtual Source Refresh",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_11",
            "text": "def configure(virtual_source, snapshot, repository)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_11",
            "text": "virtual.configure()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_11",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    snapshot  SnapshotDefinition  The snapshot of the data set to configure.    repository  RepositoryDefinition  The repository associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_11",
            "text": "SourceConfigDefinition",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_10",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   generated.defintions   import   SourceConfigDefinition  plugin   =   Plugin ()  @plugin.virtual.configure ()  def   configure ( virtual_source ,   repository ,   snapshot ): \n   source_config   =   SourceConfigDefinition ( name = \"config_name\" ) \n   return   source_config    The above command assumes a  SourceConfig Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"required\" :   [ \"name\" ], \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   } \n   }, \n   \"identityFields\" :   [ \"name\" ], \n   \"nameField\" :   [ \"name\" ]  }",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-unconfigure",
            "text": "Quiesces the virtual source on a target environment. For database data files, shutting down and unregistering a database on a host.",
            "title": "Virtual Source Unconfigure"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_12",
            "text": "Optional.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_12",
            "text": "Virtual Source Refresh  Virtual Source Delete",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_12",
            "text": "def unconfigure(virtual_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_12",
            "text": "virtual.unconfigure()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_12",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_12",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_11",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.virtual.unconfigure ()  def   unconfigure ( virtual_source ,   repository ,   source_config ): \n   pass",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-reconfigure",
            "text": "Re-configures the data for a virtual source to point to the data in a prior snapshot for the virtual source. For database data files, this may mean recovering from a crash consistent format or backup of a new snapshot. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks.",
            "title": "Virtual Source Reconfigure"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_13",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_13",
            "text": "Virtual Source Rollback  Virtual Source Enable",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_13",
            "text": "def reconfigure(virtual_source, repository, source_config, snapshot)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_13",
            "text": "virtual.reconfigure()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_13",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    snapshot  SnapshotDefinition  The snapshot of the data set to configure.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_13",
            "text": "SourceConfigDefinition",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_12",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   generated.definitions   import   SourceConfigDefinition  plugin   =   Plugin ()  @plugin.virtual.reconfigure ()  def   reconfigure ( virtual_source ,   repository ,   source_config ,   snapshot ): \n   return   SourceConfigDefinition ( name = \"updated_config_name\" )    The above command assumes a  SourceConfig Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"required\" :   [ \"name\" ], \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   } \n   }, \n   \"identityFields\" :   [ \"name\" ], \n   \"nameField\" :   [ \"name\" ]  }",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-start",
            "text": "Executed whenever the data should be placed in a \"running\" state.",
            "title": "Virtual Source Start"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_14",
            "text": "Optional.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_14",
            "text": "Virtual Source Start",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_14",
            "text": "def start(virtual_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_14",
            "text": "virtual.start()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_14",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_14",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_13",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.virtual.start ()  def   start ( virtual_source ,   repository ,   source_config ): \n   pass",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-stop",
            "text": "Executed whenever the data needs to be shut down.\nRequired to implement for Delphix Engine operations:",
            "title": "Virtual Source Stop"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_15",
            "text": "Optional.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_15",
            "text": "Virtual Source Stop",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_15",
            "text": "def stop(virtual_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_15",
            "text": "virtual.stop()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_15",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_15",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_14",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.virtual.stop ()  def   stop ( virtual_source ,   repository ,   source_config ): \n   pass",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-pre-snapshot",
            "text": "Prepares the virtual source for taking a snapshot of the data.",
            "title": "Virtual Source Pre-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_16",
            "text": "Optional.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_16",
            "text": "Virtual Source Snapshot",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_16",
            "text": "def virtual_pre_snapshot(virtual_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_16",
            "text": "virtual.pre_snapshot()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_16",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_16",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_15",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.virtual.pre_snapshot ()  def   virtual_pre_snapshot ( virtual_source ,   repository ,   source_config ): \n   pass",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-post-snapshot",
            "text": "Captures metadata after a snapshot.",
            "title": "Virtual Source Post-Snapshot"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_17",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_17",
            "text": "Virtual Source Snapshot",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_17",
            "text": "def virtual_post_snapshot(virtual_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_17",
            "text": "virtual.post_snapshot()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_17",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_17",
            "text": "SnapshotDefinition",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_16",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   generated.defintions   import   SnapshotDefinition  plugin   =   Plugin ()  @plugin.virtual.post_snapshot ()  def   virtual_post_snapshot ( virtual_source ,   repository ,   source_config ): \n   snapshot   =   SnapshotDefinition () \n   snapshot . transaction_id   =   1000 \n   return   snapshot    The above command assumes a  Snapshot Schema  defined as:   { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"transactionId\" :   {   \"type\" :   \"string\"   } \n   }  }",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-mount-specification",
            "text": "Returns configurations for the mounts associated for data in virtual source.\nThe  ownership_specification  is optional. If not specified, the platform will default the ownership settings to the environment user used for the Delphix Operation.",
            "title": "Virtual Source Mount Specification"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_18",
            "text": "Required.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_18",
            "text": "Virtual Source Enable  Virtual Source Provision  Virtual Source Refresh  Virtual Source Rollback  Virtual Source Start",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_18",
            "text": "def virtual_mount_specification(virtual_source, repository)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_18",
            "text": "virtual.mount_specification()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_18",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_18",
            "text": "MountSpecification",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_17",
            "text": "Info  ownership_specification  only applies to Unix hosts.   from   dlpx.virtualization.platform   import   Plugin  from   dlpx.virtualization.platform   import   Mount  from   dlpx.virtualization.platform   import   MountSpecification  from   dlpx.virtualization.platform   import   OwenershipSpecification  from   generated.definitions   import   SnapshotDefinition  plugin   =   Plugin ()  @plugin.virtual.mount_specification ()  def   virtual_mount_specification ( virtual_source ,   repository ): \n   mount   =   Mount ( virtual_source . connection . environment ,   \"/some/path\" ) \n   ownership_spec   =   OwenershipSpecification ( repository . uid ,   repository . gid ) \n\n   return   MountSpecification ([ mount ],   ownership_spec )",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-status",
            "text": "Determines the status of a  Virtual Source  to show end users whether it is healthy or not.",
            "title": "Virtual Source Status"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_19",
            "text": "Optional. \nIf not implemented, the platform assumes that the status is  Status.ACTIVE .",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_19",
            "text": "Virtual Source Enable",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_19",
            "text": "def virtual_status(virtual_source, repository, source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_19",
            "text": "virtual.status()",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#arguments_19",
            "text": "Argument  Type  Description      virtual_source  VirtualSource  The source associated with this operation.    repository  RepositoryDefinition  The repository associated with this source.    source_config  SourceConfigDefinition  The source config associated with this source.",
            "title": "Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_19",
            "text": "Status  Status.ACTIVE  if the plugin operation is not implemented.",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_18",
            "text": "from   dlpx.virtualization.platform   import   Plugin  from   dlpx.virtualization.platform   import   Status  plugin   =   Plugin ()  @plugin.virtual.status ()  def   virtual_status ( virtual_source ,   repository ,   source_config ): \n   return   Status . ACTIVE",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#repository-data-migration",
            "text": "A Repository  Data Migration  migrates repository data from an older  schema  format to an updated schema format.",
            "title": "Repository Data Migration"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_20",
            "text": "Optional.   Warning  You must ensure that all repository data will match your updated repository schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more repository data migrations.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_20",
            "text": "Upgrade",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_20",
            "text": "def migrate_repository(old_repository)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_20",
            "text": "upgrade.repository(migration_id)",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#decorator-arguments",
            "text": "Argument  Type  Description      migration_id  String  The ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details  here .",
            "title": "Decorator Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#function-arguments",
            "text": "Argument  Type  Description      old_repository  Dictionary  The plugin-specific data associated with a repository, that conforms to the previous schema.      Warning  The function argument  old_repository  is a Python dictionary, where each property name appears exactly as described in the previous repository schema. This differs from non-upgrade-related operations, where the function arguments are  autogenerated classes  based on the schema.",
            "title": "Function Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_20",
            "text": "Dictionary \nA migrated version of the  old_repository  input that must conform to the updated repository schema.",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_19",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.upgrade.repository ( \"2019.12.15\" )  def   add_new_flag_to_repo ( old_repository ): \n   new_repository   =   dict ( old_repository ) \n   new_repository [ \"useNewFeature\" ]   =   False \n   return   new_repository",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#source-config-data-migration",
            "text": "A Source Config  Data Migration  migrates source config data from an older  schema  format to an updated schema format.",
            "title": "Source Config Data Migration"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_21",
            "text": "Optional.   Warning  You must ensure that all source config data will match your source config schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more source config data migrations.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_21",
            "text": "Upgrade",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_21",
            "text": "def migrate_source_config(old_source_config)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_21",
            "text": "upgrade.source_config(migration_id)",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#decorator-arguments_1",
            "text": "Argument  Type  Description      migration_id  String  The ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details  here .",
            "title": "Decorator Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#function-arguments_1",
            "text": "Argument  Type  Description      old_source_config  Dictionary  The plugin-specific data associated with a source config, that conforms to the previous schema.      Warning  The function argument  old_source_config  is a Python dictionary, where each property name appears exactly as described in the previous source config schema. This differs from non-upgrade-related operations, where the function arguments are  autogenerated classes  based on the schema.",
            "title": "Function Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_21",
            "text": "Dictionary \nA migrated version of the  old_source_config  input that must conform to the updated source config schema.",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_20",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.upgrade.source_config ( \"2019.12.15\" )  def   add_new_flag_to_source_config ( old_source_config ): \n   new_source_config   =   dict ( old_source_config ) \n   new_source_config [ \"useNewFeature\" ]   =   False \n   return   new_source_config",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#linked-source-data-migration",
            "text": "A Linked Source  Data Migration  migrates linked source data from an older  schema  format to an updated schema format.",
            "title": "Linked Source Data Migration"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_22",
            "text": "Optional.   Warning  You must ensure that all linked source data will match your linked source schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more linked source data migrations.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_22",
            "text": "Upgrade",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_22",
            "text": "def migrate_linked_source(old_linked_source)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_22",
            "text": "upgrade.linked_source(migration_id)",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#decorator-arguments_2",
            "text": "Argument  Type  Description      migration_id  String  The ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details  here .",
            "title": "Decorator Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#function-arguments_2",
            "text": "Argument  Type  Description      old_linked_source  Dictionary  The plugin-specific data associated with a linked source, that conforms to the previous schema.      Warning  The function argument  old_linked_source  is a Python dictionary, where each property name appears exactly as described in the previous linked source schema. This differs from non-upgrade-related operations, where the function arguments are  autogenerated classes  based on the schema.",
            "title": "Function Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_22",
            "text": "Dictionary \nA migrated version of the  old_linked_source  input that must conform to the updated linked source schema.",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_21",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.upgrade.linked_source ( \"2019.12.15\" )  def   add_new_flag_to_dsource ( old_linked_source ): \n   new_linked_source   =   dict ( old_linked_source ) \n   new_linked_source [ \"useNewFeature\" ]   =   False \n   return   new_linked_source",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#virtual-source-data-migration",
            "text": "A Virtual Source  Data Migration  migrates virtual source data from an older  schema  format to an updated schema format.",
            "title": "Virtual Source Data Migration"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_23",
            "text": "Optional.   Warning  You must ensure that all virtual source data will match your virtual source schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more virtual source data migrations.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_23",
            "text": "Upgrade",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_23",
            "text": "def migrate_virtual_source(old_virtual_source)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_23",
            "text": "upgrade.virtual_source(migration_id)",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#decorator-arguments_3",
            "text": "Argument  Type  Description      migration_id  String  The ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details  here .",
            "title": "Decorator Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#function-arguments_3",
            "text": "Argument  Type  Description      old_virtual_source  Dictionary  The plugin-specific data associated with a virtual source, that conforms to the previous schema.      Warning  The function argument  old_virtual_source  is a Python dictionary, where each property name appears exactly as described in the previous virtual source schema. This differs from non-upgrade-related operations, where the function arguments are  autogenerated classes  based on the schema.",
            "title": "Function Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_23",
            "text": "Dictionary \nA migrated version of the  old_virtual_source  input that must conform to the updated virtual source schema.",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_22",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.upgrade.virtual_source ( \"2019.12.15\" )  def   add_new_flag_to_vdb ( old_virtual_source ): \n   new_virtual_source   =   dict ( old_virtual_source ) \n   new_virtual_source [ \"useNewFeature\" ]   =   False \n   return   new_virtual_source",
            "title": "Example"
        },
        {
            "location": "/References/Plugin_Operations/#snapshot-data-migration",
            "text": "A Snapshot  Data Migration  migrates snapshot data from an older  schema  format to an updated schema format.",
            "title": "Snapshot Data Migration"
        },
        {
            "location": "/References/Plugin_Operations/#required-optional_24",
            "text": "Optional.   Warning  You must ensure that all snapshot data will match your snapshot schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more snapshot migrations.",
            "title": "Required / Optional"
        },
        {
            "location": "/References/Plugin_Operations/#delphix-engine-operations_24",
            "text": "Upgrade",
            "title": "Delphix Engine Operations"
        },
        {
            "location": "/References/Plugin_Operations/#signature_24",
            "text": "def migrate_snapshot(old_snapshot)",
            "title": "Signature"
        },
        {
            "location": "/References/Plugin_Operations/#decorator_24",
            "text": "upgrade.snapshot(migration_id)",
            "title": "Decorator"
        },
        {
            "location": "/References/Plugin_Operations/#decorator-arguments_4",
            "text": "Argument  Type  Description      migration_id  String  The ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details  here .",
            "title": "Decorator Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#function-arguments_4",
            "text": "Argument  Type  Description      old_snapshot  Dictionary  The plugin-specific data associated with a snapshot, that conforms to the previous schema.      Warning  The function argument  old_snapshot  is a Python dictionary, where each property name appears exactly as described in the previous snapshot schema. This differs from non-upgrade-related operations, where the function arguments are  autogenerated classes  based on the schema.",
            "title": "Function Arguments"
        },
        {
            "location": "/References/Plugin_Operations/#returns_24",
            "text": "Dictionary \nA migrated version of the  old_snapshot  input that must conform to the updated snapshot schema.",
            "title": "Returns"
        },
        {
            "location": "/References/Plugin_Operations/#example_23",
            "text": "from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.upgrade.snapshot ( \"2019.12.15\" )  def   add_new_flag_to_snapshot ( old_snapshot ): \n   new_snapshot   =   dict ( old_snapshot ) \n   new_snapshot [ \"useNewFeature\" ]   =   False \n   return   new_snapshot",
            "title": "Example"
        },
        {
            "location": "/References/Schemas/",
            "text": "Schemas\n\u00b6\n\n\nAbout Schemas\n\u00b6\n\n\nAny time a plugin needs to store its own data, or needs to ask the user for data, the Delphix Engine needs to be told about the format of that data:\n\n\n\n\nWhat is the set of data needed and what should they be called?\n\n\nWhat is the type of each piece of data: Strings? Integers? Booleans?\n\n\n\n\nPlugins use \nschemas\n to describe the format of such data. Once a schema is defined, it is used in three ways\n\n\n\n\nIt tells the Delphix Engine how to store the data for later use.\n\n\nIt is used to autogenerate a custom user interface, and to validate user inputs.\n\n\nIt is used to \nautogenerate Python classes\n that can be used by plugin code to access and manipulate user input and stored data.\n\n\n\n\nThere are five plugin-customizable data formats:\n\n\n\n\n\n\n\n\nDelphix Object\n\n\nSchema\n\n\nAutogenerated Class\n\n\n\n\n\n\n\n\n\n\nRepository\n\n\nRepositoryDefinition\n\n\nRepositoryDefinition\n\n\n\n\n\n\nSource Config\n\n\nSourceConfigDefinition\n\n\nSourceConfigDefinition\n\n\n\n\n\n\nLinked Source\n\n\nLinkedSourceDefinition\n\n\nLinkedSourceDefinition\n\n\n\n\n\n\nVirtual Source\n\n\nVirtualSourceDefinition\n\n\nVirtualSourceDefinition\n\n\n\n\n\n\nSnapshot\n\n\nSnapshotDefinition\n\n\nSnapshotDefinition\n\n\n\n\n\n\n\n\nJSON Schemas\n\u00b6\n\n\nPlugins use JSON schemas for their custom datatypes. There are three main things to understand about them, which are explained just below:\n\n\n\n\nWhat is JSON?\n\n\nWhat is a JSON schema?\n\n\nHow has Delphix augmented JSON schemas?\n\n\n\n\nJSON\n\u00b6\n\n\nJSON stands for \"Javascript Object Notation\". JSON is a data-interchange format that is intended to be precise and also somewhat human-readable. Here are some simple examples of data in JSON format:\n\n\n\n\n\n\n\n\nJSON\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\"hello\"\n\n\nA string. Note the double quotes.\n\n\n\n\n\n\n17\n\n\nAn integer\n\n\n\n\n\n\ntrue\n\n\nA boolean\n\n\n\n\n\n\n{\"name\": \"Julie\", \"age\": 37}\n\n\nA JSON object with two fields, \nname\n (a string), and \nage\n (an integer). Objects are denoted with curly braces.\n\n\n\n\n\n\n[ true, false, true]\n\n\nA JSON array with three booleans. Arrays are denoted with square brackets.\n\n\n\n\n\n\n\n\nFor more details on JSON, please see \nhttps://www.json.org/\n.\n\n\nJSON Schemas\n\u00b6\n\n\nThe \"JSON schema\" format is built on top of JSON. This adds some special rules and keywords that are intended to facilitate the \ndescription\n of the format of data (whereas \"raw\" JSON is intended for storing data).\n\n\nHere is an example of a JSON schema that defines a (simplified) US address:\n\n\n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"required\"\n:\n \n[\n\"name\"\n,\n \n\"streetNumber\"\n,\n \n\"street\"\n,\n \n\"city\"\n,\n \n\"state\"\n,\n \n\"zip5\"\n],\n\n    \n\"additionalProperties\"\n:\n \nfalse\n,\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n        \n\"streetNumber\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n        \n\"street\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n        \n\"unit\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n        \n\"city\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n,\n \n\"pattern\"\n:\n \n\"^[A-Z][A-Za-z ]*$\"\n \n},\n\n        \n\"state\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n,\n \n\"pattern\"\n:\n \n\"^[A-Z]{2}$\"\n \n},\n\n        \n\"zip5\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n,\n \n\"pattern\"\n:\n \n\"^[0-9]{5}\"\n},\n\n        \n\"zipPlus4\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n,\n \n\"pattern\"\n:\n \n\"^[0-9]{4}\"\n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nNote that this is perfectly valid JSON data. It's a JSON object with four fields: \ntype\n (a JSON string), \nrequired\n (A JSON array), \nadditionalProperties\n (a JSON boolean), and \nproperties\n. \nproperties\n, in turn is a JSON object with with 8 fields, each of which is a JSON object, with its own properties, etc.\n\n\nBut, this isn't \njust\n a JSON object. This is a JSON schema. It uses special keywords like \ntype\n \nrequired\n, and \nadditionalProperties\n. These have specially-defined meanings in the context of JSON schemas.\n\n\nHere is a list of the special keywords used by the above schema. Note that this is only a small subset of JSON schema keywords.\n\n\n\n\n\n\n\n\nkeyword\n\n\ndescription\n\n\n\n\n\n\n\n\n\n\nadditionalProperties\n\n\nDetermines whether the schema allows properties that are not explicitly listed in the \nproperties\n specification. Must be a \ntrue\n or \nfalse\n.\n\n\n\n\n\n\npattern\n\n\nUsed with string types to specify a regular expression that the property must conform to.\n\n\n\n\n\n\nrequired\n\n\nA list of required properties. Properties not listed in this list are optional.\n\n\n\n\n\n\nstring\n\n\nUsed with \ntype\n to declare that a property must be a string.\n\n\n\n\n\n\ntype\n\n\nSpecifies a datatype. Common values are \nobject\n, \narray\n, \nnumber\n, \ninteger\n, \nboolean\n, and \nstring\n.\n\n\n\n\n\n\n\n\nSome points to note about the address schema above:\n\n\n\n\nBecause of the \nrequired\n list, all valid addresses must have fields called \nname\n, \nstreetNumber\n and so on.\n\n\nunit\n and \nzipPlus4\n do not appear in the \nrequired\n list, and therefore are optional.\n\n\nBecause of \nadditionalProperties\n being \nfalse\n, valid addresses cannot make up their own fields like \nnickname\n or \ndoorbellLocation\n.\n\n\nBecause of the \npattern\n, any \nstate\n field in a valid address must consist of exactly two capital letters.\n\n\nSimilarly, \ncity\n must only contain letters and spaces, and \nzip\n and \nzipPlus4\n must only contain digits.\n\n\nEach property has its own valid subschema that describes its own type definition.\n\n\n\n\nHere is a JSON object that conforms to the above schema:\n\n\n{\n\n  \n\"name\"\n:\n \n\"Delphix\"\n,\n\n  \n\"streetNumber\"\n:\n \n\"220\"\n,\n\n  \n\"street\"\n:\n \n\"Congress St.\"\n,\n\n  \n\"unit\"\n:\n \n\"200\"\n,\n\n  \n\"city\"\n:\n \n\"Boston\"\n,\n\n  \n\"state\"\n:\n \n\"MA\"\n,\n\n  \n\"zip\"\n:\n \n\"02210\"\n\n\n}\n\n\n\n\n\n\n\n\nInfo\n\n\nA common point of confusion is the distinction between a JSON schema and a JSON object that conforms to a schema. Remember, a schema describes the form of data. In our example, the schema \ndescribes\n what an address looks like. The address itself is not a schema.\n\n\n\n\nFor much more detail on JSON schemas, including which keywords are available, what they mean, and where you can use them, see \nhttps://json-schema.org/understanding-json-schema/\n.\n\n\nDelphix-specific Extensions to JSON Schema\n\u00b6\n\n\nThe JSON schema vocabulary is designed to be extensible for special uses, and Delphix has taken advantage of this to add some new Delphix-specific keywords.\n\n\nThe list below outlines each of these keywords, and provides minimal examples of how they might be used.\n\n\ndescription\n\u00b6\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nOptional\n\n\n\n\n\n\nWhere?\n\n\nIn any property subschema, at the same level as \ntype\n.\n\n\n\n\n\n\n\n\nThe \ndescription\n keyword can optionally appear on any property. If it does appear, it is used by the UI as explanatory text for the UI widget associated with the property. If it does not appear, then no explanatory text is shown.\n\n\nIn this example, the UI would show \"User-readable name for the provisioned database\" in small text under the widget.\n\n\n{\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n\n      \n\"type\"\n:\n \n\"string\"\n,\n\n      \n\"description\"\n:\n \n\"User-readable name for the provisioned database\"\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nidentityFields\n\u00b6\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nRequired (for repository and source config schemas only)\n\n\n\n\n\n\nWhere?\n\n\nAt the top level of a repository or source config schema, at the same level as \ntype\n and \nproperties\n.\n\n\n\n\n\n\n\n\nThe \nidentityFields\n is a list of property names that, together, serve as a unique identifier for a repository or source config.\n\n\nWhen a plugin's \nautomatic discovery\n code is called, it will return a list of repositories (or source configs). The Delphix Engine needs to be able to compare this new list with whatever repositories it already knows about.\n\n\nFor example, suppose the engine already knows about a single repository with data \n{\"dbname\": \"my_databsae\", \"path\": \"/var/db/db01\"}\n (note the misspelling!). And, then suppose that automatic discovery is re-run and it returns repository data \n{ \"dbname\": \"my_database\", \"path\": \"/var/db/db01\"}\n.\n\n\nWhat should the Delphix Engine do? Should it conclude that \"my_databsae\" has been deleted, and there is a completely new repository named \"my_database\"? Or, should it conclude that we still have the same old repository, but with an updated name?\n\n\nidentityFields\n is used to handle this. When the engine compares \"new\" data with \"old\" data, it concludes that they belong to the same repository if \nall\n of the identity fields match. If any of the identity fields do not match, then the \"new\" repository data is judged to represent a different repository than the old data.\n\n\nidentityFields\n is \nrequired\n for \nRepositoryDefinition\n and \nSourceConfigDefinition\n schemas, and may not be used in any other schemas.\n\n\nIn this example, we'll tell the Delphix Engine that \npath\n is the sole unique identifier.\n\n\n{\n\n  \n\"properties\"\n:\n \n{\n\n      \n\"dbname\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n},\n\n      \n\"path\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"path\"\n]\n\n\n}\n\n\n\n\n\n\nnameField\n\u00b6\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nRequired (for repository and source config schemas only)\n\n\n\n\n\n\nWhere?\n\n\nAt the top level of a repository or source config schema, at the same level as \ntype\n and \nproperties\n.\n\n\n\n\n\n\n\n\nThe \nnameField\n keyword specifies a single property that is to be used to name the object in the Delphix Engine. The property must be a string field. This keyword is used at the same level as \nproperties\n. It is \nrequired\n for \nRepositoryDefinition\n and \nSourceConfigDefinition\n schemas, and may not be used in any other schemas.\n\n\nIn this example, we will use the \npath\n property as the user-visible name.\n\n\n{\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"path\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n        \n\"port\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n    \n},\n\n    \n\"nameField\"\n:\n \n\"path\"\n\n\n}\n\n\n\n\n\n\nSo, if we have an repository object that looks like\n\n\n{\n\n  \n\"path\"\n:\n \n\"/usr/bin\"\n,\n\n  \n\"port\"\n:\n \n8800\n\n\n}\n\n\n\n\n\n\nthen the user will be able to refer to this object as \n/usr/bin\n.\n\n\nordering\n\u00b6\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nOptional\n\n\n\n\n\n\nWhere?\n\n\nAt the top level, same level as \ntype\n and \nproperties\n.\n\n\n\n\n\n\n\n\nThe \nordering\n keyword can be used to order the fields when the UI is autogenerated.\n\n\n{\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"path\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n        \n\"port\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n    \n},\n\n    \n\"ordering\"\n:\n \n[\n\"port\"\n,\n \n\"path\"\n]\n\n\n}\n\n\n\n\n\n\nIn the example above, the \nport\n will be the first field in the autogenerated UI wizard followed by \npath\n.\n\n\npassword\n\u00b6\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nOptional\n\n\n\n\n\n\nWhere?\n\n\nAs the value for the \nformat\n keyword in any string property's subschema.\n\n\n\n\n\n\n\n\nThe \npassword\n keyword can be used to specify the \nformat\n of a \nstring\n. (Note that \nformat\n is a standard keyword and is not Delphix-specific). If a property is tagged as a password, then the UI will never show the value on screen, and the value will be encrypted before being stored as described \nhere\n.\n\n\nIn this example, the \ndbPass\n field on any object will be treated as a password.\n\n\n{\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"dbPass\"\n:\n \n{\n\n      \n\"type\"\n:\n \n\"string\"\n,\n\n      \n\"format\"\n:\n \n\"password\"\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nprettyName\n\u00b6\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nOptional\n\n\n\n\n\n\nWhere?\n\n\nIn any property subschema, at the same level as \ntype\n.\n\n\n\n\n\n\n\n\nThe \nprettyName\n keyword can optionally appear on any property. If it does appear, it is used by the UI as a title for the UI widget associated with the property. If it does not appear, then the name of the property is used.\n\n\nIn this example, the user would see \"Name of Database\" on the UI, instead of just \"name\".\n\n\n{\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n\n      \n\"type\"\n:\n \n\"string\"\n,\n\n      \n\"prettyName\"\n:\n \n\"Name of Database\"\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nunixpath\n\u00b6\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nOptional\n\n\n\n\n\n\nWhere?\n\n\nAs the value for the \nformat\n keyword in any string property's subschema.\n\n\n\n\n\n\n\n\nThe \nunixpath\n keyword is used to specify the \nformat\n of a string. This will allow the Delphix Engine to verify and enforce that a particular field can be parsed as a valid Unix path.\n\n\n{\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"datapath\"\n:\n \n{\n\n      \n\"type\"\n:\n \n\"string\"\n,\n\n      \n\"format\"\n:\n \n\"unixpath\"\n\n    \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nreference\n\u00b6\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nOptional\n\n\n\n\n\n\nWhere?\n\n\nAs the value for the \nformat\n keyword in any string property's subschema.\n\n\n\n\n\n\n\n\nThe \nreference\n keyword is used to specify the \nformat\n of a string.\nThis will allow the plugin author to ask the user to select \nenvironments\n and \nenvironment users\n on the Delphix Engine.\n\n\n\"properties\"\n:\n \n{\n\n  \n\"env\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"string\"\n,\n\n    \n\"format\"\n:\n \n\"reference\"\n,\n\n    \n\"referenceType\"\n:\n \n\"UNIX_HOST_ENVIRONMENT\"\n\n  \n},\n\n  \n\"envUser\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"string\"\n,\n\n    \n\"format\"\n:\n \n\"reference\"\n,\n\n    \n\"referenceType\"\n:\n \n\"HOST_USER\"\n,\n\n    \n\"matches\u201d: \"\nenv\"\n\n  \n}\n\n\n}\n\n\n\n\n\n\nreferenceType\n\u00b6\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nOptional\n\n\n\n\n\n\nWhere?\n\n\nIn any property subschema of type \nstring\n and format \nreference\n, at the same level as type.\n\n\n\n\n\n\n\n\nThe \nreferenceType\n keyword is used to specify the \nreference\n type. Possible values:\n\n\n\n\nEnvironment\n: \nUNIX_HOST_ENVIRONMENT\n\n\nEnvironment User\n: \nHOST_USER\n\n\n\n\n\"properties\"\n:\n \n{\n\n  \n\"env\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"string\"\n,\n\n    \n\"format\"\n:\n \n\"reference\"\n,\n\n    \n\"referenceType\"\n:\n \n\"UNIX_HOST_ENVIRONMENT\"\n\n  \n},\n\n  \n\"envUser\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"string\"\n,\n\n    \n\"format\"\n:\n \n\"reference\"\n,\n\n    \n\"referenceType\"\n:\n \n\"HOST_USER\"\n,\n\n    \n\"matches\u201d: \"\nenv\"\n\n  \n}\n\n\n}\n\n\n\n\n\n\nmatches\n\u00b6\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\nRequired or Optional?\n\n\nOptional\n\n\n\n\n\n\nWhere?\n\n\nIn any property subschema of type \nstring\n and format \nreference\n, at the same level as type.\n\n\n\n\n\n\n\n\nThe \nmatches\n keyword is used to map an \nenvironment user\n to an \nenvironment\n by specifying the environment's property name.\n\n\n\"properties\"\n:\n \n{\n\n  \n\"env\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"string\"\n,\n\n    \n\"format\"\n:\n \n\"reference\"\n,\n\n    \n\"referenceType\"\n:\n \n\"UNIX_HOST_ENVIRONMENT\"\n\n  \n},\n\n  \n\"envUser\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"string\"\n,\n\n    \n\"format\"\n:\n \n\"reference\"\n,\n\n    \n\"referenceType\"\n:\n \n\"HOST_USER\"\n,\n\n    \n\"matches\u201d: \"\nenv\"\n\n  \n}\n\n\n}\n\n\n\n\n\n\nIn the example above, environment user \nenvUser\n maps to environment \nenv\n.\n\n\nJSON Schema Limitations\n\u00b6\n\n\nTo be able to autogenerate Python classes there are some restrictions to the JSON Schemas that are supported.\n\n\nGeneration Error\n\u00b6\n\n\nThere are some valid JSON schemas that will cause the property to not be generated in the autogenerated Python classes. Unfortunately the build command will silently fail so be sure to look at the generated classes and verify all the properties exist.\n\n\nMultiple types\n\u00b6\n\n\nFor the \ntype\n keyword, only a single type may be specified. Arrays of types are not supported.\n\n\n{\n\n  \n\"repositoryDefinition\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"additionalProperties\"\n:\n \n\"false\"\n,\n\n    \n\"properties\"\n:\n \n{\n\n      \n\"data\"\n:\n \n{\n\n        \n\"type\"\n:\n \n[\n\"integer\"\n,\n \n\"string\"\n]\n\n      \n}\n\n    \n},\n\n    \n\"nameField\"\n:\n \n\"data\"\n,\n\n    \n\"identityFields\"\n:\n \n[\n\"data\"\n]\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThe \ndata\n property will not even exist:\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nRepositoryDefinition\n\n\n\nrepository\n \n=\n \nRepositoryDefinition\n()\n\n\nrepository\n.\ndata\n \n=\n \n3\n\n\nprint\n(\nrepository\n)\n\n\n\n\n\n\nThis would print:\n\n\n{}\n\n\n\n\n\nCombining schemas\n\u00b6\n\n\nFor the following keywords, if they are specified the property will not exist in the class.\n\n anyOf\n\n allOf\n\n oneOf\n\n not\n\n\n{\n\n  \n\"repositoryDefinition\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"additionalProperties\"\n:\n \n\"false\"\n,\n\n    \n\"properties\"\n:\n \n{\n\n      \n\"any\"\n:\n \n{\n\n        \n\"anyOf\"\n:\n \n[\n\n          \n{\n\"type\"\n:\n \n\"integer\"\n,\n \n\"minimum\"\n:\n \n2\n},\n\n          \n{\n\"type\"\n:\n \n\"string\"\n,\n \n\"minLength\"\n:\n \n4\n}\n\n        \n]\n\n      \n},\n\n      \n\"one\"\n:\n \n{\n\n        \n\"oneOf\"\n:\n \n[\n\n          \n{\n\"type\"\n:\n \n\"integer\"\n,\n \n\"minimum\"\n:\n \n3\n},\n\n          \n{\n\"type\"\n:\n \n\"integer\"\n,\n \n\"maximum\"\n:\n \n5\n}\n\n        \n]\n\n      \n}\n\n    \n},\n\n    \n\"nameField\"\n:\n \n\"data\"\n,\n\n    \n\"identityFields\"\n:\n \n[\n\"data\"\n]\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThe \nany\n and \none\n properties would not exist:\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nRepositoryDefinition\n\n\n\nrepository\n \n=\n \nRepositoryDefinition\n()\n\n\nrepository\n.\nany\n \n=\n \n\"string\"\n\n\nrepository\n.\none\n \n=\n \n6\n\n\nprint\n(\nrepository\n)\n\n\n\n\n\n\nThis would print:\n\n\n{}\n\n\n\n\n\nObject Additional Properties\n\u00b6\n\n\nThe \nadditionalProperties\n keyword inside the object property can either be a boolean or a JSON schema. If it is a schema it needs to have the keyword \ntype\n. If the \nadditionalProperties\n is set to a JSON schema then the \nproperties\n keyword will be ignored. If the keyword is set to a boolean the behaviour will be the same regardless of if it was set to \ntrue\n or \nfalse\n.\n\n\n{\n\n  \n\"repositoryDefinition\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"additionalProperties\"\n:\n \n\"false\"\n,\n\n    \n\"properties\"\n:\n \n{\n\n      \n\"dataOne\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"object\"\n,\n\n        \n\"addtionalProperties\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n}\n\n      \n},\n\n      \n\"dataTwo\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"object\"\n,\n\n        \n\"addtionalProperties\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n},\n\n        \n\"properties\"\n:\n \n{\n\n          \n\"data\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n}\n\n        \n}\n\n      \n},\n\n      \n\"dataThree\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"object\"\n,\n\n        \n\"addtionalProperties\"\n:\n \n\"false\"\n,\n\n        \n\"properties\"\n:\n \n{\n\n          \n\"data\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n}\n\n        \n}\n\n      \n},\n\n      \n\"dataFour\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"object\"\n,\n\n        \n\"addtionalProperties\"\n:\n \n\"true\"\n,\n\n        \n\"properties\"\n:\n \n{\n\n          \n\"data\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n}\n\n        \n}\n\n      \n},\n\n      \n\"dataFive\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"object\"\n,\n\n        \n\"addtionalProperties\"\n:\n \n\"false\"\n,\n\n      \n},\n\n      \n\"dataSix\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"object\"\n,\n\n        \n\"addtionalProperties\"\n:\n \n\"true\"\n,\n\n      \n}\n\n    \n},\n\n    \n\"nameField\"\n:\n \n\"dataOne\"\n,\n\n    \n\"identityFields\"\n:\n \n[\n\"dataOne\"\n]\n\n  \n}\n\n\n}\n\n\n\n\n\n\nFrom the schema above, the properties \ndataOne\n and \ndataTwo\n, \ndataThree\n and \ndataFour\n, and \ndataFive\n and \ndataSix\n will have an identical validations. The first two will validate that the object passed in is a dict with key and value both \nstring\n type. The next two will create a new inner Python class called either \nOtherDefinitionDataThree\n or \nOtherDefinitionDataFour\n, they optomize for creating only one as they are identical. Inside that object will be one property \ndata\n. The last two properties will validate that the object passed in is a dict with the key as a \nstring\n type, and the value can be anything.\n\n\nValidation Keywords\n\u00b6\n\n\nIn general all property types are supported however some validation keywords will be ignored during the execution of the Python code. This means that if these keywords are used, no error would be raised within Python if the object violates the schema. Listed below are the keywords ignored for each type that wouldn't validate. Some have examples to be more clear.\n\n\nNumber / Integer\n\u00b6\n\n\n\n\nmultipleOf\n\n\n\n\n{\n\n  \n\"repositoryDefinition\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"additionalProperties\"\n:\n \n\"false\"\n,\n\n    \n\"properties\"\n:\n \n{\n\n      \n\"data\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"integer\"\n,\n\n        \n\"multipleOf\"\n:\n \n2\n\n      \n}\n\n    \n},\n\n    \n\"nameField\"\n:\n \n\"data\"\n,\n\n    \n\"identityFields\"\n:\n \n[\n\"data\"\n]\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThis would work even though it would fail the schema check:\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nRepositoryDefinition\n\n\n\nrepository\n \n=\n \nRepositoryDefinition\n()\n\n\nrepository\n.\ndata\n \n=\n \n3\n\n\n\n\n\n\nArrays / Tuples\n\u00b6\n\n\n\n\nadditionalItems\n\n\nminItems\n\n\nmaxItems\n\n\nuniqueItems\n\n\ncontains\n\n\nitems\n\n\nMust be a single type, not an array (tuples are not supported):\n\n\n\n\n\n\n\n\n{\n\n  \n\"repositoryDefinition\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"additionalProperties\"\n:\n \n\"false\"\n,\n\n    \n\"properties\"\n:\n \n{\n\n      \n\"data\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"array\"\n,\n\n        \n\"items\"\n:\n \n[\n\n          \n{\n\"type\"\n:\n \n\"number\"\n},\n\n          \n{\n\"type\"\n:\n \n\"string\"\n},\n\n          \n{\n\"type\"\n:\n \n\"boolean\"\n}\n\n        \n]\n\n      \n}\n\n    \n},\n\n    \n\"nameField\"\n:\n \n\"data\"\n,\n\n    \n\"identityFields\"\n:\n \n[\n\"data\"\n]\n\n  \n}\n\n\n}\n\n\n\n\n\n\nThis would work even though it would fail the schema check:\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nRepositoryDefinition\n\n\n\nrepository\n \n=\n \nRepositoryDefinition\n()\n\n\nrepository\n.\ndata\n \n=\n \n[\n\"string\"\n,\n \nFalse\n,\n \n3\n]\n\n\n\n\n\n\nObjects\n\u00b6\n\n\n\n\nminProperties\n\n\nmaxProperties\n\n\npatternProperties\n\n\ndependencies\n\n\npropertyNames\n\n\n\n\nEnumerated values\n\u00b6\n\n\nIf the \nenum\n keyword is used within a subobject, \ntype\n has to be \nstring\n.\n\n\n{\n\n  \n\"repositoryDefinition\"\n:\n \n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"additionalProperties\"\n:\n \n\"false\"\n,\n\n    \n\"properties\"\n:\n \n{\n\n      \n\"stringData\"\n:\n \n{\n\n        \n\"enum\"\n:\n \n[\n\"A\"\n,\n \n\"B\"\n,\n \n\"C\"\n]\n\n      \n},\n\n      \n\"arrayData\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"array\"\n,\n\n        \n\"items\"\n:\n \n{\n\n          \n\"enum\"\n:\n \n[\n\"DO\"\n,\n \n\"RE\"\n,\n \n\"MI\"\n]\n\n        \n}\n\n      \n},\n\n      \n\"objectData\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"object\"\n,\n\n        \n\"additionalProperties\"\n:\n \n{\n\n          \n\"enum\"\n:\n \n[\n\"ONE\"\n,\n \n\"TWO\"\n,\n \n\"THREE\"\n]\n\n        \n}\n\n      \n},\n\n      \n\"definedObjectData\"\n:\n \n{\n\n        \n\"type\"\n:\n \n\"object\"\n,\n\n        \n\"properties\"\n:\n \n{\n\n          \n\"objectStringData\"\n:\n \n{\n\n            \n\"enum\"\n:\n \n[\n\"o.A\"\n,\n \n\"o.B\"\n,\n \n\"o.C\"\n]\n\n          \n},\n\n        \n},\n\n        \n\"additionalProperties\"\n:\n \n\"false\"\n\n      \n}\n\n    \n},\n\n    \n\"nameField\"\n:\n \n\"stringData\"\n,\n\n    \n\"identityFields\"\n:\n \n[\n\"stringData\"\n]\n\n  \n}\n\n\n}\n\n\n\n\n\n\nIn the above example there are four properties: \nstringData\n, \narrayData\n, \nobjectData\n, and \ndefinedObjectData\n. Validation works for stringData but are skipped for the other three. In fact the definedObjectData which with properties would usually create a separte Python class does not at all.\nThis means the following code would work even though it would fail the schema check:\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nRepositoryDefinition\n\n\n\nrepository\n \n=\n \nRepositoryDefinition\n()\n\n\nrepository\n.\narray_data\n \n=\n \n[\n10\n,\n \n11\n,\n \n12\n]\n\n\nrepository\n.\nobject_data\n \n=\n \n{\n\"key\"\n:\n \n1\n}\n\n\nrepository\n.\ndefined_object_data\n \n=\n \n{\n\"key\"\n:\n \n2\n}\n\n\n\n\n\n\nAnd this code would actually fail with a \nGeneratedClassesError\n during the Python execution saying \nInvalid enum value D for 'string_data', must be one of [A, B, C] if defined.\n:\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nRepositoryDefinition\n\n\n\nrepository\n \n=\n \nRepositoryDefinition\n()\n\n\nrepository\n.\nstring_data\n \n=\n \n\"D\"",
            "title": "Schemas"
        },
        {
            "location": "/References/Schemas/#schemas",
            "text": "",
            "title": "Schemas"
        },
        {
            "location": "/References/Schemas/#about-schemas",
            "text": "Any time a plugin needs to store its own data, or needs to ask the user for data, the Delphix Engine needs to be told about the format of that data:   What is the set of data needed and what should they be called?  What is the type of each piece of data: Strings? Integers? Booleans?   Plugins use  schemas  to describe the format of such data. Once a schema is defined, it is used in three ways   It tells the Delphix Engine how to store the data for later use.  It is used to autogenerate a custom user interface, and to validate user inputs.  It is used to  autogenerate Python classes  that can be used by plugin code to access and manipulate user input and stored data.   There are five plugin-customizable data formats:     Delphix Object  Schema  Autogenerated Class      Repository  RepositoryDefinition  RepositoryDefinition    Source Config  SourceConfigDefinition  SourceConfigDefinition    Linked Source  LinkedSourceDefinition  LinkedSourceDefinition    Virtual Source  VirtualSourceDefinition  VirtualSourceDefinition    Snapshot  SnapshotDefinition  SnapshotDefinition",
            "title": "About Schemas"
        },
        {
            "location": "/References/Schemas/#json-schemas",
            "text": "Plugins use JSON schemas for their custom datatypes. There are three main things to understand about them, which are explained just below:   What is JSON?  What is a JSON schema?  How has Delphix augmented JSON schemas?",
            "title": "JSON Schemas"
        },
        {
            "location": "/References/Schemas/#json",
            "text": "JSON stands for \"Javascript Object Notation\". JSON is a data-interchange format that is intended to be precise and also somewhat human-readable. Here are some simple examples of data in JSON format:     JSON  Description      \"hello\"  A string. Note the double quotes.    17  An integer    true  A boolean    {\"name\": \"Julie\", \"age\": 37}  A JSON object with two fields,  name  (a string), and  age  (an integer). Objects are denoted with curly braces.    [ true, false, true]  A JSON array with three booleans. Arrays are denoted with square brackets.     For more details on JSON, please see  https://www.json.org/ .",
            "title": "JSON"
        },
        {
            "location": "/References/Schemas/#json-schemas_1",
            "text": "The \"JSON schema\" format is built on top of JSON. This adds some special rules and keywords that are intended to facilitate the  description  of the format of data (whereas \"raw\" JSON is intended for storing data).  Here is an example of a JSON schema that defines a (simplified) US address:  { \n     \"type\" :   \"object\" , \n     \"required\" :   [ \"name\" ,   \"streetNumber\" ,   \"street\" ,   \"city\" ,   \"state\" ,   \"zip5\" ], \n     \"additionalProperties\" :   false , \n     \"properties\" :   { \n         \"name\" :   {   \"type\" :   \"string\"   }, \n         \"streetNumber\" :   {   \"type\" :   \"string\"   }, \n         \"street\" :   {   \"type\" :   \"string\"   }, \n         \"unit\" :   {   \"type\" :   \"string\"   }, \n         \"city\" :   {   \"type\" :   \"string\" ,   \"pattern\" :   \"^[A-Z][A-Za-z ]*$\"   }, \n         \"state\" :   {   \"type\" :   \"string\" ,   \"pattern\" :   \"^[A-Z]{2}$\"   }, \n         \"zip5\" :   {   \"type\" :   \"string\" ,   \"pattern\" :   \"^[0-9]{5}\" }, \n         \"zipPlus4\" :   {   \"type\" :   \"string\" ,   \"pattern\" :   \"^[0-9]{4}\" } \n     }  }   Note that this is perfectly valid JSON data. It's a JSON object with four fields:  type  (a JSON string),  required  (A JSON array),  additionalProperties  (a JSON boolean), and  properties .  properties , in turn is a JSON object with with 8 fields, each of which is a JSON object, with its own properties, etc.  But, this isn't  just  a JSON object. This is a JSON schema. It uses special keywords like  type   required , and  additionalProperties . These have specially-defined meanings in the context of JSON schemas.  Here is a list of the special keywords used by the above schema. Note that this is only a small subset of JSON schema keywords.     keyword  description      additionalProperties  Determines whether the schema allows properties that are not explicitly listed in the  properties  specification. Must be a  true  or  false .    pattern  Used with string types to specify a regular expression that the property must conform to.    required  A list of required properties. Properties not listed in this list are optional.    string  Used with  type  to declare that a property must be a string.    type  Specifies a datatype. Common values are  object ,  array ,  number ,  integer ,  boolean , and  string .     Some points to note about the address schema above:   Because of the  required  list, all valid addresses must have fields called  name ,  streetNumber  and so on.  unit  and  zipPlus4  do not appear in the  required  list, and therefore are optional.  Because of  additionalProperties  being  false , valid addresses cannot make up their own fields like  nickname  or  doorbellLocation .  Because of the  pattern , any  state  field in a valid address must consist of exactly two capital letters.  Similarly,  city  must only contain letters and spaces, and  zip  and  zipPlus4  must only contain digits.  Each property has its own valid subschema that describes its own type definition.   Here is a JSON object that conforms to the above schema:  { \n   \"name\" :   \"Delphix\" , \n   \"streetNumber\" :   \"220\" , \n   \"street\" :   \"Congress St.\" , \n   \"unit\" :   \"200\" , \n   \"city\" :   \"Boston\" , \n   \"state\" :   \"MA\" , \n   \"zip\" :   \"02210\"  }    Info  A common point of confusion is the distinction between a JSON schema and a JSON object that conforms to a schema. Remember, a schema describes the form of data. In our example, the schema  describes  what an address looks like. The address itself is not a schema.   For much more detail on JSON schemas, including which keywords are available, what they mean, and where you can use them, see  https://json-schema.org/understanding-json-schema/ .",
            "title": "JSON Schemas"
        },
        {
            "location": "/References/Schemas/#delphix-specific-extensions-to-json-schema",
            "text": "The JSON schema vocabulary is designed to be extensible for special uses, and Delphix has taken advantage of this to add some new Delphix-specific keywords.  The list below outlines each of these keywords, and provides minimal examples of how they might be used.",
            "title": "Delphix-specific Extensions to JSON Schema"
        },
        {
            "location": "/References/Schemas/#description",
            "text": "Summary       Required or Optional?  Optional    Where?  In any property subschema, at the same level as  type .     The  description  keyword can optionally appear on any property. If it does appear, it is used by the UI as explanatory text for the UI widget associated with the property. If it does not appear, then no explanatory text is shown.  In this example, the UI would show \"User-readable name for the provisioned database\" in small text under the widget.  { \n   \"properties\" :   { \n     \"name\" :   { \n       \"type\" :   \"string\" , \n       \"description\" :   \"User-readable name for the provisioned database\" \n     } \n   }  }",
            "title": "description"
        },
        {
            "location": "/References/Schemas/#identityfields",
            "text": "Summary       Required or Optional?  Required (for repository and source config schemas only)    Where?  At the top level of a repository or source config schema, at the same level as  type  and  properties .     The  identityFields  is a list of property names that, together, serve as a unique identifier for a repository or source config.  When a plugin's  automatic discovery  code is called, it will return a list of repositories (or source configs). The Delphix Engine needs to be able to compare this new list with whatever repositories it already knows about.  For example, suppose the engine already knows about a single repository with data  {\"dbname\": \"my_databsae\", \"path\": \"/var/db/db01\"}  (note the misspelling!). And, then suppose that automatic discovery is re-run and it returns repository data  { \"dbname\": \"my_database\", \"path\": \"/var/db/db01\"} .  What should the Delphix Engine do? Should it conclude that \"my_databsae\" has been deleted, and there is a completely new repository named \"my_database\"? Or, should it conclude that we still have the same old repository, but with an updated name?  identityFields  is used to handle this. When the engine compares \"new\" data with \"old\" data, it concludes that they belong to the same repository if  all  of the identity fields match. If any of the identity fields do not match, then the \"new\" repository data is judged to represent a different repository than the old data.  identityFields  is  required  for  RepositoryDefinition  and  SourceConfigDefinition  schemas, and may not be used in any other schemas.  In this example, we'll tell the Delphix Engine that  path  is the sole unique identifier.  { \n   \"properties\" :   { \n       \"dbname\" :   { \"type\" :   \"string\" }, \n       \"path\" :   { \"type\" :   \"string\" } \n   }, \n   \"identityFields\" :   [ \"path\" ]  }",
            "title": "identityFields"
        },
        {
            "location": "/References/Schemas/#namefield",
            "text": "Summary       Required or Optional?  Required (for repository and source config schemas only)    Where?  At the top level of a repository or source config schema, at the same level as  type  and  properties .     The  nameField  keyword specifies a single property that is to be used to name the object in the Delphix Engine. The property must be a string field. This keyword is used at the same level as  properties . It is  required  for  RepositoryDefinition  and  SourceConfigDefinition  schemas, and may not be used in any other schemas.  In this example, we will use the  path  property as the user-visible name.  { \n     \"properties\" :   { \n         \"path\" :   {   \"type\" :   \"string\"   }, \n         \"port\" :   {   \"type\" :   \"integer\"   } \n     }, \n     \"nameField\" :   \"path\"  }   So, if we have an repository object that looks like  { \n   \"path\" :   \"/usr/bin\" , \n   \"port\" :   8800  }   then the user will be able to refer to this object as  /usr/bin .",
            "title": "nameField"
        },
        {
            "location": "/References/Schemas/#ordering",
            "text": "Summary       Required or Optional?  Optional    Where?  At the top level, same level as  type  and  properties .     The  ordering  keyword can be used to order the fields when the UI is autogenerated.  { \n     \"properties\" :   { \n         \"path\" :   {   \"type\" :   \"string\"   }, \n         \"port\" :   {   \"type\" :   \"integer\"   } \n     }, \n     \"ordering\" :   [ \"port\" ,   \"path\" ]  }   In the example above, the  port  will be the first field in the autogenerated UI wizard followed by  path .",
            "title": "ordering"
        },
        {
            "location": "/References/Schemas/#password",
            "text": "Summary       Required or Optional?  Optional    Where?  As the value for the  format  keyword in any string property's subschema.     The  password  keyword can be used to specify the  format  of a  string . (Note that  format  is a standard keyword and is not Delphix-specific). If a property is tagged as a password, then the UI will never show the value on screen, and the value will be encrypted before being stored as described  here .  In this example, the  dbPass  field on any object will be treated as a password.  { \n   \"properties\" :   { \n     \"dbPass\" :   { \n       \"type\" :   \"string\" , \n       \"format\" :   \"password\" \n     } \n   }  }",
            "title": "password"
        },
        {
            "location": "/References/Schemas/#prettyname",
            "text": "Summary       Required or Optional?  Optional    Where?  In any property subschema, at the same level as  type .     The  prettyName  keyword can optionally appear on any property. If it does appear, it is used by the UI as a title for the UI widget associated with the property. If it does not appear, then the name of the property is used.  In this example, the user would see \"Name of Database\" on the UI, instead of just \"name\".  { \n   \"properties\" :   { \n     \"name\" :   { \n       \"type\" :   \"string\" , \n       \"prettyName\" :   \"Name of Database\" \n     } \n   }  }",
            "title": "prettyName"
        },
        {
            "location": "/References/Schemas/#unixpath",
            "text": "Summary       Required or Optional?  Optional    Where?  As the value for the  format  keyword in any string property's subschema.     The  unixpath  keyword is used to specify the  format  of a string. This will allow the Delphix Engine to verify and enforce that a particular field can be parsed as a valid Unix path.  { \n   \"properties\" :   { \n     \"datapath\" :   { \n       \"type\" :   \"string\" , \n       \"format\" :   \"unixpath\" \n     } \n   }  }",
            "title": "unixpath"
        },
        {
            "location": "/References/Schemas/#reference",
            "text": "Summary       Required or Optional?  Optional    Where?  As the value for the  format  keyword in any string property's subschema.     The  reference  keyword is used to specify the  format  of a string.\nThis will allow the plugin author to ask the user to select  environments  and  environment users  on the Delphix Engine.  \"properties\" :   { \n   \"env\" :   { \n     \"type\" :   \"string\" , \n     \"format\" :   \"reference\" , \n     \"referenceType\" :   \"UNIX_HOST_ENVIRONMENT\" \n   }, \n   \"envUser\" :   { \n     \"type\" :   \"string\" , \n     \"format\" :   \"reference\" , \n     \"referenceType\" :   \"HOST_USER\" , \n     \"matches\u201d: \" env\" \n   }  }",
            "title": "reference"
        },
        {
            "location": "/References/Schemas/#referencetype",
            "text": "Summary       Required or Optional?  Optional    Where?  In any property subschema of type  string  and format  reference , at the same level as type.     The  referenceType  keyword is used to specify the  reference  type. Possible values:   Environment :  UNIX_HOST_ENVIRONMENT  Environment User :  HOST_USER   \"properties\" :   { \n   \"env\" :   { \n     \"type\" :   \"string\" , \n     \"format\" :   \"reference\" , \n     \"referenceType\" :   \"UNIX_HOST_ENVIRONMENT\" \n   }, \n   \"envUser\" :   { \n     \"type\" :   \"string\" , \n     \"format\" :   \"reference\" , \n     \"referenceType\" :   \"HOST_USER\" , \n     \"matches\u201d: \" env\" \n   }  }",
            "title": "referenceType"
        },
        {
            "location": "/References/Schemas/#matches",
            "text": "Summary       Required or Optional?  Optional    Where?  In any property subschema of type  string  and format  reference , at the same level as type.     The  matches  keyword is used to map an  environment user  to an  environment  by specifying the environment's property name.  \"properties\" :   { \n   \"env\" :   { \n     \"type\" :   \"string\" , \n     \"format\" :   \"reference\" , \n     \"referenceType\" :   \"UNIX_HOST_ENVIRONMENT\" \n   }, \n   \"envUser\" :   { \n     \"type\" :   \"string\" , \n     \"format\" :   \"reference\" , \n     \"referenceType\" :   \"HOST_USER\" , \n     \"matches\u201d: \" env\" \n   }  }   In the example above, environment user  envUser  maps to environment  env .",
            "title": "matches"
        },
        {
            "location": "/References/Schemas/#json-schema-limitations",
            "text": "To be able to autogenerate Python classes there are some restrictions to the JSON Schemas that are supported.",
            "title": "JSON Schema Limitations"
        },
        {
            "location": "/References/Schemas/#generation-error",
            "text": "There are some valid JSON schemas that will cause the property to not be generated in the autogenerated Python classes. Unfortunately the build command will silently fail so be sure to look at the generated classes and verify all the properties exist.",
            "title": "Generation Error"
        },
        {
            "location": "/References/Schemas/#multiple-types",
            "text": "For the  type  keyword, only a single type may be specified. Arrays of types are not supported.  { \n   \"repositoryDefinition\" :   { \n     \"type\" :   \"object\" , \n     \"additionalProperties\" :   \"false\" , \n     \"properties\" :   { \n       \"data\" :   { \n         \"type\" :   [ \"integer\" ,   \"string\" ] \n       } \n     }, \n     \"nameField\" :   \"data\" , \n     \"identityFields\" :   [ \"data\" ] \n   }  }   The  data  property will not even exist:  from   generated.defintions   import   RepositoryDefinition  repository   =   RepositoryDefinition ()  repository . data   =   3  print ( repository )   This would print:  {}",
            "title": "Multiple types"
        },
        {
            "location": "/References/Schemas/#combining-schemas",
            "text": "For the following keywords, if they are specified the property will not exist in the class.  anyOf  allOf  oneOf  not  { \n   \"repositoryDefinition\" :   { \n     \"type\" :   \"object\" , \n     \"additionalProperties\" :   \"false\" , \n     \"properties\" :   { \n       \"any\" :   { \n         \"anyOf\" :   [ \n           { \"type\" :   \"integer\" ,   \"minimum\" :   2 }, \n           { \"type\" :   \"string\" ,   \"minLength\" :   4 } \n         ] \n       }, \n       \"one\" :   { \n         \"oneOf\" :   [ \n           { \"type\" :   \"integer\" ,   \"minimum\" :   3 }, \n           { \"type\" :   \"integer\" ,   \"maximum\" :   5 } \n         ] \n       } \n     }, \n     \"nameField\" :   \"data\" , \n     \"identityFields\" :   [ \"data\" ] \n   }  }   The  any  and  one  properties would not exist:  from   generated.defintions   import   RepositoryDefinition  repository   =   RepositoryDefinition ()  repository . any   =   \"string\"  repository . one   =   6  print ( repository )   This would print:  {}",
            "title": "Combining schemas"
        },
        {
            "location": "/References/Schemas/#object-additional-properties",
            "text": "The  additionalProperties  keyword inside the object property can either be a boolean or a JSON schema. If it is a schema it needs to have the keyword  type . If the  additionalProperties  is set to a JSON schema then the  properties  keyword will be ignored. If the keyword is set to a boolean the behaviour will be the same regardless of if it was set to  true  or  false .  { \n   \"repositoryDefinition\" :   { \n     \"type\" :   \"object\" , \n     \"additionalProperties\" :   \"false\" , \n     \"properties\" :   { \n       \"dataOne\" :   { \n         \"type\" :   \"object\" , \n         \"addtionalProperties\" :   { \"type\" :   \"string\" } \n       }, \n       \"dataTwo\" :   { \n         \"type\" :   \"object\" , \n         \"addtionalProperties\" :   { \"type\" :   \"string\" }, \n         \"properties\" :   { \n           \"data\" :   { \"type\" :   \"string\" } \n         } \n       }, \n       \"dataThree\" :   { \n         \"type\" :   \"object\" , \n         \"addtionalProperties\" :   \"false\" , \n         \"properties\" :   { \n           \"data\" :   { \"type\" :   \"string\" } \n         } \n       }, \n       \"dataFour\" :   { \n         \"type\" :   \"object\" , \n         \"addtionalProperties\" :   \"true\" , \n         \"properties\" :   { \n           \"data\" :   { \"type\" :   \"string\" } \n         } \n       }, \n       \"dataFive\" :   { \n         \"type\" :   \"object\" , \n         \"addtionalProperties\" :   \"false\" , \n       }, \n       \"dataSix\" :   { \n         \"type\" :   \"object\" , \n         \"addtionalProperties\" :   \"true\" , \n       } \n     }, \n     \"nameField\" :   \"dataOne\" , \n     \"identityFields\" :   [ \"dataOne\" ] \n   }  }   From the schema above, the properties  dataOne  and  dataTwo ,  dataThree  and  dataFour , and  dataFive  and  dataSix  will have an identical validations. The first two will validate that the object passed in is a dict with key and value both  string  type. The next two will create a new inner Python class called either  OtherDefinitionDataThree  or  OtherDefinitionDataFour , they optomize for creating only one as they are identical. Inside that object will be one property  data . The last two properties will validate that the object passed in is a dict with the key as a  string  type, and the value can be anything.",
            "title": "Object Additional Properties"
        },
        {
            "location": "/References/Schemas/#validation-keywords",
            "text": "In general all property types are supported however some validation keywords will be ignored during the execution of the Python code. This means that if these keywords are used, no error would be raised within Python if the object violates the schema. Listed below are the keywords ignored for each type that wouldn't validate. Some have examples to be more clear.",
            "title": "Validation Keywords"
        },
        {
            "location": "/References/Schemas/#number-integer",
            "text": "multipleOf   { \n   \"repositoryDefinition\" :   { \n     \"type\" :   \"object\" , \n     \"additionalProperties\" :   \"false\" , \n     \"properties\" :   { \n       \"data\" :   { \n         \"type\" :   \"integer\" , \n         \"multipleOf\" :   2 \n       } \n     }, \n     \"nameField\" :   \"data\" , \n     \"identityFields\" :   [ \"data\" ] \n   }  }   This would work even though it would fail the schema check:  from   generated.defintions   import   RepositoryDefinition  repository   =   RepositoryDefinition ()  repository . data   =   3",
            "title": "Number / Integer"
        },
        {
            "location": "/References/Schemas/#arrays-tuples",
            "text": "additionalItems  minItems  maxItems  uniqueItems  contains  items  Must be a single type, not an array (tuples are not supported):     { \n   \"repositoryDefinition\" :   { \n     \"type\" :   \"object\" , \n     \"additionalProperties\" :   \"false\" , \n     \"properties\" :   { \n       \"data\" :   { \n         \"type\" :   \"array\" , \n         \"items\" :   [ \n           { \"type\" :   \"number\" }, \n           { \"type\" :   \"string\" }, \n           { \"type\" :   \"boolean\" } \n         ] \n       } \n     }, \n     \"nameField\" :   \"data\" , \n     \"identityFields\" :   [ \"data\" ] \n   }  }   This would work even though it would fail the schema check:  from   generated.defintions   import   RepositoryDefinition  repository   =   RepositoryDefinition ()  repository . data   =   [ \"string\" ,   False ,   3 ]",
            "title": "Arrays / Tuples"
        },
        {
            "location": "/References/Schemas/#objects",
            "text": "minProperties  maxProperties  patternProperties  dependencies  propertyNames",
            "title": "Objects"
        },
        {
            "location": "/References/Schemas/#enumerated-values",
            "text": "If the  enum  keyword is used within a subobject,  type  has to be  string .  { \n   \"repositoryDefinition\" :   { \n     \"type\" :   \"object\" , \n     \"additionalProperties\" :   \"false\" , \n     \"properties\" :   { \n       \"stringData\" :   { \n         \"enum\" :   [ \"A\" ,   \"B\" ,   \"C\" ] \n       }, \n       \"arrayData\" :   { \n         \"type\" :   \"array\" , \n         \"items\" :   { \n           \"enum\" :   [ \"DO\" ,   \"RE\" ,   \"MI\" ] \n         } \n       }, \n       \"objectData\" :   { \n         \"type\" :   \"object\" , \n         \"additionalProperties\" :   { \n           \"enum\" :   [ \"ONE\" ,   \"TWO\" ,   \"THREE\" ] \n         } \n       }, \n       \"definedObjectData\" :   { \n         \"type\" :   \"object\" , \n         \"properties\" :   { \n           \"objectStringData\" :   { \n             \"enum\" :   [ \"o.A\" ,   \"o.B\" ,   \"o.C\" ] \n           }, \n         }, \n         \"additionalProperties\" :   \"false\" \n       } \n     }, \n     \"nameField\" :   \"stringData\" , \n     \"identityFields\" :   [ \"stringData\" ] \n   }  }   In the above example there are four properties:  stringData ,  arrayData ,  objectData , and  definedObjectData . Validation works for stringData but are skipped for the other three. In fact the definedObjectData which with properties would usually create a separte Python class does not at all.\nThis means the following code would work even though it would fail the schema check:  from   generated.defintions   import   RepositoryDefinition  repository   =   RepositoryDefinition ()  repository . array_data   =   [ 10 ,   11 ,   12 ]  repository . object_data   =   { \"key\" :   1 }  repository . defined_object_data   =   { \"key\" :   2 }   And this code would actually fail with a  GeneratedClassesError  during the Python execution saying  Invalid enum value D for 'string_data', must be one of [A, B, C] if defined. :  from   generated.defintions   import   RepositoryDefinition  repository   =   RepositoryDefinition ()  repository . string_data   =   \"D\"",
            "title": "Enumerated values"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/",
            "text": "Schemas and Autogenerated Classes\n\u00b6\n\n\nPlugin operations\n will sometimes need to work with data in these custom formats. For example, the \nconfigure\n operation will accept snapshot data as an input, and must produce source config data as an output.\n\n\nTo enable this, Python classes are generated from the snapshot schema. The aforementioned inputs and outputs are instances of these autogenerated classes.\n\n\n\n\nInfo\n\n\nAutogenerated Python code will use \nlower_case_with_underscores\n as attribute names as per Python variable naming conventions.\nThat is, if we were to use \nmountLocation\n as the schema property name, it would be called\n\nmount_location\n in the generated Python code.\n\n\n\n\n\n\nInfo\n\n\nNote that, wherever they can, these generated Python classes will enforce the constraints made by the schema. For example, if a property is listed as \nrequired\n in the schema, then every Python object will be required to always have this property. This implies that all \nrequired\n fields must be given values when the object is constructed. For various examples of this, see the examples below.\n\n\n\n\nRepositoryDefinition\n\u00b6\n\n\nDefines properties used to identify a \nRepository\n.\n\n\nRepositoryDefinition Schema\n\u00b6\n\n\nThe plugin must also decide on a \nname\n field and a set of \nidentityFields\n to display and uniquely identify the \nrepository\n.\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"required\"\n:\n \n[\n\"name\"\n,\n \n\"path\"\n],\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"path\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"name\"\n,\n \n\"path\"\n],\n\n  \n\"nameField\"\n:\n \n\"name\"\n\n\n}\n\n\n\n\n\n\nRepositoryDefinition Class\n\u00b6\n\n\nAutogenerated based on the \nRepositoryDefinition Schema\n.\n\n\nclass\n \nRepositoryDefinition\n:\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nname\n,\n \npath\n):\n\n    \nself\n.\n_inner_dict\n \n=\n \n{\n\"name\"\n:\n \nname\n,\n \n\"path\"\n:\n \npath\n}\n\n\n\n\n\n\n\n\nTo use the class:\n\n\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nRepositoryDefinition\n\n\n\n# Since both properties are required, they must be specified when constructing the object\n\n\nrepository\n \n=\n \nRepositoryDefinition\n(\nname\n=\n\"name\"\n,\n \npath\n=\n\"/some/path\"\n)\n\n\n\n\n\n\nSourceConfigDefinition\n\u00b6\n\n\nDefines properties used to identify a \nSource Config\n.\n\n\nSourceConfigDefinition Schema\n\u00b6\n\n\nThe plugin must also decide on a \nname\n field and a set of \nidentityFields\n to display and uniquely identify the \nsource config\n.\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"required\"\n:\n \n[\n\"name\"\n],\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"path\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n}\n\n  \n},\n\n  \n\"identityFields\"\n:\n \n[\n\"name\"\n],\n\n  \n\"nameField\"\n:\n \n\"name\"\n\n\n}\n\n\n\n\n\n\nSourceConfigDefinition Class\n\u00b6\n\n\nAutogenerated based on the \nSourceConfigDefinition Schema\n.\n\n\nclass\n \nSourceConfigDefinition\n:\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nname\n,\n \npath\n):\n\n    \nself\n.\n_inner_dict\n \n=\n \n{\n\"name\"\n:\n \nname\n,\n \n\"path\"\n:\n \npath\n}\n\n\n\n\n\n\n\n\nTo use the class:\n\n\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nSourceConfigDefinition\n\n\n\n# A source config that only defines the required \"name\" property.\n\n\nsource_config1\n \n=\n \nSourceConfigDefinition\n(\nname\n=\n\"sc1\"\n)\n\n\n\n# A Source config that defines both \"name\" and \"path\".\n\n\nsource_config2\n \n=\n \nSourceConfigDefinition\n(\nname\n=\n\"sc2\"\n,\n \npath\n=\n\"/some/path\"\n)\n\n\n\n# Setting the optional \"path\" property after construction\n\n\nsource_config3\n \n=\n \nSourceConfigDefinition\n(\nname\n=\n\"sc3\"\n)\n\n\ninstall_path\n \n=\n \nfind_install_path\n()\n\n\nsource_config3\n.\npath\n \n=\n \ninstall_path\n\n\n\n\n\n\nLinkedSourceDefinition\n\u00b6\n\n\nDefines properties used to identify \nlinked sources\n.\n\n\nLinkedSourceDefinition Schema\n\u00b6\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"required\"\n:\n \n[\n\"name\"\n,\n \n\"port\"\n],\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"port\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nLinkedSourceDefinition Class\n\u00b6\n\n\nAutogenerated based on the \nLinkedSourceDefinition Schema\n.\n\n\nclass\n \nLinkedSourceDefinition\n:\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nname\n,\n \nport\n):\n\n    \nself\n.\n_inner_dict\n \n=\n \n{\n\"name\"\n:\n \nname\n,\n \n\"port\"\n:\n \nport\n}\n\n\n\n\n\n\n\n\nTo use the class:\n\n\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nLinkedSourceDefinition\n\n\n\nsource\n \n=\n \nLinkedSourceDefinition\n(\nname\n=\n\"name\"\n,\n \nport\n=\n1000\n)\n\n\n\n# Retrieve the properties from the object and log them\n\n\nname\n \n=\n \nsource\n.\nname\n\n\nport\n \n=\n \nsource\n.\nport\n\n\nlogger\n.\ndebug\n(\n\"Creating source \n\\\"\n{}\n\\\"\n with port {}\"\n.\nformat\n(\nname\n,\n \nport\n))\n\n\n\n\n\n\nVirtualSourceDefinition\n\u00b6\n\n\nDefines properties used to identify \nvirtual sources\n.\n\n\nVirtualSourceDefinition Schema\n\u00b6\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"required\"\n:\n \n[\n\"name\"\n,\n \n\"port\"\n],\n\n  \n\"additionalProperties\"\n:\n \nfalse\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"name\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"port\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nVirtualSourceDefinition Class\n\u00b6\n\n\nAutogenerated based on the \nVirtualSourceDefinition Schema\n.\n\n\nclass\n \nVirtualSourceDefinition\n:\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nname\n,\n \nport\n):\n\n    \nself\n.\n_inner_dict\n \n=\n \n{\n\"name\"\n:\n \nname\n,\n \n\"port\"\n:\n \nport\n}\n\n\n\n\n\n\n\n\nTo use the class:\n\n\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nVirtualSourceDefinition\n\n\n\nsource\n \n=\n \nVirtualSourceDefinition\n(\nname\n=\n\"name\"\n,\n \nport\n=\n1000\n)\n\n\n\n\n\n\nSnapshotDefinition\n\u00b6\n\n\nDefines properties used to \nsnapshots\n.\n\n\nSnapshotDefinition Schema\n\u00b6\n\n\n{\n\n  \n\"type\"\n:\n \n\"object\"\n,\n\n  \n\"properties\"\n:\n \n{\n\n    \n\"version\"\n:\n \n{\n \n\"type\"\n:\n \n\"string\"\n \n},\n\n    \n\"transation_id\"\n:\n \n{\n \n\"type\"\n:\n \n\"integer\"\n \n}\n\n  \n}\n\n\n}\n\n\n\n\n\n\nSnapshotDefinition Class\n\u00b6\n\n\nAutogenerated based on the \nVirtualSourceDefinition Schema\n.\n\n\nclass\n \nVirtualSourceDefinition\n:\n\n\n  \ndef\n \n__init__\n(\nself\n,\n \nversion\n,\n \ntransaction_id\n):\n\n    \nself\n.\n_inner_dict\n \n=\n\n      \n{\n\n        \n\"version\"\n:\n \nversion\n,\n\n        \n\"transaction_id\"\n:\n \ntransaction_id\n\n      \n}\n\n\n\n\n\n\n\n\nTo use the class:\n\n\n\n\nfrom\n \ngenerated.defintions\n \nimport\n \nSnapshotDefinition\n\n\n\n# A snapshot with both properties defined at construction time\n\n\nsnapshot1\n \n=\n \nSnapshotDefinition\n(\nversion\n=\n\"1.2.3\"\n,\n \ntransaction_id\n=\n1000\n)\n\n\n\n# A snapshot with properties defined after construction\n\n\nsnapshot2\n \n=\n \nSnapshotDefinition\n()\n\n\nsnapshot2\n.\nversion\n \n=\n \n\"2.0.0\"\n\n\nsnapshot2\n.\ntransaction_id\n \n=\n \n1500\n\n\n\n# A snapshot that omits the optional \"transaction_id\" property\n\n\nsnapshot3\n \n=\n \nSnapshotDefinition\n(\nversion\n=\n\"1.0.0\"\n)",
            "title": "Schemas and Autogenerated Classes"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#schemas-and-autogenerated-classes",
            "text": "Plugin operations  will sometimes need to work with data in these custom formats. For example, the  configure  operation will accept snapshot data as an input, and must produce source config data as an output.  To enable this, Python classes are generated from the snapshot schema. The aforementioned inputs and outputs are instances of these autogenerated classes.   Info  Autogenerated Python code will use  lower_case_with_underscores  as attribute names as per Python variable naming conventions.\nThat is, if we were to use  mountLocation  as the schema property name, it would be called mount_location  in the generated Python code.    Info  Note that, wherever they can, these generated Python classes will enforce the constraints made by the schema. For example, if a property is listed as  required  in the schema, then every Python object will be required to always have this property. This implies that all  required  fields must be given values when the object is constructed. For various examples of this, see the examples below.",
            "title": "Schemas and Autogenerated Classes"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#repositorydefinition",
            "text": "Defines properties used to identify a  Repository .",
            "title": "RepositoryDefinition"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#repositorydefinition-schema",
            "text": "The plugin must also decide on a  name  field and a set of  identityFields  to display and uniquely identify the  repository .  { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"required\" :   [ \"name\" ,   \"path\" ], \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   }, \n     \"path\" :   {   \"type\" :   \"string\"   } \n   }, \n   \"identityFields\" :   [ \"name\" ,   \"path\" ], \n   \"nameField\" :   \"name\"  }",
            "title": "RepositoryDefinition Schema"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#repositorydefinition-class",
            "text": "Autogenerated based on the  RepositoryDefinition Schema .  class   RepositoryDefinition : \n\n   def   __init__ ( self ,   name ,   path ): \n     self . _inner_dict   =   { \"name\" :   name ,   \"path\" :   path }    To use the class:   from   generated.defintions   import   RepositoryDefinition  # Since both properties are required, they must be specified when constructing the object  repository   =   RepositoryDefinition ( name = \"name\" ,   path = \"/some/path\" )",
            "title": "RepositoryDefinition Class"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#sourceconfigdefinition",
            "text": "Defines properties used to identify a  Source Config .",
            "title": "SourceConfigDefinition"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#sourceconfigdefinition-schema",
            "text": "The plugin must also decide on a  name  field and a set of  identityFields  to display and uniquely identify the  source config .  { \n   \"type\" :   \"object\" , \n   \"additionalProperties\" :   false , \n   \"required\" :   [ \"name\" ], \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   }, \n     \"path\" :   {   \"type\" :   \"string\"   } \n   }, \n   \"identityFields\" :   [ \"name\" ], \n   \"nameField\" :   \"name\"  }",
            "title": "SourceConfigDefinition Schema"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#sourceconfigdefinition-class",
            "text": "Autogenerated based on the  SourceConfigDefinition Schema .  class   SourceConfigDefinition : \n\n   def   __init__ ( self ,   name ,   path ): \n     self . _inner_dict   =   { \"name\" :   name ,   \"path\" :   path }    To use the class:   from   generated.defintions   import   SourceConfigDefinition  # A source config that only defines the required \"name\" property.  source_config1   =   SourceConfigDefinition ( name = \"sc1\" )  # A Source config that defines both \"name\" and \"path\".  source_config2   =   SourceConfigDefinition ( name = \"sc2\" ,   path = \"/some/path\" )  # Setting the optional \"path\" property after construction  source_config3   =   SourceConfigDefinition ( name = \"sc3\" )  install_path   =   find_install_path ()  source_config3 . path   =   install_path",
            "title": "SourceConfigDefinition Class"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#linkedsourcedefinition",
            "text": "Defines properties used to identify  linked sources .",
            "title": "LinkedSourceDefinition"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#linkedsourcedefinition-schema",
            "text": "{ \n   \"type\" :   \"object\" , \n   \"required\" :   [ \"name\" ,   \"port\" ], \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   }, \n     \"port\" :   {   \"type\" :   \"integer\"   } \n   }  }",
            "title": "LinkedSourceDefinition Schema"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#linkedsourcedefinition-class",
            "text": "Autogenerated based on the  LinkedSourceDefinition Schema .  class   LinkedSourceDefinition : \n\n   def   __init__ ( self ,   name ,   port ): \n     self . _inner_dict   =   { \"name\" :   name ,   \"port\" :   port }    To use the class:   from   generated.defintions   import   LinkedSourceDefinition  source   =   LinkedSourceDefinition ( name = \"name\" ,   port = 1000 )  # Retrieve the properties from the object and log them  name   =   source . name  port   =   source . port  logger . debug ( \"Creating source  \\\" {} \\\"  with port {}\" . format ( name ,   port ))",
            "title": "LinkedSourceDefinition Class"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#virtualsourcedefinition",
            "text": "Defines properties used to identify  virtual sources .",
            "title": "VirtualSourceDefinition"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#virtualsourcedefinition-schema",
            "text": "{ \n   \"type\" :   \"object\" , \n   \"required\" :   [ \"name\" ,   \"port\" ], \n   \"additionalProperties\" :   false , \n   \"properties\" :   { \n     \"name\" :   {   \"type\" :   \"string\"   }, \n     \"port\" :   {   \"type\" :   \"integer\"   } \n   }  }",
            "title": "VirtualSourceDefinition Schema"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#virtualsourcedefinition-class",
            "text": "Autogenerated based on the  VirtualSourceDefinition Schema .  class   VirtualSourceDefinition : \n\n   def   __init__ ( self ,   name ,   port ): \n     self . _inner_dict   =   { \"name\" :   name ,   \"port\" :   port }    To use the class:   from   generated.defintions   import   VirtualSourceDefinition  source   =   VirtualSourceDefinition ( name = \"name\" ,   port = 1000 )",
            "title": "VirtualSourceDefinition Class"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#snapshotdefinition",
            "text": "Defines properties used to  snapshots .",
            "title": "SnapshotDefinition"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#snapshotdefinition-schema",
            "text": "{ \n   \"type\" :   \"object\" , \n   \"properties\" :   { \n     \"version\" :   {   \"type\" :   \"string\"   }, \n     \"transation_id\" :   {   \"type\" :   \"integer\"   } \n   }  }",
            "title": "SnapshotDefinition Schema"
        },
        {
            "location": "/References/Schemas_and_Autogenerated_Classes/#snapshotdefinition-class",
            "text": "Autogenerated based on the  VirtualSourceDefinition Schema .  class   VirtualSourceDefinition : \n\n   def   __init__ ( self ,   version ,   transaction_id ): \n     self . _inner_dict   = \n       { \n         \"version\" :   version , \n         \"transaction_id\" :   transaction_id \n       }    To use the class:   from   generated.defintions   import   SnapshotDefinition  # A snapshot with both properties defined at construction time  snapshot1   =   SnapshotDefinition ( version = \"1.2.3\" ,   transaction_id = 1000 )  # A snapshot with properties defined after construction  snapshot2   =   SnapshotDefinition ()  snapshot2 . version   =   \"2.0.0\"  snapshot2 . transaction_id   =   1500  # A snapshot that omits the optional \"transaction_id\" property  snapshot3   =   SnapshotDefinition ( version = \"1.0.0\" )",
            "title": "SnapshotDefinition Class"
        },
        {
            "location": "/References/Platform_Libraries/",
            "text": "Platform Libraries\n\u00b6\n\n\nSet of functions that plugins can use these for executing remote commands, etc.\n\n\nrun_bash\n\u00b6\n\n\nExecutes a bash command on a remote Unix host.\n\n\nSignature\n\u00b6\n\n\ndef run_bash(remote_connection, command, variables=None, use_login_shell=False, check=False)\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nremote_connection\n\n\nRemoteConnection\n\n\nConnection associated with the remote host to run the command on.\n\n\n\n\n\n\ncommand\n\n\nString\n\n\nCommand to run on the host.\n\n\n\n\n\n\nvariables\n\n\ndict[String, String]\n\n\nOptional\n. Environement variables to set when running the command.\n\n\n\n\n\n\nuse_login_shell\n\n\nboolean\n\n\nOptional\n. Whether to use a login shell.\n\n\n\n\n\n\ncheck\n\n\nboolean\n\n\nOptional\n. Whether or not to raise an exception if the \nexit_code\n in the \nRunBashResponse\n is non-zero.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nAn object of \nRunBashResponse\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nexit_code\n\n\nInteger\n\n\nExit code from the command.\n\n\n\n\n\n\nstdout\n\n\nString\n\n\nStdout from the command.\n\n\n\n\n\n\nstderr\n\n\nString\n\n\nStderr from the command.\n\n\n\n\n\n\n\n\nExamples\n\u00b6\n\n\nCalling bash with an inline command.\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\ncommand\n \n=\n \n\"echo 'Hi' >> /tmp/debug.log\"\n\n\nvariables\n \n=\n \n{\n\"var\"\n:\n \n\"val\"\n}\n\n\n\nresponse\n \n=\n \nlibs\n.\nrun_bash\n(\nconnection\n,\n \ncommand\n,\n \nvariables\n)\n\n\n\nprint\n \nresponse\n.\nexit_code\n\n\nprint\n \nresponse\n.\nstdout\n\n\nprint\n \nresponse\n.\nstderr\n\n\n\n\n\n\nUsing parameters to construct a bash command.\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\nname\n \n=\n \nvirtual_source\n.\nparameters\n.\nusername\n\n\nport\n \n=\n \nvirtual_source\n.\nparameters\n.\nport\n\n\ncommand\n \n=\n \n\"mysqldump -u {} -p {}\"\n.\nformat\n(\nname\n,\nport\n)\n\n\n\nresponse\n \n=\n \nlibs\n.\nrun_bash\n(\nconnection\n,\n \ncommand\n)\n\n\n\n\n\n\nRunning a bash script that is saved in a directory.\n\n\n \nimport\n \npkgutil\n\n \nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n \nscript_content\n \n=\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_date.sh'\n)\n\n\n \n# Execute script on remote host\n\n \nresponse\n \n=\n \nlibs\n.\nrun_bash\n(\ndirect_source\n.\nconnection\n,\n \nscript_content\n)\n\n\n\n\n\n\nFor more information please go to \nManaging Scripts for Remote Execution\n section.\n\n\nrun_expect\n\u00b6\n\n\nExecutes a tcl command or script on a remote Unix host.\n\n\nSignature\n\u00b6\n\n\ndef run_expect(remote_connection, command, variables=None)\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nremote_connection\n\n\nRemoteConnection\n\n\nConnection associated with the remote host to run the command on.\n\n\n\n\n\n\ncommand\n\n\nString\n\n\nExpect(Tcl) command to run.\n\n\n\n\n\n\nvariables\n\n\ndict[String, String]\n\n\nOptional\n. Environement variables to set when running the command.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nAn object of \nRunExpectResponse\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nexit_code\n\n\nInteger\n\n\nExit code from the command.\n\n\n\n\n\n\nstdout\n\n\nString\n\n\nStdout from the command.\n\n\n\n\n\n\nstderr\n\n\nString\n\n\nStderr from the command.\n\n\n\n\n\n\n\n\nExample\n\u00b6\n\n\nCalling expect  with an inline command.\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\ncommand\n \n=\n \n\"puts 'Hi'\"\n\n\nvariables\n \n=\n \n{\n\"var\"\n:\n \n\"val\"\n}\n\n\n\nrepsonse\n \n=\n \nlibs\n.\nrun_expect\n(\nconnection\n,\n \ncommand\n,\n \nvariables\n)\n\n\n\nprint\n \nresponse\n.\nexit_code\n\n\nprint\n \nresponse\n.\nstdout\n\n\nprint\n \nresponse\n.\nstderr\n\n\n\n\n\n\nrun_powershell\n\u00b6\n\n\nExecutes a powershell command on a remote Windows host.\n\n\nSignature\n\u00b6\n\n\ndef run_powershell(remote_connection, command, variables=None, check=False)\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nremote_connection\n\n\nRemoteConnection\n\n\nConnection associated with the remote host to run the command on.\n\n\n\n\n\n\ncommand\n\n\nString\n\n\nCommand to run to the remote host.\n\n\n\n\n\n\nvariables\n\n\ndict[String, String]\n\n\nOptional\n. Environement variables to set when running the command.\n\n\n\n\n\n\ncheck\n\n\nboolean\n\n\nOptional\n. Whether or not to raise an exception if the \nexit_code\n in the \nRunPowershellResponse\n is non-zero.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nAn object of \nRunPowershellResponse\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nexit_code\n\n\nInteger\n\n\nExit code from the command.\n\n\n\n\n\n\nstdout\n\n\nString\n\n\nStdout from the command.\n\n\n\n\n\n\nstderr\n\n\nString\n\n\nStderr from the command.\n\n\n\n\n\n\n\n\nExample\n\u00b6\n\n\nCalling powershell with an inline command.\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\ncommand\n \n=\n \n\"Write-Output 'Hi'\"\n\n\nvariables\n \n=\n \n{\n\"var\"\n:\n \n\"val\"\n}\n\n\n\nresponse\n \n=\n \nlibs\n.\nrun_powershell\n(\nconnection\n,\n \ncommand\n,\n \nvariables\n)\n\n\n\nprint\n \nresponse\n.\nexit_code\n\n\nprint\n \nresponse\n.\nstdout\n\n\nprint\n \nresponse\n.\nstderr\n\n\n\n\n\n\nrun_sync\n\u00b6\n\n\nCopies files from the remote source host directly into the dSource, without involving a staging host.\n\n\nSignature\n\u00b6\n\n\ndef run_sync(remote_connection, source_directory, rsync_user=None, exclude_paths=None, sym_links_to_follow=None)\n\n\nArguments\n\u00b6\n\n\n\n\n\n\n\n\nArgument\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nremote_connection\n\n\nRemoteConnection\n\n\nConnection associated with the remote host to run the command on.\n\n\n\n\n\n\nsource_directory\n\n\nString\n\n\nDirectory of files to be synced.\n\n\n\n\n\n\nrsync_user\n\n\nString\n\n\nOptional\n User who has access to the directory to be synced.\n\n\n\n\n\n\nexclude_paths\n\n\nlist[String]\n\n\nOptional\n Paths to be excluded.\n\n\n\n\n\n\nsym_links_to_follow\n\n\nlist[String]\n\n\nOptional\n Symbollic links to follow if any.\n\n\n\n\n\n\n\n\nReturns\n\u00b6\n\n\nNone\n\n\nExample\n\u00b6\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\nsource_directory\n \n=\n \n\"sourceDirectory\"\n\n\nrsync_user\n \n=\n \n\"rsyncUser\"\n\n\nexclude_paths\n \n=\n \n[\n\"/path1\"\n,\n \n\"/path2\"\n]\n\n\nsym_links_to_follow\n \n=\n \n[\n\"/path3\"\n,\n \n\"/path4\"\n]\n\n\n\nlibs\n.\nrun_sync\n(\nconnection\n,\n \nsource_directory\n,\n \nrsync_user\n,\n \nexclude_paths\n,\n \nsym_links_to_follow\n)",
            "title": "Platform Libraries"
        },
        {
            "location": "/References/Platform_Libraries/#platform-libraries",
            "text": "Set of functions that plugins can use these for executing remote commands, etc.",
            "title": "Platform Libraries"
        },
        {
            "location": "/References/Platform_Libraries/#run_bash",
            "text": "Executes a bash command on a remote Unix host.",
            "title": "run_bash"
        },
        {
            "location": "/References/Platform_Libraries/#signature",
            "text": "def run_bash(remote_connection, command, variables=None, use_login_shell=False, check=False)",
            "title": "Signature"
        },
        {
            "location": "/References/Platform_Libraries/#arguments",
            "text": "Argument  Type  Description      remote_connection  RemoteConnection  Connection associated with the remote host to run the command on.    command  String  Command to run on the host.    variables  dict[String, String]  Optional . Environement variables to set when running the command.    use_login_shell  boolean  Optional . Whether to use a login shell.    check  boolean  Optional . Whether or not to raise an exception if the  exit_code  in the  RunBashResponse  is non-zero.",
            "title": "Arguments"
        },
        {
            "location": "/References/Platform_Libraries/#returns",
            "text": "An object of  RunBashResponse     Field  Type  Description      exit_code  Integer  Exit code from the command.    stdout  String  Stdout from the command.    stderr  String  Stderr from the command.",
            "title": "Returns"
        },
        {
            "location": "/References/Platform_Libraries/#examples",
            "text": "Calling bash with an inline command.  from   dlpx.virtualization   import   libs  command   =   \"echo 'Hi' >> /tmp/debug.log\"  variables   =   { \"var\" :   \"val\" }  response   =   libs . run_bash ( connection ,   command ,   variables )  print   response . exit_code  print   response . stdout  print   response . stderr   Using parameters to construct a bash command.  from   dlpx.virtualization   import   libs  name   =   virtual_source . parameters . username  port   =   virtual_source . parameters . port  command   =   \"mysqldump -u {} -p {}\" . format ( name , port )  response   =   libs . run_bash ( connection ,   command )   Running a bash script that is saved in a directory.    import   pkgutil \n  from   dlpx.virtualization   import   libs \n\n  script_content   =   pkgutil . get_data ( 'resources' ,   'get_date.sh' ) \n\n  # Execute script on remote host \n  response   =   libs . run_bash ( direct_source . connection ,   script_content )   For more information please go to  Managing Scripts for Remote Execution  section.",
            "title": "Examples"
        },
        {
            "location": "/References/Platform_Libraries/#run_expect",
            "text": "Executes a tcl command or script on a remote Unix host.",
            "title": "run_expect"
        },
        {
            "location": "/References/Platform_Libraries/#signature_1",
            "text": "def run_expect(remote_connection, command, variables=None)",
            "title": "Signature"
        },
        {
            "location": "/References/Platform_Libraries/#arguments_1",
            "text": "Argument  Type  Description      remote_connection  RemoteConnection  Connection associated with the remote host to run the command on.    command  String  Expect(Tcl) command to run.    variables  dict[String, String]  Optional . Environement variables to set when running the command.",
            "title": "Arguments"
        },
        {
            "location": "/References/Platform_Libraries/#returns_1",
            "text": "An object of  RunExpectResponse     Field  Type  Description      exit_code  Integer  Exit code from the command.    stdout  String  Stdout from the command.    stderr  String  Stderr from the command.",
            "title": "Returns"
        },
        {
            "location": "/References/Platform_Libraries/#example",
            "text": "Calling expect  with an inline command.  from   dlpx.virtualization   import   libs  command   =   \"puts 'Hi'\"  variables   =   { \"var\" :   \"val\" }  repsonse   =   libs . run_expect ( connection ,   command ,   variables )  print   response . exit_code  print   response . stdout  print   response . stderr",
            "title": "Example"
        },
        {
            "location": "/References/Platform_Libraries/#run_powershell",
            "text": "Executes a powershell command on a remote Windows host.",
            "title": "run_powershell"
        },
        {
            "location": "/References/Platform_Libraries/#signature_2",
            "text": "def run_powershell(remote_connection, command, variables=None, check=False)",
            "title": "Signature"
        },
        {
            "location": "/References/Platform_Libraries/#arguments_2",
            "text": "Argument  Type  Description      remote_connection  RemoteConnection  Connection associated with the remote host to run the command on.    command  String  Command to run to the remote host.    variables  dict[String, String]  Optional . Environement variables to set when running the command.    check  boolean  Optional . Whether or not to raise an exception if the  exit_code  in the  RunPowershellResponse  is non-zero.",
            "title": "Arguments"
        },
        {
            "location": "/References/Platform_Libraries/#returns_2",
            "text": "An object of  RunPowershellResponse     Field  Type  Description      exit_code  Integer  Exit code from the command.    stdout  String  Stdout from the command.    stderr  String  Stderr from the command.",
            "title": "Returns"
        },
        {
            "location": "/References/Platform_Libraries/#example_1",
            "text": "Calling powershell with an inline command.  from   dlpx.virtualization   import   libs  command   =   \"Write-Output 'Hi'\"  variables   =   { \"var\" :   \"val\" }  response   =   libs . run_powershell ( connection ,   command ,   variables )  print   response . exit_code  print   response . stdout  print   response . stderr",
            "title": "Example"
        },
        {
            "location": "/References/Platform_Libraries/#run_sync",
            "text": "Copies files from the remote source host directly into the dSource, without involving a staging host.",
            "title": "run_sync"
        },
        {
            "location": "/References/Platform_Libraries/#signature_3",
            "text": "def run_sync(remote_connection, source_directory, rsync_user=None, exclude_paths=None, sym_links_to_follow=None)",
            "title": "Signature"
        },
        {
            "location": "/References/Platform_Libraries/#arguments_3",
            "text": "Argument  Type  Description      remote_connection  RemoteConnection  Connection associated with the remote host to run the command on.    source_directory  String  Directory of files to be synced.    rsync_user  String  Optional  User who has access to the directory to be synced.    exclude_paths  list[String]  Optional  Paths to be excluded.    sym_links_to_follow  list[String]  Optional  Symbollic links to follow if any.",
            "title": "Arguments"
        },
        {
            "location": "/References/Platform_Libraries/#returns_3",
            "text": "None",
            "title": "Returns"
        },
        {
            "location": "/References/Platform_Libraries/#example_2",
            "text": "from   dlpx.virtualization   import   libs  source_directory   =   \"sourceDirectory\"  rsync_user   =   \"rsyncUser\"  exclude_paths   =   [ \"/path1\" ,   \"/path2\" ]  sym_links_to_follow   =   [ \"/path3\" ,   \"/path4\" ]  libs . run_sync ( connection ,   source_directory ,   rsync_user ,   exclude_paths ,   sym_links_to_follow )",
            "title": "Example"
        },
        {
            "location": "/References/Logging/",
            "text": "Logging\n\u00b6\n\n\nWhat is logging?\n\u00b6\n\n\nThe Virtualization Platform keeps plugin-specific log files. A plugin can, at any point in any of its \nplugin operations\n, write out some text to its log file(s). These log files can be examined later, typically to try to debug a problem with the plugin.\n\n\nOverview\n\u00b6\n\n\nThe Virtualization Platform integrates with Python's built-in \nlogging framework\n. A special \nHandler\n is exposed by the platform at \ndlpx.virtualization.libs.PlatformHandler\n. This handler needs to be added to the Python logger your plugin creates. Logging statements made through Python's logging framework will then be routed to the platform.\n\n\nBasic Setup\n\u00b6\n\n\nBelow is the absolute minimum needed to setup logging for the platform. Please refer to Python's \nlogging documentation\n and the \nexample below\n to better understand how it can be customized.\n\n\nimport\n \nlogging\n\n\n\nfrom\n \ndlpx.virtualization.libs\n \nimport\n \nPlatformHandler\n\n\n\n# Get the root logger.\n\n\nlogger\n \n=\n \nlogging\n.\ngetLogger\n()\n\n\nlogger\n.\naddHandler\n(\nPlatformHandler\n())\n\n\n\n# The root logger's default level is logging.WARNING.\n\n\n# Without the line below, logging statements of levels\n\n\n# lower than logging.WARNING will be suppressed.\n\n\nlogger\n.\nsetLevel\n(\nlogging\n.\nDEBUG\n)\n\n\n\n\n\n\n\n\nLogging Setup\n\n\nPython's logging framework is global. Setup only needs to happen once, but where it happens is important. Any logging statements that occur before the \nPlatformHandler\n is added will not be logged by the platform.\n\n\nIt is highly recommended that the logging setup is done in the plugin's entry point module before any operations are ran.\n\n\n\n\n\n\nAdd the PlatformHandler to the root logger\n\n\nLoggers in Python have a hierarchy and all loggers are children of a special logger called the \"root logger\". Logging hierarchy is not always intuitive and depends on how modules are structured.\n\n\nTo avoid this complexity, add the \nPlatformHandler\n to the root logger. The root logger can be retrieved with \nlogging.getLogger()\n.\n\n\n\n\nUsage\n\u00b6\n\n\nOnce the \nPlatformHandler\n has been added to the logger, logging is done with Python's \nLogger\n object. Below is a simple example including the basic setup code used above:\n\n\nimport\n \nlogging\n\n\n\nfrom\n \ndlpx.virtualization.libs\n \nimport\n \nPlatformHandler\n\n\n\nlogger\n \n=\n \nlogging\n.\ngetLogger\n()\n\n\nlogger\n.\naddHandler\n(\nPlatformHandler\n())\n\n\n\n# The root logger's default level is logging.WARNING.\n\n\n# Without the line below, logging statements of levels\n\n\n# lower than logging.WARNING will be suppressed.\n\n\nlogger\n.\nsetLevel\n(\nlogging\n.\nDEBUG\n)\n\n\n\nlogger\n.\ndebug\n(\n'debug'\n)\n\n\nlogger\n.\ninfo\n(\n'info'\n)\n\n\nlogger\n.\nerror\n(\n'error'\n)\n\n\n\n\n\n\nExample\n\u00b6\n\n\nImagine you notice that your plugin is taking a very long time to do discovery. Everything works, it just takes much longer than expected. You'd like to figure out why.\n\n\n\n\nInfo\n\n\nRefer to \nManaging Scripts for Remote Execution\n for how remote scripts can be stored and retrieved.\n\n\n\n\nSuppose your plugin has a source config discovery operation that looks like this (code is abbreviated to be easier to follow):\n\n\nimport\n \npkgutil\n\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.discovery.repository\n()\n\n\ndef\n \nrepository_discovery\n(\nsource_connection\n):\n \n  \nreturn\n \n[\nRepositoryDefinition\n(\n'Logging Example'\n)]\n\n\n\n\n@plugin.discovery.source_config\n()\n\n\ndef\n \nsource_config_discovery\n(\nsource_connection\n,\n \nrepository\n):\n\n  \nversion_result\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_db_version.sh'\n))\n\n  \nusers_result\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_db_users.sh'\n))\n\n  \ndb_results\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_databases.sh'\n))\n\n  \nstatus_result\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_database_statuses.sh'\n))\n\n\n  \n# Return an empty list for simplicity. In reality\n\n  \n# something would be done with the results above.\n\n  \nreturn\n \n[]\n\n\n\n\n\n\nNow, imagine that you notice that it's taking a long time to do discovery, and you'd like to try to figure out why. One thing that might help is to add logging, like this:\n\n\nimport\n \nlogging\n\n\nimport\n \npkgutil\n\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nRepositoryDefinition\n\n\n\n# This should probably be defined in its own module outside\n\n\n# of the plugin's entry point file. It is here for simplicity.\n\n\ndef\n \n_setup_logger\n():\n\n    \n# This will log the time, level, filename, line number, and log message.\n\n    \nlog_message_format\n \n=\n \n'[\n%(asctime)s\n] [\n%(levelname)s\n] [\n%(filename)s\n:\n%(lineno)d\n] \n%(message)s\n'\n\n    \nlog_message_date_format\n \n=\n \n'%Y-%m-\n%d\n %H:%M:%S'\n\n\n    \n# Create a custom formatter. This will help with diagnosability.\n\n    \nformatter\n \n=\n \nlogging\n.\nFormatter\n(\nlog_message_format\n,\n \ndatefmt\n=\n \nlog_message_date_format\n)\n\n\n    \nplatform_handler\n \n=\n \nlibs\n.\nPlatformHandler\n()\n\n    \nplatform_handler\n.\nsetFormatter\n(\nformatter\n)\n\n\n    \nlogger\n \n=\n \nlogging\n.\ngetLogger\n()\n\n    \nlogger\n.\naddHandler\n(\nplatform_handler\n)\n\n\n    \n# By default the root logger's level is logging.WARNING.\n\n    \nlogger\n.\nsetLevel\n(\nlogging\n.\nDEBUG\n)\n\n\n\n\n# Setup the logger.\n\n\n_setup_logger\n()\n\n\n\n# logging.getLogger(__name__) is the convention way to get a logger in Python.\n\n\n# It returns a new logger per module and will be a child of the root logger.\n\n\n# Since we setup the root logger, nothing else needs to be done to set this\n\n\n# one up.\n\n\nlogger\n \n=\n \nlogging\n.\ngetLogger\n(\n__name__\n)\n\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.discovery.repository\n()\n\n\ndef\n \nrepository_discovery\n(\nsource_connection\n):\n \n  \nreturn\n \n[\nRepositoryDefinition\n(\n'Logging Example'\n)]\n\n\n\n@plugin.discovery.source_config\n()\n\n\ndef\n \nsource_config_discovery\n(\nsource_connection\n,\n \nrepository\n):\n\n  \nlogger\n.\ndebug\n(\n'About to get DB version'\n)\n\n  \nversion_result\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_db_version.sh'\n))\n\n  \nlogger\n.\ndebug\n(\n'About to get DB users'\n)\n\n  \nusers_result\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_db_users.sh'\n))\n\n  \nlogger\n.\ndebug\n(\n'About to get databases'\n)\n\n  \ndb_results\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_databases.sh'\n))\n\n  \nlogger\n.\ndebug\n(\n'About to get DB statuses'\n)\n\n  \nstatus_result\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_database_statuses.sh'\n))\n\n  \nlogger\n.\ndebug\n(\n'Done collecting data'\n)\n\n\n  \n# Return an empty list for simplicity. In reality\n\n  \n# something would be done with the results above.\n\n  \nreturn\n \n[]\n\n\n\n\n\n\nWhen you look at the log file, perhaps you'll see something like this:\n\n\n[Worker-360|JOB-315|ENVIRONMENT_DISCOVER(UNIX_HOST_ENVIRONMENT-5)] [2019-04-30 12:10:42] [DEBUG] [python_runner.py:44] About to get DB version\n[Worker-360|JOB-316|DB_SYNC(APPDATA_CONTAINER-21)] [2019-04-30 12:19:35] [DEBUG] [python_runner.py:49] About to get DB users\n[Worker-325|JOB-280|ENVIRONMENT_REFRESH(UNIX_HOST_ENVIRONMENT-5)] [DEBUG] [plugin_runner.py:51] About to get databases\n[Worker-326|JOB-281|SOURCES_DISABLE(UNIX_HOST_ENVIRONMENT-5)] [DEBUG] [plugin_runner.py:53] About to get DB statuses\n\n\n\n\n\nYou can see that it only takes a few seconds for us do each of our data collection steps, with the exception of getting the users, which takes over 13 minutes!\n\n\nWe now know that our slowdown is something to do with how our bash script is collecting all the users. Logging has gotten us a lot closer to figuring out the problem.\n\n\nHow to retrieve logs\n\u00b6\n\n\nDownload a support bundle by going to \nHelp\n > \nSupport Logs\n  and select \nDownload\n. The logs will be in a the support bundle under \nlog/mgmt_log/plugin_log/<plugin name>\n.\n\n\nLogging Levels\n\u00b6\n\n\nPython has a number of \npreset logging levels\n and allows for custom ones as well. Since logging on the Virtualization Platform uses the \nlogging\n framework, log statements of all levels are supported.\n\n\nHowever, the Virtualization Platform will map all logging levels into three files: \ndebug.log\n, \ninfo.log\n, and \nerror.log\n in the following way:\n\n\n\n\n\n\n\n\nPython Logging Level\n\n\nLogging File\n\n\n\n\n\n\n\n\n\n\nDEBUG\n\n\ndebug.log\n\n\n\n\n\n\nINFO\n\n\ninfo.log\n\n\n\n\n\n\nWARN\n\n\nerror.log\n\n\n\n\n\n\nWARNING\n\n\nerror.log\n\n\n\n\n\n\nERROR\n\n\nerror.log\n\n\n\n\n\n\nCRITICAL\n\n\nerror.log\n\n\n\n\n\n\n\n\nAs is the case with the \nlogging\n framework, logging statements are hierarchical: logging statements made at the \nlogging.DEBUG\n level will be written only to \ndebug.log\n while logging statements made at the \nlogging.ERROR\n level will be written to \ndebug.log\n, \ninfo.log\n, and \nerror.log\n.\n\n\nSensitive data\n\u00b6\n\n\nRemember that logging data means writing that data out in cleartext. Make sure you never log any data that could be secret or sensitive (passwords, etc.). For more details please see our section on \nsensitive data",
            "title": "Logging"
        },
        {
            "location": "/References/Logging/#logging",
            "text": "",
            "title": "Logging"
        },
        {
            "location": "/References/Logging/#what-is-logging",
            "text": "The Virtualization Platform keeps plugin-specific log files. A plugin can, at any point in any of its  plugin operations , write out some text to its log file(s). These log files can be examined later, typically to try to debug a problem with the plugin.",
            "title": "What is logging?"
        },
        {
            "location": "/References/Logging/#overview",
            "text": "The Virtualization Platform integrates with Python's built-in  logging framework . A special  Handler  is exposed by the platform at  dlpx.virtualization.libs.PlatformHandler . This handler needs to be added to the Python logger your plugin creates. Logging statements made through Python's logging framework will then be routed to the platform.",
            "title": "Overview"
        },
        {
            "location": "/References/Logging/#basic-setup",
            "text": "Below is the absolute minimum needed to setup logging for the platform. Please refer to Python's  logging documentation  and the  example below  to better understand how it can be customized.  import   logging  from   dlpx.virtualization.libs   import   PlatformHandler  # Get the root logger.  logger   =   logging . getLogger ()  logger . addHandler ( PlatformHandler ())  # The root logger's default level is logging.WARNING.  # Without the line below, logging statements of levels  # lower than logging.WARNING will be suppressed.  logger . setLevel ( logging . DEBUG )    Logging Setup  Python's logging framework is global. Setup only needs to happen once, but where it happens is important. Any logging statements that occur before the  PlatformHandler  is added will not be logged by the platform.  It is highly recommended that the logging setup is done in the plugin's entry point module before any operations are ran.    Add the PlatformHandler to the root logger  Loggers in Python have a hierarchy and all loggers are children of a special logger called the \"root logger\". Logging hierarchy is not always intuitive and depends on how modules are structured.  To avoid this complexity, add the  PlatformHandler  to the root logger. The root logger can be retrieved with  logging.getLogger() .",
            "title": "Basic Setup"
        },
        {
            "location": "/References/Logging/#usage",
            "text": "Once the  PlatformHandler  has been added to the logger, logging is done with Python's  Logger  object. Below is a simple example including the basic setup code used above:  import   logging  from   dlpx.virtualization.libs   import   PlatformHandler  logger   =   logging . getLogger ()  logger . addHandler ( PlatformHandler ())  # The root logger's default level is logging.WARNING.  # Without the line below, logging statements of levels  # lower than logging.WARNING will be suppressed.  logger . setLevel ( logging . DEBUG )  logger . debug ( 'debug' )  logger . info ( 'info' )  logger . error ( 'error' )",
            "title": "Usage"
        },
        {
            "location": "/References/Logging/#example",
            "text": "Imagine you notice that your plugin is taking a very long time to do discovery. Everything works, it just takes much longer than expected. You'd like to figure out why.   Info  Refer to  Managing Scripts for Remote Execution  for how remote scripts can be stored and retrieved.   Suppose your plugin has a source config discovery operation that looks like this (code is abbreviated to be easier to follow):  import   pkgutil  from   dlpx.virtualization   import   libs  from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.discovery.repository ()  def   repository_discovery ( source_connection ):  \n   return   [ RepositoryDefinition ( 'Logging Example' )]  @plugin.discovery.source_config ()  def   source_config_discovery ( source_connection ,   repository ): \n   version_result   =   libs . run_bash ( source_connection ,   pkgutil . get_data ( 'resources' ,   'get_db_version.sh' )) \n   users_result   =   libs . run_bash ( source_connection ,   pkgutil . get_data ( 'resources' ,   'get_db_users.sh' )) \n   db_results   =   libs . run_bash ( source_connection ,   pkgutil . get_data ( 'resources' ,   'get_databases.sh' )) \n   status_result   =   libs . run_bash ( source_connection ,   pkgutil . get_data ( 'resources' ,   'get_database_statuses.sh' )) \n\n   # Return an empty list for simplicity. In reality \n   # something would be done with the results above. \n   return   []   Now, imagine that you notice that it's taking a long time to do discovery, and you'd like to try to figure out why. One thing that might help is to add logging, like this:  import   logging  import   pkgutil  from   dlpx.virtualization   import   libs  from   dlpx.virtualization.platform   import   Plugin  from   generated.definitions   import   RepositoryDefinition  # This should probably be defined in its own module outside  # of the plugin's entry point file. It is here for simplicity.  def   _setup_logger (): \n     # This will log the time, level, filename, line number, and log message. \n     log_message_format   =   '[ %(asctime)s ] [ %(levelname)s ] [ %(filename)s : %(lineno)d ]  %(message)s ' \n     log_message_date_format   =   '%Y-%m- %d  %H:%M:%S' \n\n     # Create a custom formatter. This will help with diagnosability. \n     formatter   =   logging . Formatter ( log_message_format ,   datefmt =   log_message_date_format ) \n\n     platform_handler   =   libs . PlatformHandler () \n     platform_handler . setFormatter ( formatter ) \n\n     logger   =   logging . getLogger () \n     logger . addHandler ( platform_handler ) \n\n     # By default the root logger's level is logging.WARNING. \n     logger . setLevel ( logging . DEBUG )  # Setup the logger.  _setup_logger ()  # logging.getLogger(__name__) is the convention way to get a logger in Python.  # It returns a new logger per module and will be a child of the root logger.  # Since we setup the root logger, nothing else needs to be done to set this  # one up.  logger   =   logging . getLogger ( __name__ )  plugin   =   Plugin ()  @plugin.discovery.repository ()  def   repository_discovery ( source_connection ):  \n   return   [ RepositoryDefinition ( 'Logging Example' )]  @plugin.discovery.source_config ()  def   source_config_discovery ( source_connection ,   repository ): \n   logger . debug ( 'About to get DB version' ) \n   version_result   =   libs . run_bash ( source_connection ,   pkgutil . get_data ( 'resources' ,   'get_db_version.sh' )) \n   logger . debug ( 'About to get DB users' ) \n   users_result   =   libs . run_bash ( source_connection ,   pkgutil . get_data ( 'resources' ,   'get_db_users.sh' )) \n   logger . debug ( 'About to get databases' ) \n   db_results   =   libs . run_bash ( source_connection ,   pkgutil . get_data ( 'resources' ,   'get_databases.sh' )) \n   logger . debug ( 'About to get DB statuses' ) \n   status_result   =   libs . run_bash ( source_connection ,   pkgutil . get_data ( 'resources' ,   'get_database_statuses.sh' )) \n   logger . debug ( 'Done collecting data' ) \n\n   # Return an empty list for simplicity. In reality \n   # something would be done with the results above. \n   return   []   When you look at the log file, perhaps you'll see something like this:  [Worker-360|JOB-315|ENVIRONMENT_DISCOVER(UNIX_HOST_ENVIRONMENT-5)] [2019-04-30 12:10:42] [DEBUG] [python_runner.py:44] About to get DB version\n[Worker-360|JOB-316|DB_SYNC(APPDATA_CONTAINER-21)] [2019-04-30 12:19:35] [DEBUG] [python_runner.py:49] About to get DB users\n[Worker-325|JOB-280|ENVIRONMENT_REFRESH(UNIX_HOST_ENVIRONMENT-5)] [DEBUG] [plugin_runner.py:51] About to get databases\n[Worker-326|JOB-281|SOURCES_DISABLE(UNIX_HOST_ENVIRONMENT-5)] [DEBUG] [plugin_runner.py:53] About to get DB statuses  You can see that it only takes a few seconds for us do each of our data collection steps, with the exception of getting the users, which takes over 13 minutes!  We now know that our slowdown is something to do with how our bash script is collecting all the users. Logging has gotten us a lot closer to figuring out the problem.",
            "title": "Example"
        },
        {
            "location": "/References/Logging/#how-to-retrieve-logs",
            "text": "Download a support bundle by going to  Help  >  Support Logs   and select  Download . The logs will be in a the support bundle under  log/mgmt_log/plugin_log/<plugin name> .",
            "title": "How to retrieve logs"
        },
        {
            "location": "/References/Logging/#logging-levels",
            "text": "Python has a number of  preset logging levels  and allows for custom ones as well. Since logging on the Virtualization Platform uses the  logging  framework, log statements of all levels are supported.  However, the Virtualization Platform will map all logging levels into three files:  debug.log ,  info.log , and  error.log  in the following way:     Python Logging Level  Logging File      DEBUG  debug.log    INFO  info.log    WARN  error.log    WARNING  error.log    ERROR  error.log    CRITICAL  error.log     As is the case with the  logging  framework, logging statements are hierarchical: logging statements made at the  logging.DEBUG  level will be written only to  debug.log  while logging statements made at the  logging.ERROR  level will be written to  debug.log ,  info.log , and  error.log .",
            "title": "Logging Levels"
        },
        {
            "location": "/References/Logging/#sensitive-data",
            "text": "Remember that logging data means writing that data out in cleartext. Make sure you never log any data that could be secret or sensitive (passwords, etc.). For more details please see our section on  sensitive data",
            "title": "Sensitive data"
        },
        {
            "location": "/References/Workflows/",
            "text": "Workflows\n\u00b6\n\n\nLegend\n\u00b6\n\n\n\n\nEnvironment Discovery / Refresh\n\u00b6\n\n\n\n\nLinked Source Sync\n\u00b6\n\n\n\n\nLinked Source Enable\n\u00b6\n\n\n\n\nLinked Source Disable\n\u00b6\n\n\n\n\nLinked Source Delete\n\u00b6\n\n\n\n\nVirtual Source Provision\n\u00b6\n\n\n\n\nVirtual Source Snapshot\n\u00b6\n\n\n\n\nVirtual Source Refresh\n\u00b6\n\n\n\n\nVirtual Source Rollback\n\u00b6\n\n\n\n\nVirtual Source Delete\n\u00b6\n\n\n\n\nVirtual Source Start\n\u00b6\n\n\n\n\nVirtual Source Stop\n\u00b6\n\n\n\n\nVirtual Source Enable\n\u00b6\n\n\n\n\nVirtual Source Disable\n\u00b6\n\n\n\n\nUpgrade\n\u00b6",
            "title": "Workflows"
        },
        {
            "location": "/References/Workflows/#workflows",
            "text": "",
            "title": "Workflows"
        },
        {
            "location": "/References/Workflows/#legend",
            "text": "",
            "title": "Legend"
        },
        {
            "location": "/References/Workflows/#environment-discovery-refresh",
            "text": "",
            "title": "Environment Discovery / Refresh"
        },
        {
            "location": "/References/Workflows/#linked-source-sync",
            "text": "",
            "title": "Linked Source Sync"
        },
        {
            "location": "/References/Workflows/#linked-source-enable",
            "text": "",
            "title": "Linked Source Enable"
        },
        {
            "location": "/References/Workflows/#linked-source-disable",
            "text": "",
            "title": "Linked Source Disable"
        },
        {
            "location": "/References/Workflows/#linked-source-delete",
            "text": "",
            "title": "Linked Source Delete"
        },
        {
            "location": "/References/Workflows/#virtual-source-provision",
            "text": "",
            "title": "Virtual Source Provision"
        },
        {
            "location": "/References/Workflows/#virtual-source-snapshot",
            "text": "",
            "title": "Virtual Source Snapshot"
        },
        {
            "location": "/References/Workflows/#virtual-source-refresh",
            "text": "",
            "title": "Virtual Source Refresh"
        },
        {
            "location": "/References/Workflows/#virtual-source-rollback",
            "text": "",
            "title": "Virtual Source Rollback"
        },
        {
            "location": "/References/Workflows/#virtual-source-delete",
            "text": "",
            "title": "Virtual Source Delete"
        },
        {
            "location": "/References/Workflows/#virtual-source-start",
            "text": "",
            "title": "Virtual Source Start"
        },
        {
            "location": "/References/Workflows/#virtual-source-stop",
            "text": "",
            "title": "Virtual Source Stop"
        },
        {
            "location": "/References/Workflows/#virtual-source-enable",
            "text": "",
            "title": "Virtual Source Enable"
        },
        {
            "location": "/References/Workflows/#virtual-source-disable",
            "text": "",
            "title": "Virtual Source Disable"
        },
        {
            "location": "/References/Workflows/#upgrade",
            "text": "",
            "title": "Upgrade"
        },
        {
            "location": "/References/Classes/",
            "text": "Classes\n\u00b6\n\n\nDirectSource\n\u00b6\n\n\nRepresents a Linked Source object and its properties when using a \nDirect Linking\n strategy.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nDirectSource\n\n\n\ndirect_source\n \n=\n \nDirectSource\n(\nguid\n,\n \nconnection\n,\n \nparameters\n)\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nguid\n\n\nString\n\n\nUnique Identifier for the source.\n\n\n\n\n\n\nconnection\n\n\nRemoteConnection\n\n\nConnection for the source environment.\n\n\n\n\n\n\nparameters\n\n\nLinkedSourceDefinition\n\n\nUser input as per the \nLinkedSource Schema\n.\n\n\n\n\n\n\n\n\nStagedSource\n\u00b6\n\n\nRepresents a Linked Source object and its properties when using a \nStaged Linking\n strategy.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nStagedSource\n\n\n\nstaged_source\n \n=\n \nStagedSource\n(\nguid\n,\n \nsource_connection\n,\n \nparameters\n,\n \nmount\n,\n \nstaged_connection\n)\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nguid\n\n\nString\n\n\nUnique Identifier for the source.\n\n\n\n\n\n\nsource_connection\n\n\nRemoteConnection\n\n\nConnection for the source environment.\n\n\n\n\n\n\nparameters\n\n\nLinkedSourceDefinition\n\n\nUser input as per the \nLinkedSource Schema\n.\n\n\n\n\n\n\nmount\n\n\nMount\n\n\nMount point associated with the source.\n\n\n\n\n\n\nstaged_connection\n\n\nRemoteConnection\n\n\nConnection for the staging environment.\n\n\n\n\n\n\n\n\nVirtualSource\n\u00b6\n\n\nRepresents a Virtual Source object and its properties.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nVirtualSource\n\n\n\nvirtual_source\n \n=\n \nVirtualSource\n(\nguid\n,\n \nconnection\n,\n \nparameters\n,\n \nmounts\n)\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nguid\n\n\nString\n\n\nUnique Identifier for the source.\n\n\n\n\n\n\nconnection\n\n\nRemoteConnection\n\n\nConnection for the source environment.\n\n\n\n\n\n\nparameters\n\n\nVirtualSourceDefinition\n\n\nUser input as per the \nVirtualSource Schema\n.\n\n\n\n\n\n\nmounts\n\n\nlist[\nMount\n]\n\n\nMount points associated with the source.\n\n\n\n\n\n\n\n\nRemoteConnection\n\u00b6\n\n\nRepresents a connection to a source.\n\n\nfrom\n \ndlpx.virtualization.common\n \nimport\n \nRemoteConnection\n\n\n\nconnection\n \n=\n \nRemoteConnection\n(\nenvironment\n,\n \nuser\n)\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nenvironment\n\n\nRemoteEnvironment\n\n\nEnvironment for the connection.\n\n\n\n\n\n\nuser\n\n\nRemoteUser\n\n\nUser for the connection.\n\n\n\n\n\n\n\n\nStatus\n\u00b6\n\n\nAn enum used to represent the state of a linked or virtual source and whether it is functioning as expected.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nStatus\n\n\n\nstatus\n \n=\n \nStatus\n.\nACTIVE\n\n\n\n\n\n\nValues\n\u00b6\n\n\n\n\n\n\n\n\nValue\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nACTIVE\n\n\nSource is healthy and functioning as expected.\n\n\n\n\n\n\nINACTIVE\n\n\nSource is not functioning as expected.\n\n\n\n\n\n\n\n\nMount\n\u00b6\n\n\nRepresents a mount exported and mounted to a remote host.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nMount\n\n\n\nmount\n \n=\n \nMount\n(\nenvironment\n,\n \npath\n)\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nremote_environment\n\n\nRemoteEnvironment\n or \nReference\n\n\nEnvironment for the connection.\n\n\n\n\n\n\nmount_path\n\n\nString\n\n\nThe path on the remote host that has the mounted data set.\n\n\n\n\n\n\nshared_path\n\n\nString\n\n\nOptional.\n The path of the subdirectory of the data set to mount to the remote host.\n\n\n\n\n\n\n\n\nOwnershipSpecification\n\u00b6\n\n\nRepresents how to set the ownership for a data set. This only applies to Unix Hosts.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nOwnershipSpecification\n\n\n\nownership_specification\n \n=\n \nOwnershipSpecification\n(\nuid\n,\n \ngid\n)\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nuid\n\n\nInteger\n\n\nThe user id to set the ownership of the data set to.\n\n\n\n\n\n\ngid\n\n\nInteger\n\n\nThe group id to set the ownership of the data set to.\n\n\n\n\n\n\n\n\nMountSpecification\n\u00b6\n\n\nRepresents properties for the mount associated with an exported data set.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nMountSpecification\n\n\n\nmount_specification\n \n=\n \nMountSpecification\n([\nmount\n],\n \nownership_specification\n)\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmounts\n\n\nlist[\nMount\n]\n\n\nThe list of mounts to export the data sets to.\n\n\n\n\n\n\nownership_specification\n\n\nOwnershipSpecification\n\n\nOptional.\n Control the ownership attributes for the data set. It defaults to the environment user of the remote environment if it is not specified.\n\n\n\n\n\n\n\n\nSnapshotParametersDefinition\n\u00b6\n\n\nUser provided parameters for the snapshot operation. It includes a boolean property named \nresync\n that can be used to indicate to the plugin whether or not to initiate a full ingestion of the dSource. The parameters are only set during a manual snapshot. When using a sync policy, \nresync\n defaults to \nfalse\n.\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \nlinked_pre_snapshot\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n,\n \nsnapshot_parameters\n):\n\n  \nif\n \nsnapshot_parameter\n.\nresync\n:\n\n      \nprint\n(\nsnapshot_parameter\n.\nresync\n)\n\n\n\n\n\n\n\n\nThis class will be generated during build and is located with the autogenerated classes. As it is passed into the operation, importing it is not neccessary.\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nresync\n\n\nBoolean\n\n\nDetermines if this snapshot should ingest the dSource from scratch.\n\n\n\n\n\n\n\n\nRemoteEnvironment\n\u00b6\n\n\nRepresents a remote environment.\n\n\nfrom\n \ndlpx.virtualization.common\n \nimport\n \nRemoteEnvironment\n\n\n\nenvironment\n \n=\n \nRemoteEnvironment\n(\nname\n,\n \nreference\n,\n \nhost\n)\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nString\n\n\nName of the environment.\n\n\n\n\n\n\nreference\n\n\nString\n\n\nUnique identifier for the environment.\n\n\n\n\n\n\nhost\n\n\nRemoteHost\n\n\nHost that belongs to the environment.\n\n\n\n\n\n\n\n\nRemoteHost\n\u00b6\n\n\nRepresents a remote host, can be Unix or Windows.\n\n\nfrom\n \ndlpx.virtualization.common\n \nimport\n \nRemoteHost\n\n\n\nhost\n \n=\n \nRemoteHost\n(\nname\n,\n \nreference\n,\n \nbinary_path\n,\n \nscratch_path\n)\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nString\n\n\nHost address.\n\n\n\n\n\n\nreference\n\n\nString\n\n\nUnique identifier for the host.\n\n\n\n\n\n\nbinary_path\n\n\nString\n\n\nPath to Delphix provided binaries on the host, which are present in the toolkit pushed to the remote host like \ndlpx_db_exec\n, \ndlpx_pfexec\n, etc. This property is only available for Unix hosts.\n\n\n\n\n\n\nscratch_path\n\n\nString\n\n\nPath to scratch path on the host.\n\n\n\n\n\n\n\n\nRemoteUser\n\u00b6\n\n\nRepresents a user on a remote host.\n\n\nfrom\n \ndlpx.virtualization.common\n \nimport\n \nRemoteUser\n\n\n\nuser\n \n=\n \nRemoteUser\n(\nname\n,\n \nreference\n)\n\n\n\n\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nname\n\n\nString\n\n\nUser name.\n\n\n\n\n\n\nreference\n\n\nString\n\n\nUnique identifier for the user.",
            "title": "Classes"
        },
        {
            "location": "/References/Classes/#classes",
            "text": "",
            "title": "Classes"
        },
        {
            "location": "/References/Classes/#directsource",
            "text": "Represents a Linked Source object and its properties when using a  Direct Linking  strategy.  from   dlpx.virtualization.platform   import   DirectSource  direct_source   =   DirectSource ( guid ,   connection ,   parameters )",
            "title": "DirectSource"
        },
        {
            "location": "/References/Classes/#fields",
            "text": "Field  Type  Description      guid  String  Unique Identifier for the source.    connection  RemoteConnection  Connection for the source environment.    parameters  LinkedSourceDefinition  User input as per the  LinkedSource Schema .",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#stagedsource",
            "text": "Represents a Linked Source object and its properties when using a  Staged Linking  strategy.  from   dlpx.virtualization.platform   import   StagedSource  staged_source   =   StagedSource ( guid ,   source_connection ,   parameters ,   mount ,   staged_connection )",
            "title": "StagedSource"
        },
        {
            "location": "/References/Classes/#fields_1",
            "text": "Field  Type  Description      guid  String  Unique Identifier for the source.    source_connection  RemoteConnection  Connection for the source environment.    parameters  LinkedSourceDefinition  User input as per the  LinkedSource Schema .    mount  Mount  Mount point associated with the source.    staged_connection  RemoteConnection  Connection for the staging environment.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#virtualsource",
            "text": "Represents a Virtual Source object and its properties.  from   dlpx.virtualization.platform   import   VirtualSource  virtual_source   =   VirtualSource ( guid ,   connection ,   parameters ,   mounts )",
            "title": "VirtualSource"
        },
        {
            "location": "/References/Classes/#fields_2",
            "text": "Field  Type  Description      guid  String  Unique Identifier for the source.    connection  RemoteConnection  Connection for the source environment.    parameters  VirtualSourceDefinition  User input as per the  VirtualSource Schema .    mounts  list[ Mount ]  Mount points associated with the source.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#remoteconnection",
            "text": "Represents a connection to a source.  from   dlpx.virtualization.common   import   RemoteConnection  connection   =   RemoteConnection ( environment ,   user )",
            "title": "RemoteConnection"
        },
        {
            "location": "/References/Classes/#fields_3",
            "text": "Field  Type  Description      environment  RemoteEnvironment  Environment for the connection.    user  RemoteUser  User for the connection.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#status",
            "text": "An enum used to represent the state of a linked or virtual source and whether it is functioning as expected.  from   dlpx.virtualization.platform   import   Status  status   =   Status . ACTIVE",
            "title": "Status"
        },
        {
            "location": "/References/Classes/#values",
            "text": "Value  Description      ACTIVE  Source is healthy and functioning as expected.    INACTIVE  Source is not functioning as expected.",
            "title": "Values"
        },
        {
            "location": "/References/Classes/#mount",
            "text": "Represents a mount exported and mounted to a remote host.  from   dlpx.virtualization.platform   import   Mount  mount   =   Mount ( environment ,   path )",
            "title": "Mount"
        },
        {
            "location": "/References/Classes/#fields_4",
            "text": "Field  Type  Description      remote_environment  RemoteEnvironment  or  Reference  Environment for the connection.    mount_path  String  The path on the remote host that has the mounted data set.    shared_path  String  Optional.  The path of the subdirectory of the data set to mount to the remote host.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#ownershipspecification",
            "text": "Represents how to set the ownership for a data set. This only applies to Unix Hosts.  from   dlpx.virtualization.platform   import   OwnershipSpecification  ownership_specification   =   OwnershipSpecification ( uid ,   gid )",
            "title": "OwnershipSpecification"
        },
        {
            "location": "/References/Classes/#fields_5",
            "text": "Field  Type  Description      uid  Integer  The user id to set the ownership of the data set to.    gid  Integer  The group id to set the ownership of the data set to.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#mountspecification",
            "text": "Represents properties for the mount associated with an exported data set.  from   dlpx.virtualization.platform   import   MountSpecification  mount_specification   =   MountSpecification ([ mount ],   ownership_specification )",
            "title": "MountSpecification"
        },
        {
            "location": "/References/Classes/#fields_6",
            "text": "Field  Type  Description      mounts  list[ Mount ]  The list of mounts to export the data sets to.    ownership_specification  OwnershipSpecification  Optional.  Control the ownership attributes for the data set. It defaults to the environment user of the remote environment if it is not specified.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#snapshotparametersdefinition",
            "text": "User provided parameters for the snapshot operation. It includes a boolean property named  resync  that can be used to indicate to the plugin whether or not to initiate a full ingestion of the dSource. The parameters are only set during a manual snapshot. When using a sync policy,  resync  defaults to  false .  from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.linked.pre_snapshot ()  def   linked_pre_snapshot ( staged_source ,   repository ,   source_config ,   snapshot_parameters ): \n   if   snapshot_parameter . resync : \n       print ( snapshot_parameter . resync )    This class will be generated during build and is located with the autogenerated classes. As it is passed into the operation, importing it is not neccessary.",
            "title": "SnapshotParametersDefinition"
        },
        {
            "location": "/References/Classes/#fields_7",
            "text": "Field  Type  Description      resync  Boolean  Determines if this snapshot should ingest the dSource from scratch.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#remoteenvironment",
            "text": "Represents a remote environment.  from   dlpx.virtualization.common   import   RemoteEnvironment  environment   =   RemoteEnvironment ( name ,   reference ,   host )",
            "title": "RemoteEnvironment"
        },
        {
            "location": "/References/Classes/#fields_8",
            "text": "Field  Type  Description      name  String  Name of the environment.    reference  String  Unique identifier for the environment.    host  RemoteHost  Host that belongs to the environment.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#remotehost",
            "text": "Represents a remote host, can be Unix or Windows.  from   dlpx.virtualization.common   import   RemoteHost  host   =   RemoteHost ( name ,   reference ,   binary_path ,   scratch_path )",
            "title": "RemoteHost"
        },
        {
            "location": "/References/Classes/#fields_9",
            "text": "Field  Type  Description      name  String  Host address.    reference  String  Unique identifier for the host.    binary_path  String  Path to Delphix provided binaries on the host, which are present in the toolkit pushed to the remote host like  dlpx_db_exec ,  dlpx_pfexec , etc. This property is only available for Unix hosts.    scratch_path  String  Path to scratch path on the host.",
            "title": "Fields"
        },
        {
            "location": "/References/Classes/#remoteuser",
            "text": "Represents a user on a remote host.  from   dlpx.virtualization.common   import   RemoteUser  user   =   RemoteUser ( name ,   reference )",
            "title": "RemoteUser"
        },
        {
            "location": "/References/Classes/#fields_10",
            "text": "Field  Type  Description      name  String  User name.    reference  String  Unique identifier for the user.",
            "title": "Fields"
        },
        {
            "location": "/References/Glossary/",
            "text": "Glossary\n\u00b6\n\n\nArtifact\n\u00b6\n\n\nA single file that is the result of a \nbuild\n. It is this artifact which is distributed to users, and which is installed onto engines.\n\n\nAutomatic Discovery\n\u00b6\n\n\nDiscovery\n which is done by the Delphix Engine (with help from a plugin) itself, with no need for the end user to provide any information.\n\n\nBuilding\n\u00b6\n\n\nThe process of creating an \nartifact\n from the collection of files that make up the plugin's source code.\n\n\nData Migration\n\u00b6\n\n\nA python function which is called as part of the upgrade process. It handles transforming data from an older format to a newer format. More details \nhere\n.\n\n\nData Migration ID\n\u00b6\n\n\nEach data migration is tagged with a unique ID. This allows the Delphix Engine to know which data migrations need to be run, in which order, when upgrading to a new plugin version. More details \nhere\n.\n\n\nDecorator\n\u00b6\n\n\nA Python construct which is used by plugins to \"tag\" certain functions, so that the Delphix Engine knows which function corresponds to which plugin operation.\n\n\nDirect Linking\n\u00b6\n\n\nA strategy that involves data being ingested directly from the source environment onto the Delphix Engine, without the assistance of a staging environment.\n\n\nDiscovery\n\u00b6\n\n\nThe process by which the Delphix Engine learns about how a particular environment can be used for ingesting or virtualizing datasets.\n\n\ndSource\n\u00b6\n\n\nSee \nLinked Dataset\n\n\nEnvironment\n\u00b6\n\n\nA remote system that the Delphix Engine can interact with. An environment can be used as a \nsource\n, \nstaging\n or \ntarget\n environment (or any combination of those).  For example, a Linux machine that the Delphix Engine can connect to is an environment.\n\n\nEnvironment User\n\u00b6\n\n\nA set of user credentials that the Delphix Engine can use to interact with an \nEnvironmnet\n. For example, a username and password to login to a Linux machine.\n\n\nLinked Dataset\n\u00b6\n\n\nA dataset on the Delphix Engine which holds an ingested copy of a pre-existing external dataset from a source environment. A linked dataset is often called a \ndSource\n.\n\n\nLinked Source\n\u00b6\n\n\nAn object on the Delphix Engine that holds information related to a \nlinked dataset\n.\n\n\nLinking\n\u00b6\n\n\nThe process by which the Delphix Engine connects a new \ndSource\n to a pre-existing dataset on a source environment.\n\n\nLogging\n\u00b6\n\n\nLogging is when a plugin writes out some human-readable information to a log file. The log file can then be examined, typically in order to debug a problem with the plugin.\n\n\nPlugin Config\n\u00b6\n\n\nA \nYAML\n file containing a list of plugin properties: What is the plugin's name? What version of the plugin is this? Etc. More details \nhere\n.\n\n\nManual Discovery\n\u00b6\n\n\nDiscovery\n which the end user does by manually entering the necessary information into the Delphix Engine.\n\n\nMount Specification\n\u00b6\n\n\nA collection of information, provided by the plugin, which give all the details about how and where \nvirtual datasets\n should be mounted onto \ntarget environments\n. This term is often shortened to \"Mount Spec\".\n\n\nPassword Properties\n\u00b6\n\n\nIn \nschemas\n, any string property can be tagged with \n\"format\": \"password\"\n. This will let the Delphix Engine know that the property contains sensitive information. Any such values will only be stored in encrypted format, and the UI will not display the values on screen.\n\n\nPlatform Libraries\n\u00b6\n\n\nA set of Python functions that are provided by the Virtualization Platform. Plugins use these library functions to request that the Virtualization Platform do some task on behalf of the plugin. For example, running a Bash command on an environment, or making an log entry.\n\n\nPlugin\n\u00b6\n\n\nA tool that customizes the Delphix Engine so it knows how to interact with a particular kind of dataset.\n\n\nPlugin Operation\n\u00b6\n\n\nA piece of functionality that provided by a plugin in order to customize Delphix Engine behavior to work with a particular kind of dataset. A plugin operation is implemented as a Python function.\nFor example, a MySQL plugin might provide an operation called \"stop\" which knows how to stop a MySQL database.\n\n\nProvisioning\n\u00b6\n\n\nThe process of making a virtual copy of a dataset and making it available for use on a target environment.\n\n\nReplication\n\u00b6\n\n\nDelphix allows end users to replicate data objects between Delphix Engines by creating a replication profile. Data objects that belong to a plugin can also be part of the replication profile. Refer to the \nDelphix Engine Documentation\n for more details.\n\n\nRepository\n\u00b6\n\n\nInformation that represents a set of dependencies that a dataset requires in order to be functional. For example, a particular Postgres database might require an installed Postgres 9.6 DBMS, and so its associated repository would contain all the information required to interact with that DBMS.\n\n\nSchema\n\u00b6\n\n\nA formal description of a data type. Plugins use JSON format for their \nschemas\n.\n\n\nSnapshot\n\u00b6\n\n\nA point-in-time read-only copy of a dataset. A snapshot includes associated metadata represented by the \nSnapshotDefinition Schema\n\n\nSnapshot Parameter\n\u00b6\n\n\nUser provided parameters for the snapshot operation. Currently the only properties the parameter has is resync.\n\n\nSource Config\n\u00b6\n\n\nA collection of information that the Delphix Engine needs to interact with a dataset (whether \nlinked\n or \nvirtual\n on an \nenvironment\n.\n\n\nSource Environment\n\u00b6\n\n\nAn \nenvironment\n containing data that is ingested by the Delphix Engine.\n\n\nStaged Linking\n\u00b6\n\n\nA strategy where a \nstaging environment\n is used to coordinate the ingestion of data into a \ndsource\n.\n\n\nStaging Environment\n\u00b6\n\n\nAn \nenvironment\n used by the Delphix Engine to coordinate ingestion from a \nsource environment\n.\n\n\nSyncing\n\u00b6\n\n\nThe process by which the Delphix Engine ingests data from a dataset on a \nsource environment\n into a \ndsource\n. Syncing always happens immediately after \nlinking\n, and typically is done periodically thereafter.\n\n\nTarget Environment\n\u00b6\n\n\nAn \nenvironment\n on which Delphix-provided virtualized datasets can be used.\n\n\nUpgrade Operation\n\u00b6\n\n\nA special plugin operation that takes data produced by an older version of a plugin, and transforms it into the format expected by the new version of the plugin.\n\n\nVDB\n\u00b6\n\n\nSee \nVirtual Dataset\n\n\nVersion\n\u00b6\n\n\nA string identifier that is unique for every public release of a plugin.\n\n\nVirtual Dataset\n\u00b6\n\n\nA dataset that has been cloned from a snapshot, and whose data is stored on the Delphix Engine. A virtual dataset is made available for use by mounting it to a \ntarget environment\n. A virtual dataset is often called a \"VDB\".\n\n\nVirtual Source\n\u00b6\n\n\nAn object on the Delphix Engine that holds information related to a \nvirtual dataset\n.\n\n\nYAML\n\u00b6\n\n\nYAML is a simple language often used for configuration files. Plugins define their \nplugin config\n using YAML.",
            "title": "Glossary"
        },
        {
            "location": "/References/Glossary/#glossary",
            "text": "",
            "title": "Glossary"
        },
        {
            "location": "/References/Glossary/#artifact",
            "text": "A single file that is the result of a  build . It is this artifact which is distributed to users, and which is installed onto engines.",
            "title": "Artifact"
        },
        {
            "location": "/References/Glossary/#automatic-discovery",
            "text": "Discovery  which is done by the Delphix Engine (with help from a plugin) itself, with no need for the end user to provide any information.",
            "title": "Automatic Discovery"
        },
        {
            "location": "/References/Glossary/#building",
            "text": "The process of creating an  artifact  from the collection of files that make up the plugin's source code.",
            "title": "Building"
        },
        {
            "location": "/References/Glossary/#data-migration",
            "text": "A python function which is called as part of the upgrade process. It handles transforming data from an older format to a newer format. More details  here .",
            "title": "Data Migration"
        },
        {
            "location": "/References/Glossary/#data-migration-id",
            "text": "Each data migration is tagged with a unique ID. This allows the Delphix Engine to know which data migrations need to be run, in which order, when upgrading to a new plugin version. More details  here .",
            "title": "Data Migration ID"
        },
        {
            "location": "/References/Glossary/#decorator",
            "text": "A Python construct which is used by plugins to \"tag\" certain functions, so that the Delphix Engine knows which function corresponds to which plugin operation.",
            "title": "Decorator"
        },
        {
            "location": "/References/Glossary/#direct-linking",
            "text": "A strategy that involves data being ingested directly from the source environment onto the Delphix Engine, without the assistance of a staging environment.",
            "title": "Direct Linking"
        },
        {
            "location": "/References/Glossary/#discovery",
            "text": "The process by which the Delphix Engine learns about how a particular environment can be used for ingesting or virtualizing datasets.",
            "title": "Discovery"
        },
        {
            "location": "/References/Glossary/#dsource",
            "text": "See  Linked Dataset",
            "title": "dSource"
        },
        {
            "location": "/References/Glossary/#environment",
            "text": "A remote system that the Delphix Engine can interact with. An environment can be used as a  source ,  staging  or  target  environment (or any combination of those).  For example, a Linux machine that the Delphix Engine can connect to is an environment.",
            "title": "Environment"
        },
        {
            "location": "/References/Glossary/#environment-user",
            "text": "A set of user credentials that the Delphix Engine can use to interact with an  Environmnet . For example, a username and password to login to a Linux machine.",
            "title": "Environment User"
        },
        {
            "location": "/References/Glossary/#linked-dataset",
            "text": "A dataset on the Delphix Engine which holds an ingested copy of a pre-existing external dataset from a source environment. A linked dataset is often called a  dSource .",
            "title": "Linked Dataset"
        },
        {
            "location": "/References/Glossary/#linked-source",
            "text": "An object on the Delphix Engine that holds information related to a  linked dataset .",
            "title": "Linked Source"
        },
        {
            "location": "/References/Glossary/#linking",
            "text": "The process by which the Delphix Engine connects a new  dSource  to a pre-existing dataset on a source environment.",
            "title": "Linking"
        },
        {
            "location": "/References/Glossary/#logging",
            "text": "Logging is when a plugin writes out some human-readable information to a log file. The log file can then be examined, typically in order to debug a problem with the plugin.",
            "title": "Logging"
        },
        {
            "location": "/References/Glossary/#plugin-config",
            "text": "A  YAML  file containing a list of plugin properties: What is the plugin's name? What version of the plugin is this? Etc. More details  here .",
            "title": "Plugin Config"
        },
        {
            "location": "/References/Glossary/#manual-discovery",
            "text": "Discovery  which the end user does by manually entering the necessary information into the Delphix Engine.",
            "title": "Manual Discovery"
        },
        {
            "location": "/References/Glossary/#mount-specification",
            "text": "A collection of information, provided by the plugin, which give all the details about how and where  virtual datasets  should be mounted onto  target environments . This term is often shortened to \"Mount Spec\".",
            "title": "Mount Specification"
        },
        {
            "location": "/References/Glossary/#password-properties",
            "text": "In  schemas , any string property can be tagged with  \"format\": \"password\" . This will let the Delphix Engine know that the property contains sensitive information. Any such values will only be stored in encrypted format, and the UI will not display the values on screen.",
            "title": "Password Properties"
        },
        {
            "location": "/References/Glossary/#platform-libraries",
            "text": "A set of Python functions that are provided by the Virtualization Platform. Plugins use these library functions to request that the Virtualization Platform do some task on behalf of the plugin. For example, running a Bash command on an environment, or making an log entry.",
            "title": "Platform Libraries"
        },
        {
            "location": "/References/Glossary/#plugin",
            "text": "A tool that customizes the Delphix Engine so it knows how to interact with a particular kind of dataset.",
            "title": "Plugin"
        },
        {
            "location": "/References/Glossary/#plugin-operation",
            "text": "A piece of functionality that provided by a plugin in order to customize Delphix Engine behavior to work with a particular kind of dataset. A plugin operation is implemented as a Python function.\nFor example, a MySQL plugin might provide an operation called \"stop\" which knows how to stop a MySQL database.",
            "title": "Plugin Operation"
        },
        {
            "location": "/References/Glossary/#provisioning",
            "text": "The process of making a virtual copy of a dataset and making it available for use on a target environment.",
            "title": "Provisioning"
        },
        {
            "location": "/References/Glossary/#replication",
            "text": "Delphix allows end users to replicate data objects between Delphix Engines by creating a replication profile. Data objects that belong to a plugin can also be part of the replication profile. Refer to the  Delphix Engine Documentation  for more details.",
            "title": "Replication"
        },
        {
            "location": "/References/Glossary/#repository",
            "text": "Information that represents a set of dependencies that a dataset requires in order to be functional. For example, a particular Postgres database might require an installed Postgres 9.6 DBMS, and so its associated repository would contain all the information required to interact with that DBMS.",
            "title": "Repository"
        },
        {
            "location": "/References/Glossary/#schema",
            "text": "A formal description of a data type. Plugins use JSON format for their  schemas .",
            "title": "Schema"
        },
        {
            "location": "/References/Glossary/#snapshot",
            "text": "A point-in-time read-only copy of a dataset. A snapshot includes associated metadata represented by the  SnapshotDefinition Schema",
            "title": "Snapshot"
        },
        {
            "location": "/References/Glossary/#snapshot-parameter",
            "text": "User provided parameters for the snapshot operation. Currently the only properties the parameter has is resync.",
            "title": "Snapshot Parameter"
        },
        {
            "location": "/References/Glossary/#source-config",
            "text": "A collection of information that the Delphix Engine needs to interact with a dataset (whether  linked  or  virtual  on an  environment .",
            "title": "Source Config"
        },
        {
            "location": "/References/Glossary/#source-environment",
            "text": "An  environment  containing data that is ingested by the Delphix Engine.",
            "title": "Source Environment"
        },
        {
            "location": "/References/Glossary/#staged-linking",
            "text": "A strategy where a  staging environment  is used to coordinate the ingestion of data into a  dsource .",
            "title": "Staged Linking"
        },
        {
            "location": "/References/Glossary/#staging-environment",
            "text": "An  environment  used by the Delphix Engine to coordinate ingestion from a  source environment .",
            "title": "Staging Environment"
        },
        {
            "location": "/References/Glossary/#syncing",
            "text": "The process by which the Delphix Engine ingests data from a dataset on a  source environment  into a  dsource . Syncing always happens immediately after  linking , and typically is done periodically thereafter.",
            "title": "Syncing"
        },
        {
            "location": "/References/Glossary/#target-environment",
            "text": "An  environment  on which Delphix-provided virtualized datasets can be used.",
            "title": "Target Environment"
        },
        {
            "location": "/References/Glossary/#upgrade-operation",
            "text": "A special plugin operation that takes data produced by an older version of a plugin, and transforms it into the format expected by the new version of the plugin.",
            "title": "Upgrade Operation"
        },
        {
            "location": "/References/Glossary/#vdb",
            "text": "See  Virtual Dataset",
            "title": "VDB"
        },
        {
            "location": "/References/Glossary/#version",
            "text": "A string identifier that is unique for every public release of a plugin.",
            "title": "Version"
        },
        {
            "location": "/References/Glossary/#virtual-dataset",
            "text": "A dataset that has been cloned from a snapshot, and whose data is stored on the Delphix Engine. A virtual dataset is made available for use by mounting it to a  target environment . A virtual dataset is often called a \"VDB\".",
            "title": "Virtual Dataset"
        },
        {
            "location": "/References/Glossary/#virtual-source",
            "text": "An object on the Delphix Engine that holds information related to a  virtual dataset .",
            "title": "Virtual Source"
        },
        {
            "location": "/References/Glossary/#yaml",
            "text": "YAML is a simple language often used for configuration files. Plugins define their  plugin config  using YAML.",
            "title": "YAML"
        },
        {
            "location": "/Best_Practices/CLI_Configuration_File/",
            "text": "CLI Configuration File\n\u00b6\n\n\nThe CLI configuration file can be used to set default values for CLI command options.\n\n\nLocation\n\u00b6\n\n\nThe configuration file is located in the user's home directory under \n.dvp/config\n.\n\n\n<USER_HOME>\n    \u2514\u2500\u2500 .dvp\n        \u2514\u2500\u2500 config\n\n\n\n\n\nYour user's home directory will depend on the operating system, but can be referred to using \n~\n in Unix-based operating systems or \n%UserProfile%\n in Windows.\n\n\nSupported Options\n\u00b6\n\n\n\n\nUse \ndefault\n profile\n\n\nOnly the values listed in the \ndefault\n profile are used unless they are overridden by values passed in from a command line option with the same name.\n\n\n\n\nThe CLI configuration file supports the following options:\n\n\nengine\n\u00b6\n\n\nSpecifies the Delphix Engine which can be used as part of the \ndvp upload\n or \ndvp download-logs\n command.\n\n\nengine = engine.example.com\n\n\n\n\n\nuser\n\u00b6\n\n\nSpecifies the user to a Delphix Engine which is used as part of the \ndvp upload\n or \ndvp download-logs\n command.\n\n\nuser = admin\n\n\n\n\n\npassword\n\u00b6\n\n\nSpecifies the password for the user to a Delphix Engine which is used as part of the \ndvp upload\n or \ndvp download-logs\n command.\n\n\npassword = userpassword\n\n\n\n\n\nExample\n\u00b6\n\n\nThe following example uses all of the supported options for the CLI configuration file:\n\n\n[default]\nengine = engine.example.com\nuser = admin\npassword = userpassword",
            "title": "CLI Configuration File"
        },
        {
            "location": "/Best_Practices/CLI_Configuration_File/#cli-configuration-file",
            "text": "The CLI configuration file can be used to set default values for CLI command options.",
            "title": "CLI Configuration File"
        },
        {
            "location": "/Best_Practices/CLI_Configuration_File/#location",
            "text": "The configuration file is located in the user's home directory under  .dvp/config .  <USER_HOME>\n    \u2514\u2500\u2500 .dvp\n        \u2514\u2500\u2500 config  Your user's home directory will depend on the operating system, but can be referred to using  ~  in Unix-based operating systems or  %UserProfile%  in Windows.",
            "title": "Location"
        },
        {
            "location": "/Best_Practices/CLI_Configuration_File/#supported-options",
            "text": "Use  default  profile  Only the values listed in the  default  profile are used unless they are overridden by values passed in from a command line option with the same name.   The CLI configuration file supports the following options:",
            "title": "Supported Options"
        },
        {
            "location": "/Best_Practices/CLI_Configuration_File/#engine",
            "text": "Specifies the Delphix Engine which can be used as part of the  dvp upload  or  dvp download-logs  command.  engine = engine.example.com",
            "title": "engine"
        },
        {
            "location": "/Best_Practices/CLI_Configuration_File/#user",
            "text": "Specifies the user to a Delphix Engine which is used as part of the  dvp upload  or  dvp download-logs  command.  user = admin",
            "title": "user"
        },
        {
            "location": "/Best_Practices/CLI_Configuration_File/#password",
            "text": "Specifies the password for the user to a Delphix Engine which is used as part of the  dvp upload  or  dvp download-logs  command.  password = userpassword",
            "title": "password"
        },
        {
            "location": "/Best_Practices/CLI_Configuration_File/#example",
            "text": "The following example uses all of the supported options for the CLI configuration file:  [default]\nengine = engine.example.com\nuser = admin\npassword = userpassword",
            "title": "Example"
        },
        {
            "location": "/Best_Practices/Code_Sharing/",
            "text": "Code Sharing\n\u00b6\n\n\nAll Python modules inside of \nsrcDir\n can be imported just as they would be if the plugin was executing locally. When a plugin operation is executed \nsrcDir\n is the current working directory so all imports need to be relative to \nsrcDir\n regardless of the path of the module doing the import.\n\n\nPlease refer to Python's \ndocumentation on modules\n to learn more about modules and imports.\n\n\nExample\n\u00b6\n\n\nAssume we have the following file structure:\n\n\npostgres\n\u251c\u2500\u2500 plugin_config.yml\n\u251c\u2500\u2500 schema.json\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 operations\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 discovery.py\n    \u251c\u2500\u2500 plugin_runner.py\n    \u251c\u2500\u2500 resources\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 execute_sql.sh\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 list_installs.sh\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 list_schemas.sql\n    \u2514\u2500\u2500 utils\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 execution_util.py\n\n\n\n\n\nAny module in the plugin could import \nexecution_util.py\n with \nfrom utils import execution_util\n.\n\n\n\n\nGotcha\n\n\nSince the platform uses Python 2.7, every directory needs to have an \n__init__.py\n file in it otherwise the modules and resources in the folder will not be found at runtime. For more information on \n__init__.py\n files refer to Python's \ndocumentation on packages\n.\n\n\nNote that the \nsrcDir\n in the plugin config file (\nsrc\n in this example) does \nnot\n need an \n__init__.py\n file.\n\n\n\n\nAssume \nschema.json\n contains:\n\n\n{\n    \"repositoryDefinition\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": { \"type\": \"string\" }\n        },\n        \"nameField\": \"name\",\n        \"identityFields\": [\"name\"]\n    },\n    \"sourceConfigDefinition\": {\n        \"type\": \"object\",\n        \"required\": [\"name\"],\n        \"additionalProperties\": false,\n        \"properties\": {\n            \"name\": { \"type\": \"string\" }\n        },\n        \"nameField\": \"name\",\n        \"identityFields\": [\"name\"]\n    }\n}\n\n\n\n\n\nTo keep the code cleaner, this plugin does two things:\n\n\n\n\nSplits discovery logic into its own module: \ndiscovery.py\n.\n\n\nUses two helper funtions \nexecute_sql\n and \nexecute_shell\n in \nutils/execution_util.py\n to abstract all remote execution.\n\n\n\n\nplugin_runner.py\n\u00b6\n\n\nWhen the platform needs to execute a plugin operation, it always calls into the function decorated by the \nentryPoint\n object. The rest of the control flow is determined by the plugin. In order to split logic, the decorated function must delegate into the appropriate module. Below is an example of \nplugin_runner.py\n delegating into \ndiscovery.py\n to handle repository and source config discovery:\n\n\nfrom\n \noperations\n \nimport\n \ndiscovery\n\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n\n@plugin.discovery.repository\n()\n\n\ndef\n \nrepository_discovery\n(\nsource_connection\n):\n\n    \nreturn\n \ndiscovery\n.\nfind_installs\n(\nsource_connection\n);\n\n\n\n\n@plugin.discovery.source_config\n()\n\n\ndef\n \nsource_config_discovery\n(\nsource_connection\n,\n \nrepository\n):\n\n    \nreturn\n \ndiscovery\n.\nfind_schemas\n(\nsource_connection\n,\n \nrepository\n)\n\n\n\n\n\n\n\n\nNote\n\n\ndiscovery.py\n is in the \noperations\n package so it is imported with \nfrom operations import discovery\n.\n\n\n\n\ndiscovery.py\n\u00b6\n\n\nIn \ndiscovery.py\n the plugin delegates even further to split business logic away from remote execution. \nutils/execution_util.py\n deals with remote execution and error handling so \ndiscovery.py\n can focus on business logic. Note that \ndiscovery.py\n still needs to know the format of the return value from each script.\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nRepositoryDefinition\n,\n \nSourceConfigDefinition\n\n\nfrom\n \nutils\n \nimport\n \nexecution_util\n\n\n\n\ndef\n \nfind_installs\n(\nsource_connection\n):\n\n    \ninstalls\n \n=\n \nexecution_util\n.\nexecute_shell\n(\nsource_connection\n,\n \n'list_installs.sh'\n)\n\n\n    \n# Assume 'installs' is a comma separated list of the names of Postgres installations.\n\n    \ninstall_names\n \n=\n \ninstalls\n.\nsplit\n(\n','\n)\n\n    \nreturn\n \n[\nRepositoryDefinition\n(\nname\n=\nname\n)\n \nfor\n \nname\n \nin\n \ninstall_names\n]\n\n\n\n\ndef\n \nfind_schemas\n(\nsource_connection\n,\n \nrepository\n):\n\n    \nschemas\n \n=\n \nexecution_util\n.\nexecute_sql\n(\nsource_connection\n,\n \nrepository\n.\nname\n,\n \n'list_schemas.sql'\n)\n\n\n    \n# Assume 'schemas' is a comma separated list of the schema names.\n\n    \nschema_names\n \n=\n \nschemas\n.\nsplit\n(\n','\n)\n\n    \nreturn\n \n[\nSourceConfigDefinition\n(\nname\n=\nname\n)\n \nfor\n \nname\n \nin\n \nschema_names\n]\n\n\n\n\n\n\n\n\nNote\n\n\nEven though \ndiscovery.py\n is in the \noperations\n package, the import for \nexecution_util\n is still relative to the \nsrcDir\n specified in the plugin config file. \nexecution_util\n is in the \nutils\n package so it is imported with \nfrom utils import execution_util\n. \n\n\n\n\nexecution_util.py\n\u00b6\n\n\nexecution_util.py\n has two methods \nexecute_sql\n and \nexecute_shell\n. \nexecute_sql\n takes the name of a SQL script in \nresources/\n and executes it with \nresources/execute_sql.sh\n. \nexecute_shell\n takes the name of a shell script in \nresources/\n and executes it.\n\n\nimport\n \npkgutil\n\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\n\ndef\n \nexecute_sql\n(\nsource_connection\n,\n \ninstall_name\n,\n \nscript_name\n):\n\n    \npsql_script\n \n=\n \npkgutil\n.\nget_data\n(\n\"resources\"\n,\n \n\"execute_sql.sh\"\n)\n\n    \nsql_script\n \n=\n \npkgutil\n.\nget_data\n(\n\"resources\"\n,\n \nscript_name\n)\n\n\n    \nresult\n \n=\n \nlibs\n.\nrun_bash\n(\n\n        \nsource_connection\n,\n \npsql_script\n,\n \nvariables\n=\n{\n\"SCRIPT\"\n:\n \nsql_script\n},\n \ncheck\n=\nTrue\n\n    \n)\n\n    \nreturn\n \nresult\n.\nstdout\n\n\n\n\ndef\n \nexecute_shell\n(\nsource_connection\n,\n \nscript_name\n):\n\n    \nscript\n \n=\n \npkgutil\n.\nget_data\n(\n\"resources\"\n,\n \nscript_name\n)\n\n\n    \nresult\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \nscript\n,\n \ncheck\n=\nTrue\n)\n\n    \nreturn\n \nresult\n.\nstdout\n\n\n\n\n\n\n\n\nNote\n\n\nBoth \nexecute_sql\n and \nexecute_shell\n use the \ncheck\n parameter which will cause an error to be raised if the exit code is non-zero. For more information refer to the \nrun_bash\n \ndocumentation\n.",
            "title": "Code Sharing"
        },
        {
            "location": "/Best_Practices/Code_Sharing/#code-sharing",
            "text": "All Python modules inside of  srcDir  can be imported just as they would be if the plugin was executing locally. When a plugin operation is executed  srcDir  is the current working directory so all imports need to be relative to  srcDir  regardless of the path of the module doing the import.  Please refer to Python's  documentation on modules  to learn more about modules and imports.",
            "title": "Code Sharing"
        },
        {
            "location": "/Best_Practices/Code_Sharing/#example",
            "text": "Assume we have the following file structure:  postgres\n\u251c\u2500\u2500 plugin_config.yml\n\u251c\u2500\u2500 schema.json\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 operations\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 discovery.py\n    \u251c\u2500\u2500 plugin_runner.py\n    \u251c\u2500\u2500 resources\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 execute_sql.sh\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 list_installs.sh\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 list_schemas.sql\n    \u2514\u2500\u2500 utils\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 execution_util.py  Any module in the plugin could import  execution_util.py  with  from utils import execution_util .   Gotcha  Since the platform uses Python 2.7, every directory needs to have an  __init__.py  file in it otherwise the modules and resources in the folder will not be found at runtime. For more information on  __init__.py  files refer to Python's  documentation on packages .  Note that the  srcDir  in the plugin config file ( src  in this example) does  not  need an  __init__.py  file.   Assume  schema.json  contains:  {\n    \"repositoryDefinition\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": { \"type\": \"string\" }\n        },\n        \"nameField\": \"name\",\n        \"identityFields\": [\"name\"]\n    },\n    \"sourceConfigDefinition\": {\n        \"type\": \"object\",\n        \"required\": [\"name\"],\n        \"additionalProperties\": false,\n        \"properties\": {\n            \"name\": { \"type\": \"string\" }\n        },\n        \"nameField\": \"name\",\n        \"identityFields\": [\"name\"]\n    }\n}  To keep the code cleaner, this plugin does two things:   Splits discovery logic into its own module:  discovery.py .  Uses two helper funtions  execute_sql  and  execute_shell  in  utils/execution_util.py  to abstract all remote execution.",
            "title": "Example"
        },
        {
            "location": "/Best_Practices/Code_Sharing/#plugin_runnerpy",
            "text": "When the platform needs to execute a plugin operation, it always calls into the function decorated by the  entryPoint  object. The rest of the control flow is determined by the plugin. In order to split logic, the decorated function must delegate into the appropriate module. Below is an example of  plugin_runner.py  delegating into  discovery.py  to handle repository and source config discovery:  from   operations   import   discovery  from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.discovery.repository ()  def   repository_discovery ( source_connection ): \n     return   discovery . find_installs ( source_connection );  @plugin.discovery.source_config ()  def   source_config_discovery ( source_connection ,   repository ): \n     return   discovery . find_schemas ( source_connection ,   repository )    Note  discovery.py  is in the  operations  package so it is imported with  from operations import discovery .",
            "title": "plugin_runner.py"
        },
        {
            "location": "/Best_Practices/Code_Sharing/#discoverypy",
            "text": "In  discovery.py  the plugin delegates even further to split business logic away from remote execution.  utils/execution_util.py  deals with remote execution and error handling so  discovery.py  can focus on business logic. Note that  discovery.py  still needs to know the format of the return value from each script.  from   dlpx.virtualization   import   libs  from   generated.definitions   import   RepositoryDefinition ,   SourceConfigDefinition  from   utils   import   execution_util  def   find_installs ( source_connection ): \n     installs   =   execution_util . execute_shell ( source_connection ,   'list_installs.sh' ) \n\n     # Assume 'installs' is a comma separated list of the names of Postgres installations. \n     install_names   =   installs . split ( ',' ) \n     return   [ RepositoryDefinition ( name = name )   for   name   in   install_names ]  def   find_schemas ( source_connection ,   repository ): \n     schemas   =   execution_util . execute_sql ( source_connection ,   repository . name ,   'list_schemas.sql' ) \n\n     # Assume 'schemas' is a comma separated list of the schema names. \n     schema_names   =   schemas . split ( ',' ) \n     return   [ SourceConfigDefinition ( name = name )   for   name   in   schema_names ]    Note  Even though  discovery.py  is in the  operations  package, the import for  execution_util  is still relative to the  srcDir  specified in the plugin config file.  execution_util  is in the  utils  package so it is imported with  from utils import execution_util .",
            "title": "discovery.py"
        },
        {
            "location": "/Best_Practices/Code_Sharing/#execution_utilpy",
            "text": "execution_util.py  has two methods  execute_sql  and  execute_shell .  execute_sql  takes the name of a SQL script in  resources/  and executes it with  resources/execute_sql.sh .  execute_shell  takes the name of a shell script in  resources/  and executes it.  import   pkgutil  from   dlpx.virtualization   import   libs  def   execute_sql ( source_connection ,   install_name ,   script_name ): \n     psql_script   =   pkgutil . get_data ( \"resources\" ,   \"execute_sql.sh\" ) \n     sql_script   =   pkgutil . get_data ( \"resources\" ,   script_name ) \n\n     result   =   libs . run_bash ( \n         source_connection ,   psql_script ,   variables = { \"SCRIPT\" :   sql_script },   check = True \n     ) \n     return   result . stdout  def   execute_shell ( source_connection ,   script_name ): \n     script   =   pkgutil . get_data ( \"resources\" ,   script_name ) \n\n     result   =   libs . run_bash ( source_connection ,   script ,   check = True ) \n     return   result . stdout    Note  Both  execute_sql  and  execute_shell  use the  check  parameter which will cause an error to be raised if the exit code is non-zero. For more information refer to the  run_bash   documentation .",
            "title": "execution_util.py"
        },
        {
            "location": "/Best_Practices/Managing_Scripts_For_Remote_Execution/",
            "text": "Managing Scripts for Remote Execution\n\u00b6\n\n\nTo execute a PowerShell or Bash script or Expect script on a remote host, you must provide the script as a string to \nrun_powershell\n or \nrun_bash\n or \nrun_expect\n. While you can keep these strings as literals in your Python code, best practice is to keep them as resource files in your source directory and access them with \npkgutil\n.\n\n\npkgutil\n is part of the standard Python library. The method that is applicable to resources is \npkgutil.get_data\n.\n\n\nBasic Usage\n\u00b6\n\n\nGiven the following plugin structure:\n\n\n\u251c\u2500\u2500 plugin_config.yml\n\u251c\u2500\u2500 schema.json\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 plugin_runner.py\n    \u2514\u2500\u2500 resources\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 get_date.sh\n\n\n\n\n\nAssume \nSnapshotDefinition\n is:\n\n\n\"snapshotDefinition\": {\n    \"type\" : \"object\",\n    \"additionalProperties\" : false,\n    \"properties\" : {\n        \"name\": {\"type\": \"string\"},\n        \"date\": {\"type\": \"string\"}\n    }\n}\n\n\n\n\n\nand \nsrc/resources/get_date.sh\n contains:\n\n\n1\n2\n#!/usr/bin/env bash\n\ndate\n\n\n\n\n\n\nIf \nget_date.sh\n is needed in \npost_snapshot\n, it can be retrieved and executed:\n\n\nimport\n \npkgutil\n\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ndlpx.virtualization.platform.exceptions\n \nimport\n \nUserError\n\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nSnapshotDefinition\n\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.post_snapshot\n()\n\n\ndef\n \npost_snapshot\n(\ndirect_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n    \n# Retrieve script contents\n\n    \nscript_content\n \n=\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'get_date.sh'\n)\n\n\n    \n# Execute script on remote host\n\n    \nresponse\n \n=\n \nlibs\n.\nrun_bash\n(\ndirect_source\n.\nconnection\n,\n \nscript_content\n)\n\n\n    \n# Fail operation if the timestamp couldn't be retrieved\n\n    \nif\n \nresponse\n.\nexit_code\n \n!=\n \n0\n:\n\n        \nraise\n \nUserError\n(\n\n        \n'Failed to get date'\n,\n\n        \n'Make sure the user has the required permissions'\n,\n\n        \n'{}\n\\n\n{}'\n.\nformat\n(\nresponse\n.\nstdout\n,\n \nrsponse\n.\nstderr\n))\n\n\n    \nreturn\n \nSnapshotDefinition\n(\nname\n=\n'Snapshot'\n,\n \ndate\n=\nresponse\n.\nstdout\n)\n\n\n\n\n\n\n\n\nPython's Working Directory\n\n\nThis assumes that \nsrc/\n is Python's current working directory. This is the behavior of the Virtualization Platform.\n\n\n\n\n\n\nResources need to be in a Python module\n\n\npkgutil.get_data\n cannot retrieve the contents of a resource that is not in a Python package. This means that a resource that is in the first level of your source directory will not be retrievable with \npkgutil\n. Resources must be in a subdirectory of your source directory, and that subdirectory must contain an \n__init__.py\n file.\n\n\n\n\nMulti-level Packages\n\u00b6\n\n\nGiven the following plugin structure:\n\n\n\u251c\u2500\u2500 plugin_config.yml\n\u251c\u2500\u2500 schema.json\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 plugin_runner.py\n    \u2514\u2500\u2500 resources\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 database\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 execute_sql.sh\n        \u2514\u2500\u2500 platform\n            \u251c\u2500\u2500 __init__.py\n            \u2514\u2500\u2500 get_date.sh\n\n\n\n\n\nThe contents of \nsrc/resources/platform/get_date.sh\n can be retrieved with:\n\n\nscript_content\n \n=\n \npkgutil\n.\nget_data\n(\n'resources.platform'\n,\n \n'get_date.sh'\n)",
            "title": "Managing Scripts for Remote Execution"
        },
        {
            "location": "/Best_Practices/Managing_Scripts_For_Remote_Execution/#managing-scripts-for-remote-execution",
            "text": "To execute a PowerShell or Bash script or Expect script on a remote host, you must provide the script as a string to  run_powershell  or  run_bash  or  run_expect . While you can keep these strings as literals in your Python code, best practice is to keep them as resource files in your source directory and access them with  pkgutil .  pkgutil  is part of the standard Python library. The method that is applicable to resources is  pkgutil.get_data .",
            "title": "Managing Scripts for Remote Execution"
        },
        {
            "location": "/Best_Practices/Managing_Scripts_For_Remote_Execution/#basic-usage",
            "text": "Given the following plugin structure:  \u251c\u2500\u2500 plugin_config.yml\n\u251c\u2500\u2500 schema.json\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 plugin_runner.py\n    \u2514\u2500\u2500 resources\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 get_date.sh  Assume  SnapshotDefinition  is:  \"snapshotDefinition\": {\n    \"type\" : \"object\",\n    \"additionalProperties\" : false,\n    \"properties\" : {\n        \"name\": {\"type\": \"string\"},\n        \"date\": {\"type\": \"string\"}\n    }\n}  and  src/resources/get_date.sh  contains:  1\n2 #!/usr/bin/env bash \ndate   If  get_date.sh  is needed in  post_snapshot , it can be retrieved and executed:  import   pkgutil  from   dlpx.virtualization   import   libs  from   dlpx.virtualization.platform   import   Plugin  from   dlpx.virtualization.platform.exceptions   import   UserError  from   generated.definitions   import   SnapshotDefinition  plugin   =   Plugin ()  @plugin.linked.post_snapshot ()  def   post_snapshot ( direct_source ,   repository ,   source_config ): \n     # Retrieve script contents \n     script_content   =   pkgutil . get_data ( 'resources' ,   'get_date.sh' ) \n\n     # Execute script on remote host \n     response   =   libs . run_bash ( direct_source . connection ,   script_content ) \n\n     # Fail operation if the timestamp couldn't be retrieved \n     if   response . exit_code   !=   0 : \n         raise   UserError ( \n         'Failed to get date' , \n         'Make sure the user has the required permissions' , \n         '{} \\n {}' . format ( response . stdout ,   rsponse . stderr )) \n\n     return   SnapshotDefinition ( name = 'Snapshot' ,   date = response . stdout )    Python's Working Directory  This assumes that  src/  is Python's current working directory. This is the behavior of the Virtualization Platform.    Resources need to be in a Python module  pkgutil.get_data  cannot retrieve the contents of a resource that is not in a Python package. This means that a resource that is in the first level of your source directory will not be retrievable with  pkgutil . Resources must be in a subdirectory of your source directory, and that subdirectory must contain an  __init__.py  file.",
            "title": "Basic Usage"
        },
        {
            "location": "/Best_Practices/Managing_Scripts_For_Remote_Execution/#multi-level-packages",
            "text": "Given the following plugin structure:  \u251c\u2500\u2500 plugin_config.yml\n\u251c\u2500\u2500 schema.json\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 plugin_runner.py\n    \u2514\u2500\u2500 resources\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 database\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 execute_sql.sh\n        \u2514\u2500\u2500 platform\n            \u251c\u2500\u2500 __init__.py\n            \u2514\u2500\u2500 get_date.sh  The contents of  src/resources/platform/get_date.sh  can be retrieved with:  script_content   =   pkgutil . get_data ( 'resources.platform' ,   'get_date.sh' )",
            "title": "Multi-level Packages"
        },
        {
            "location": "/Best_Practices/User_Visible_Errors/",
            "text": "User Visible Errors\n\u00b6\n\n\nPlugin authors can choose to fail a plugin operation by raising an exception of type \nUserError\n with a custom message, action and output for the end user.\n\n\nFields\n\u00b6\n\n\n\n\n\n\n\n\nField\n\n\nType\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nmessage\n\n\nString\n\n\nDescription of the failure to show the end user.\n\n\n\n\n\n\naction\n\n\nString\n\n\nOptional\n. List of actions that the end user could take to fix the problem. If not provided, it defaults to \nContact the plugin author to correct the error.\n\n\n\n\n\n\noutput\n\n\nString\n\n\nOptional\n. Output or stack trace from the failure to give the end user more information so that they can self diagnose. If not provided, it defaults to the stack trace of the failure.\n\n\n\n\n\n\n\n\nExample\n\u00b6\n\n\nimport\n \npkgutil\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nSourceConfigDefinition\n\n\nfrom\n \ndlpx.virtualization.platform.exceptions\n \nimport\n \nUserError\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.start\n()\n\n\ndef\n \nstart\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \nscript_content\n \n=\n \npkgutil\n.\nget_data\n(\n'resources'\n,\n \n'start_database.sh'\n)\n\n\n  \nresponse\n \n=\n \nlibs\n.\nrun_bash\n(\nvirtual_source\n.\nconnection\n,\n \nscript_content\n)\n\n\n  \n# Fail operation if the database could not be started\n\n  \nif\n \nresponse\n.\nexit_code\n \n!=\n \n0\n:\n\n    \nraise\n \nUserError\n(\n\n    \n'Failed to start the database'\n,\n\n    \n'Make sure the user has appropriate permissions'\n,\n\n    \n'{}\n\\n\n{}'\n.\nformat\n(\nresponse\n.\nstdout\n,\n \nresponse\n.\nstderr\n))\n\n\n\n\n\n\nThe UI would show the end user if the plugin operation above fails:",
            "title": "User Visible Errors"
        },
        {
            "location": "/Best_Practices/User_Visible_Errors/#user-visible-errors",
            "text": "Plugin authors can choose to fail a plugin operation by raising an exception of type  UserError  with a custom message, action and output for the end user.",
            "title": "User Visible Errors"
        },
        {
            "location": "/Best_Practices/User_Visible_Errors/#fields",
            "text": "Field  Type  Description      message  String  Description of the failure to show the end user.    action  String  Optional . List of actions that the end user could take to fix the problem. If not provided, it defaults to  Contact the plugin author to correct the error.    output  String  Optional . Output or stack trace from the failure to give the end user more information so that they can self diagnose. If not provided, it defaults to the stack trace of the failure.",
            "title": "Fields"
        },
        {
            "location": "/Best_Practices/User_Visible_Errors/#example",
            "text": "import   pkgutil  from   dlpx.virtualization.platform   import   Plugin  from   generated.definitions   import   SourceConfigDefinition  from   dlpx.virtualization.platform.exceptions   import   UserError  plugin   =   Plugin ()  @plugin.virtual.start ()  def   start ( virtual_source ,   repository ,   source_config ): \n   script_content   =   pkgutil . get_data ( 'resources' ,   'start_database.sh' ) \n\n   response   =   libs . run_bash ( virtual_source . connection ,   script_content ) \n\n   # Fail operation if the database could not be started \n   if   response . exit_code   !=   0 : \n     raise   UserError ( \n     'Failed to start the database' , \n     'Make sure the user has appropriate permissions' , \n     '{} \\n {}' . format ( response . stdout ,   response . stderr ))   The UI would show the end user if the plugin operation above fails:",
            "title": "Example"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/",
            "text": "Dealing With Sensitive Data\n\u00b6\n\n\nOften, a plugin will need to handle sensitive user-provided data. The most common example of this is a database password.\n\n\nPlugins must be careful to handle sensitive data appropriately. Three tips for handling sensitive data are:\n\n\n\n\nTell the Delphix Engine which parts of your data are sensitive.\n\n\nWhen passing sensitive data to remote plugin library functions (such as \nrun_bash\n), use environment variables.\n\n\nAvoid logging, or otherwise writing out the sensitive data.\n\n\n\n\nEach of these tips are explained below.\n\n\nMarking Your Data As Sensitive\n\u00b6\n\n\nBecause the Delphix Engine manages the storing and retrieving of plugin-defined data, it needs to know which pieces of data are sensitive. The plugin does this in its \nschemas\n, by using the special \npassword\n keyword.\n\n\nThe following example of a schema defines an object with three properties, one of which is sensitive and tagged with the \npassword\n keyword:\n\n\n{\n\n    \n\"type\"\n:\n \n\"object\"\n,\n\n    \n\"properties\"\n:\n \n{\n\n        \n\"db_connectionPort\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n},\n\n        \n\"db_username\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n},\n\n        \n\"db_password\"\n:\n \n{\n\"type\"\n:\n \n\"string\"\n,\n \n\"format\"\n:\n \n\"password\"\n}\n\n    \n}\n\n\n}\n\n\n\n\n\n\nThis tells the Delphix Engine to take special precautions with this password property, as follows:\n\n\n\n\nThe Delphix Engine will encrypt the password before storing it, and decrypt it only as necessary to pass back to the plugin.\n\n\nThe Delphix Engine will not write this password anywhere (for example, it will not appear in any system logs).\n\n\nThe Delphix Engine's UI and CLI will not display the password.\n\n\nClients of the Delphix Engine's public API will not be able to access the password.\n\n\n\n\nUsing Environment Variables For Remote Data Passing\n\u00b6\n\n\nSometimes, a plugin will need to pass sensitive data to a remote environment. For example, perhaps a database command needs to be run on a \nstaging environment\n, and that database command will need to use a password.\n\n\nExample\n\u00b6\n\n\nLet us take a look at a very simple example where we need to shutdown a database called \"inventory\" on a target environment by using the \ndb_cmd shutdown inventory\n command. This command will ask for a password on \nstdin\n, and for our example our password is \"hunter2\".\n\n\nIf we were running this command by hand, it might look like this:\n\n\n$ db_cmd shutdown inventory\nConnecting to database instance...\nPlease enter database password:\n\n\n\n\n\nAt this point, we would type in \"hunter2\", and the command would proceed to shut down the database.\n\n\nSince a plugin cannot type in the password by hand, it will do something like this instead:\n\n\n$ \necho\n \n\"hunter2\"\n \n|\n db_cmd shutdown inventory\n\n\n\n\n\nDon't Do This\n\u00b6\n\n\nFirst, let us take a look at how \nnot\n to do this! Here is a bit of plugin python code that will run the above command.\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.stop\n()\n\n\ndef\n \nmy_virtual_stop\n(\nvirtual_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \n# THIS IS INSECURE! DO NOT DO THIS!\n\n  \nfull_command\n \n=\n \n\"echo {} | db_cmd shutdown {}\"\n.\nformat\n(\npassword\n,\n \ndb_name\n)\n\n  \nlibs\n.\nrun_bash\n(\nvirtual_source\n.\nconnection\n,\n \nfull_command\n)\n\n\n\n\n\n\nThis constructs a Python string containing exactly the desired command from above. However, this is not recommended.\n\n\nThe problem here is that there is a cleartext password in the Python string. But, this Python string is not treated as sensitive by the Virtualization Platform. For example, suppose the Virtualization Platform cannot make a connection to the target environment. In which case, it will raise an error containing the Python string, so that people will know what command failed. But, in our example, that would result in the password being part of the cleartext error message.\n\n\nUsing Environment Variables\n\u00b6\n\n\nThe Delphix Engine provides a better way to pass sensitive data to remote bash (or powershell) calls: environment variables. Let us look at a different way to run the same command as above.\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.virtual.stop\n()\n\n  \n# Use environment variables to pass sensitive data to remote commands\n\n  \nenvironment_vars\n \n=\n \n{\n\n    \n\"DATABASE_PASSWORD\"\n \n:\n \npassword\n\n  \n}\n\n  \nfull_command\n \n=\n \n\"echo $DATABASE_PASSWORD | db_cmd shutdown {}\"\n.\nformat\n(\ndb_name\n)\n\n  \nlibs\n.\nrun_bash\n(\nvirtual_source\n.\nconnection\n,\n \nfull_command\n,\n \nvariables\n=\nenvironment_vars\n)\n\n\n\n\n\n\n\n\nNote\n\n\nWe are no longer putting the cleartext password into the Python command string. Instead, we are instructing the Virtualization Platform to put the password into an environment variable on the target environment. The Python command string merely mentions the name of the environment variable, and does not contain the password itself.\n\n\n\n\nOnce the command runs on the target environment, Bash will substitute in the password, and the database shutdown will run as expected.\n\n\nUnlike with the command string, the Virtualization Platform \ndoes\n treat environment variables as sensitive information, and will not include them in error messages or internal logs, etc.\n\n\nDon't Write Out Sensitive Data\n\u00b6\n\n\nPlugin writers are strongly advised to never write out unencrypted sensitive data. This is common-sense general advice that applies to all areas of programming, not just for plugins. However, there are a couple of special concerns for plugins.\n\n\nThe Virtualization Platform provides logging capabilities to plugins. The generated logs are unencrypted and not treated as sensitive. Therefore, it is important for plugins to \nnever log sensitive data\n.\n\n\nIn addition, remember that your plugin is not treated as sensitive by the Virtualization Platform. Plugin code is distributed unencrypted, and is viewable in cleartext by Delphix Engine users. Sensitive data such as passwords should never be hard-coded in your plugin code.",
            "title": "Dealing With Sensitive Data"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#dealing-with-sensitive-data",
            "text": "Often, a plugin will need to handle sensitive user-provided data. The most common example of this is a database password.  Plugins must be careful to handle sensitive data appropriately. Three tips for handling sensitive data are:   Tell the Delphix Engine which parts of your data are sensitive.  When passing sensitive data to remote plugin library functions (such as  run_bash ), use environment variables.  Avoid logging, or otherwise writing out the sensitive data.   Each of these tips are explained below.",
            "title": "Dealing With Sensitive Data"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#marking-your-data-as-sensitive",
            "text": "Because the Delphix Engine manages the storing and retrieving of plugin-defined data, it needs to know which pieces of data are sensitive. The plugin does this in its  schemas , by using the special  password  keyword.  The following example of a schema defines an object with three properties, one of which is sensitive and tagged with the  password  keyword:  { \n     \"type\" :   \"object\" , \n     \"properties\" :   { \n         \"db_connectionPort\" :   { \"type\" :   \"string\" }, \n         \"db_username\" :   { \"type\" :   \"string\" }, \n         \"db_password\" :   { \"type\" :   \"string\" ,   \"format\" :   \"password\" } \n     }  }   This tells the Delphix Engine to take special precautions with this password property, as follows:   The Delphix Engine will encrypt the password before storing it, and decrypt it only as necessary to pass back to the plugin.  The Delphix Engine will not write this password anywhere (for example, it will not appear in any system logs).  The Delphix Engine's UI and CLI will not display the password.  Clients of the Delphix Engine's public API will not be able to access the password.",
            "title": "Marking Your Data As Sensitive"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#using-environment-variables-for-remote-data-passing",
            "text": "Sometimes, a plugin will need to pass sensitive data to a remote environment. For example, perhaps a database command needs to be run on a  staging environment , and that database command will need to use a password.",
            "title": "Using Environment Variables For Remote Data Passing"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#example",
            "text": "Let us take a look at a very simple example where we need to shutdown a database called \"inventory\" on a target environment by using the  db_cmd shutdown inventory  command. This command will ask for a password on  stdin , and for our example our password is \"hunter2\".  If we were running this command by hand, it might look like this:  $ db_cmd shutdown inventory\nConnecting to database instance...\nPlease enter database password:  At this point, we would type in \"hunter2\", and the command would proceed to shut down the database.  Since a plugin cannot type in the password by hand, it will do something like this instead:  $  echo   \"hunter2\"   |  db_cmd shutdown inventory",
            "title": "Example"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#dont-do-this",
            "text": "First, let us take a look at how  not  to do this! Here is a bit of plugin python code that will run the above command.  from   dlpx.virtualization   import   libs  from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.virtual.stop ()  def   my_virtual_stop ( virtual_source ,   repository ,   source_config ): \n   # THIS IS INSECURE! DO NOT DO THIS! \n   full_command   =   \"echo {} | db_cmd shutdown {}\" . format ( password ,   db_name ) \n   libs . run_bash ( virtual_source . connection ,   full_command )   This constructs a Python string containing exactly the desired command from above. However, this is not recommended.  The problem here is that there is a cleartext password in the Python string. But, this Python string is not treated as sensitive by the Virtualization Platform. For example, suppose the Virtualization Platform cannot make a connection to the target environment. In which case, it will raise an error containing the Python string, so that people will know what command failed. But, in our example, that would result in the password being part of the cleartext error message.",
            "title": "Don't Do This"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#using-environment-variables",
            "text": "The Delphix Engine provides a better way to pass sensitive data to remote bash (or powershell) calls: environment variables. Let us look at a different way to run the same command as above.  from   dlpx.virtualization   import   libs  from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.virtual.stop () \n   # Use environment variables to pass sensitive data to remote commands \n   environment_vars   =   { \n     \"DATABASE_PASSWORD\"   :   password \n   } \n   full_command   =   \"echo $DATABASE_PASSWORD | db_cmd shutdown {}\" . format ( db_name ) \n   libs . run_bash ( virtual_source . connection ,   full_command ,   variables = environment_vars )    Note  We are no longer putting the cleartext password into the Python command string. Instead, we are instructing the Virtualization Platform to put the password into an environment variable on the target environment. The Python command string merely mentions the name of the environment variable, and does not contain the password itself.   Once the command runs on the target environment, Bash will substitute in the password, and the database shutdown will run as expected.  Unlike with the command string, the Virtualization Platform  does  treat environment variables as sensitive information, and will not include them in error messages or internal logs, etc.",
            "title": "Using Environment Variables"
        },
        {
            "location": "/Best_Practices/Sensitive_Data/#dont-write-out-sensitive-data",
            "text": "Plugin writers are strongly advised to never write out unencrypted sensitive data. This is common-sense general advice that applies to all areas of programming, not just for plugins. However, there are a couple of special concerns for plugins.  The Virtualization Platform provides logging capabilities to plugins. The generated logs are unencrypted and not treated as sensitive. Therefore, it is important for plugins to  never log sensitive data .  In addition, remember that your plugin is not treated as sensitive by the Virtualization Platform. Plugin code is distributed unencrypted, and is viewable in cleartext by Delphix Engine users. Sensitive data such as passwords should never be hard-coded in your plugin code.",
            "title": "Don't Write Out Sensitive Data"
        },
        {
            "location": "/Best_Practices/Unicode_Data/",
            "text": "Working with Unicode Data\n\u00b6\n\n\nTo use unicode characters in the plugin code, the following lines should be included at top of the plugin code:\n\n\n#!/usr/bin/env python\n\n\n# -*- coding: utf-8 -*-\n\n\n\n\n\n\nOtherwise, there may be errors when building the plugin using \ndvp build\n or during the execution of a plugin operation.\n\n\nExample\n\u00b6\n\n\n#!/usr/bin/env python\n\n\n# -*- coding: utf-8 -*-\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\nfrom\n \ngenerated.definitions\n \nimport\n \nRepositoryDefinition\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.discovery.repository\n()\n\n\ndef\n \nrepository_discovery\n(\nsource_connection\n):\n\n    \n# Create a repository with name \u2603\n\n    \ncommand\n \n=\n \n'echo \u2603'\n\n    \nresult\n \n=\n \nlibs\n.\nrun_bash\n(\nsource_connection\n,\n \ncommand\n)\n\n    \nreturn\n \n[\nRepositoryDefinition\n(\nname\n=\nresult\n.\nstdout\n)]",
            "title": "Working with Unicode Data"
        },
        {
            "location": "/Best_Practices/Unicode_Data/#working-with-unicode-data",
            "text": "To use unicode characters in the plugin code, the following lines should be included at top of the plugin code:  #!/usr/bin/env python  # -*- coding: utf-8 -*-   Otherwise, there may be errors when building the plugin using  dvp build  or during the execution of a plugin operation.",
            "title": "Working with Unicode Data"
        },
        {
            "location": "/Best_Practices/Unicode_Data/#example",
            "text": "#!/usr/bin/env python  # -*- coding: utf-8 -*-  from   dlpx.virtualization.platform   import   Plugin  from   dlpx.virtualization   import   libs  from   generated.definitions   import   RepositoryDefinition  plugin   =   Plugin ()  @plugin.discovery.repository ()  def   repository_discovery ( source_connection ): \n     # Create a repository with name \u2603 \n     command   =   'echo \u2603' \n     result   =   libs . run_bash ( source_connection ,   command ) \n     return   [ RepositoryDefinition ( name = result . stdout )]",
            "title": "Example"
        },
        {
            "location": "/Best_Practices/Working_with_Powershell/",
            "text": "Error handling in Powershell\n\u00b6\n\n\n\n\nInfo\n\n\nCommands run via run_powershell are executed as a script. The exit code returned by run_powershell as part of the RunPowershellResult is determined by the exit code from the script.\n\n\n\n\nPowerShell gives you a few ways to handle errors in your scripts:\n\n\n\n\n\n\nSet $ErrorActionPreference. This only applies to PowerShell Cmdlets. For scripts or other executables such as sqlcmd, PowerShell will return with exit code 0 even if there is an error, regardless of the value of $ErrorActionPrefe       rence. The allowable values for $ErrorActionPreference are:\n\n\nContinue (default) \u2013 Continue even if there is an error.                          \n\n  SilentlyContinue \u2013 Acts like Continue with the exception that errors are not displayed         \n\n  Inquire \u2013 Prompts the user in case of error          \n\n  Stop -  Stops execution after the first error\n\n\n\n\n\n\nUse exception handling by using traps and try/catch blocks or if statements to detect errors and return with non-zero exit codes\n\n\n\n\n\n\nUse custom error handling that can be invoked after launching each command in the script to correctly detect errors. \n\n\n\n\n\n\nExamples\n\u00b6\n\n\nThe following example will show you how setting $ErrorActionPreference will return exit codes\n\n\nIn the below code, \nls nothing123\n is expected to fail.\n\n\nls nothing123\nWrite-Host \"Test\"\n\n\n\n\n\nHere is the output when the above commands runs  on a remote host and the script will return the value of \n$?\n to be True eventhough the script failed.\n\n\n```PS C:\\Users\\dtully\\test> ./test1.ps1\nls : Cannot find path 'C:\\Users\\dtully\\test\\nothing123' because it does not exist.\nAt C:\\Users\\dtully\\test\\test1.ps1:1 char:1\n+ ls nothing123\n+ ~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound: (C:\\Users\\dtully\\test\\nothing123:String) [Get-ChildItem], ItemNotFoundEx\n   ception\n    + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetChildItemCommand\n\n\nPS C:\\Users\\dtully\\test> Write-Host $?\nTrue\n\n\nNow lets set $ErrorActionPreference=Stop.\n\n```Windows\n$ErrorActionPreference = \"Stop\"\nls nothing123\nWrite-Host \"Test\"\n\n\n\n\n\nNow when we run the command again we see the return value of \n$?\n to be False.\n\n\nPS C:\\Users\\dtully\\test> ./test1.ps1\nls : Cannot find path 'C:\\Users\\dtully\\test\\nothing123' because it does not exist.\nAt C:\\Users\\dtully\\test\\test1.ps1:2 char:1\n+ ls nothing123\n+ ~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound: (C:\\Users\\dtully\\test\\nothing123:String) [Get-ChildItem], ItemNotFoundException\n    + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetChildItemCommand\n\nPS C:\\Users\\dtully\\test> Write-Host $?\nFalse\n\n\n\n\n\nThe following example shows how you can use the function verifySuccess to detect whether the previous command failed, and if it did print, print an error message and return with an exit code of 1.\n\n\nfunction die {\n    Write-Error \"Error: $($args[0])\"\n    exit 1\n}\n\nfunction verifySuccess {\n    if (!$?) {\n        die \"$($args[0])\"\n    }\n}\n\nWrite-Output \"I'd rather be in Hawaii\"\nverifySuccess \"WRITE_OUTPUT_FAILED\"\n\n& C:\\Program Files\\Delphix\\scripts\\myscript.ps1\nverifySuccess \"MY_SCRIPT_FAILED\"",
            "title": "Working with Powershell"
        },
        {
            "location": "/Best_Practices/Working_with_Powershell/#error-handling-in-powershell",
            "text": "Info  Commands run via run_powershell are executed as a script. The exit code returned by run_powershell as part of the RunPowershellResult is determined by the exit code from the script.   PowerShell gives you a few ways to handle errors in your scripts:    Set $ErrorActionPreference. This only applies to PowerShell Cmdlets. For scripts or other executables such as sqlcmd, PowerShell will return with exit code 0 even if there is an error, regardless of the value of $ErrorActionPrefe       rence. The allowable values for $ErrorActionPreference are:  Continue (default) \u2013 Continue even if there is an error.                           \n  SilentlyContinue \u2013 Acts like Continue with the exception that errors are not displayed          \n  Inquire \u2013 Prompts the user in case of error           \n  Stop -  Stops execution after the first error    Use exception handling by using traps and try/catch blocks or if statements to detect errors and return with non-zero exit codes    Use custom error handling that can be invoked after launching each command in the script to correctly detect errors.",
            "title": "Error handling in Powershell"
        },
        {
            "location": "/Best_Practices/Working_with_Powershell/#examples",
            "text": "The following example will show you how setting $ErrorActionPreference will return exit codes  In the below code,  ls nothing123  is expected to fail.  ls nothing123\nWrite-Host \"Test\"  Here is the output when the above commands runs  on a remote host and the script will return the value of  $?  to be True eventhough the script failed.  ```PS C:\\Users\\dtully\\test> ./test1.ps1\nls : Cannot find path 'C:\\Users\\dtully\\test\\nothing123' because it does not exist.\nAt C:\\Users\\dtully\\test\\test1.ps1:1 char:1\n+ ls nothing123\n+ ~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound: (C:\\Users\\dtully\\test\\nothing123:String) [Get-ChildItem], ItemNotFoundEx\n   ception\n    + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetChildItemCommand  PS C:\\Users\\dtully\\test> Write-Host $?\nTrue  Now lets set $ErrorActionPreference=Stop.\n\n```Windows\n$ErrorActionPreference = \"Stop\"\nls nothing123\nWrite-Host \"Test\"  Now when we run the command again we see the return value of  $?  to be False.  PS C:\\Users\\dtully\\test> ./test1.ps1\nls : Cannot find path 'C:\\Users\\dtully\\test\\nothing123' because it does not exist.\nAt C:\\Users\\dtully\\test\\test1.ps1:2 char:1\n+ ls nothing123\n+ ~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound: (C:\\Users\\dtully\\test\\nothing123:String) [Get-ChildItem], ItemNotFoundException\n    + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetChildItemCommand\n\nPS C:\\Users\\dtully\\test> Write-Host $?\nFalse  The following example shows how you can use the function verifySuccess to detect whether the previous command failed, and if it did print, print an error message and return with an exit code of 1.  function die {\n    Write-Error \"Error: $($args[0])\"\n    exit 1\n}\n\nfunction verifySuccess {\n    if (!$?) {\n        die \"$($args[0])\"\n    }\n}\n\nWrite-Output \"I'd rather be in Hawaii\"\nverifySuccess \"WRITE_OUTPUT_FAILED\"\n\n& C:\\Program Files\\Delphix\\scripts\\myscript.ps1\nverifySuccess \"MY_SCRIPT_FAILED\"",
            "title": "Examples"
        },
        {
            "location": "/Release_Notes/2.0.0/2.0.0/",
            "text": "Release - GA (v2.0.0)\n\u00b6\n\n\nTo install or upgrade the SDK, refer to instructions \nhere\n.\n\n\nNew & Improved\n\u00b6\n\n\n\n\n\n\nAdded the ability for plugins to upgrade across plugin versions with schema changes. Some hightlights:\n\n\n\n\nSchema updates using data migrations.\n\n\nFlexiblity for plugins to pick any release strategy.\n\n\nPlugin upgrades supported across multiple plugin versions. \n\n\nZero dSource and VDB downtime during plugin upgrade.\n\n\n\n\nMore details about Plugin Upgrade can be found \nhere\n.\n\n\n\n\n\n\nAdded a new field \nexternalVersion\n to the \nPlugin Config\n that allows plugins to display an end-user friendly version. More details \nhere\n.\n\n\n\n\nAdded a new option to \ninit\n to select a host type for the plugin (\nUnix\n or \nWindows\n) to make it easier to get started with plugins that support either host platform.\n\n\nAdded a new option to \nupload\n to block and wait for the upload job to finish on the Delphix Engine before the command returns.\n\n\n\n\nBreaking Changes\n\u00b6\n\n\n\n\n\n\nThe following field in the \nPlugin Config\n was renamed:\n\n\n\n\n\n\n\n\nPrevious\n\n\nUpdated\n\n\n\n\n\n\n\n\n\n\nversion\n\n\nbuidNumber\n\n\n\n\n\n\n\n\nAdditionally \nbuildNumber\n has to conform to the format described \nhere\n.\n\n\nDetailed steps to detect and make changes.",
            "title": "Release - GA (v2.0.0)"
        },
        {
            "location": "/Release_Notes/2.0.0/2.0.0/#release-ga-v200",
            "text": "To install or upgrade the SDK, refer to instructions  here .",
            "title": "Release - GA (v2.0.0)"
        },
        {
            "location": "/Release_Notes/2.0.0/2.0.0/#new-improved",
            "text": "Added the ability for plugins to upgrade across plugin versions with schema changes. Some hightlights:   Schema updates using data migrations.  Flexiblity for plugins to pick any release strategy.  Plugin upgrades supported across multiple plugin versions.   Zero dSource and VDB downtime during plugin upgrade.   More details about Plugin Upgrade can be found  here .    Added a new field  externalVersion  to the  Plugin Config  that allows plugins to display an end-user friendly version. More details  here .   Added a new option to  init  to select a host type for the plugin ( Unix  or  Windows ) to make it easier to get started with plugins that support either host platform.  Added a new option to  upload  to block and wait for the upload job to finish on the Delphix Engine before the command returns.",
            "title": "New &amp; Improved"
        },
        {
            "location": "/Release_Notes/2.0.0/2.0.0/#breaking-changes",
            "text": "The following field in the  Plugin Config  was renamed:     Previous  Updated      version  buidNumber     Additionally  buildNumber  has to conform to the format described  here .  Detailed steps to detect and make changes.",
            "title": "Breaking Changes"
        },
        {
            "location": "/Release_Notes/2.0.0/2.0.0_Breaking_Changes/",
            "text": "Breaking Changes - GA (v.2.0.0)\n\u00b6\n\n\nPlugin Config Field Renamed\n\u00b6\n\n\nThe following field in the \nPlugin Config\n were replaced:\n\n\n\n\n\n\n\n\nPrevious\n\n\nUpdated\n\n\n\n\n\n\n\n\n\n\nversion\n\n\nbuildNumber\n\n\n\n\n\n\n\n\nAdditionally, the \nbuildNumber\n must be a string that conforms to the following rules:\n\n\n\n\nThe string must be composed of a sequence of non-negative integers, not all zero, separated by periods.\n\n\nTrailing zeros are ignored. So, \"1.0.0\" is treated the same as \"1\".\n\n\nBuild numbers are sortable numerically, with earlier numbers having more significance than later numbers. So, \"2.0\" comes after \"1.99999\", and \"1.10\" comes after \"1.2\".\n\n\nThe Delphix Engine will never allow installation of plugin with a build number that is ordered before the the already-installed build number.\n\n\n\n\nMore details about the format are \nhere\n.\n\n\nWhat is affected\n\u00b6\n\n\nAll plugins built with v1.0.0 or below will be affected. The \nPlugin Config\n field \nversion\n will have to be updated to \nbuildNumber\n.\n\n\nHow does it fail\n\u00b6\n\n\ndvp build\n will fail with the following error message if the \nPlugin Config\n \nversion\n field is not updated to \nbuildNumber\n:\n\n\n$ dvp build\nError: Additional properties are not allowed \n(\n'version'\n was unexpected\n)\n on \n[\n'additionalProperties'\n]\n\nError: \n'buildNumber'\n is a required property on \n[\n'required'\n]\n\n\nValidation failed on /private/var/tmp/fp/plugin_config.yml. \n\n0\n Warning\n(\ns\n)\n. \n2\n Error\n(\ns\n)\n \n\nBUILD FAILED.\n\n\n\n\n\nHow to fix it\n\u00b6\n\n\nRename the \nPlugin Config\n \nversion\n field to \nbuildNumber\n. Make sure that the \nbuildNumber\n conforms to the format described \nhere\n.\n\n\n\n\nPrevious releases\n\n\n\n\nid: 4174f1b8-45df-43cc-8e4c-21d309c17861\nname: My Plugin\nversion: 1.0.0\nlanguage: PYTHON27\nhostTypes:\n- UNIX\npluginType: DIRECT\nmanualDiscovery: true\nentryPoint: plugin_runner:plugin\nsrcDir: src\nschemaFile: schema.json\n\n\n\n\n\n\n\n2.0.0\n\n\n\n\nid: 4174f1b8-45df-43cc-8e4c-21d309c17861\nname: My Plugin\nbuildNumber: 1.0.0\nlanguage: PYTHON27\nhostTypes:\n- UNIX\npluginType: DIRECT\nmanualDiscovery: true\nentryPoint: plugin_runner:plugin\nsrcDir: src\nschemaFile: schema.json",
            "title": "Breaking Changes - GA (v.2.0.0)"
        },
        {
            "location": "/Release_Notes/2.0.0/2.0.0_Breaking_Changes/#breaking-changes-ga-v200",
            "text": "",
            "title": "Breaking Changes - GA (v.2.0.0)"
        },
        {
            "location": "/Release_Notes/2.0.0/2.0.0_Breaking_Changes/#plugin-config-field-renamed",
            "text": "The following field in the  Plugin Config  were replaced:     Previous  Updated      version  buildNumber     Additionally, the  buildNumber  must be a string that conforms to the following rules:   The string must be composed of a sequence of non-negative integers, not all zero, separated by periods.  Trailing zeros are ignored. So, \"1.0.0\" is treated the same as \"1\".  Build numbers are sortable numerically, with earlier numbers having more significance than later numbers. So, \"2.0\" comes after \"1.99999\", and \"1.10\" comes after \"1.2\".  The Delphix Engine will never allow installation of plugin with a build number that is ordered before the the already-installed build number.   More details about the format are  here .",
            "title": "Plugin Config Field Renamed"
        },
        {
            "location": "/Release_Notes/2.0.0/2.0.0_Breaking_Changes/#what-is-affected",
            "text": "All plugins built with v1.0.0 or below will be affected. The  Plugin Config  field  version  will have to be updated to  buildNumber .",
            "title": "What is affected"
        },
        {
            "location": "/Release_Notes/2.0.0/2.0.0_Breaking_Changes/#how-does-it-fail",
            "text": "dvp build  will fail with the following error message if the  Plugin Config   version  field is not updated to  buildNumber :  $ dvp build\nError: Additional properties are not allowed  ( 'version'  was unexpected )  on  [ 'additionalProperties' ] \nError:  'buildNumber'  is a required property on  [ 'required' ] \n\nValidation failed on /private/var/tmp/fp/plugin_config.yml.  0  Warning ( s ) .  2  Error ( s )  \n\nBUILD FAILED.",
            "title": "How does it fail"
        },
        {
            "location": "/Release_Notes/2.0.0/2.0.0_Breaking_Changes/#how-to-fix-it",
            "text": "Rename the  Plugin Config   version  field to  buildNumber . Make sure that the  buildNumber  conforms to the format described  here .   Previous releases   id: 4174f1b8-45df-43cc-8e4c-21d309c17861\nname: My Plugin\nversion: 1.0.0\nlanguage: PYTHON27\nhostTypes:\n- UNIX\npluginType: DIRECT\nmanualDiscovery: true\nentryPoint: plugin_runner:plugin\nsrcDir: src\nschemaFile: schema.json   2.0.0   id: 4174f1b8-45df-43cc-8e4c-21d309c17861\nname: My Plugin\nbuildNumber: 1.0.0\nlanguage: PYTHON27\nhostTypes:\n- UNIX\npluginType: DIRECT\nmanualDiscovery: true\nentryPoint: plugin_runner:plugin\nsrcDir: src\nschemaFile: schema.json",
            "title": "How to fix it"
        },
        {
            "location": "/Release_Notes/1.0.0/1.0.0/",
            "text": "Release - GA (v1.0.0)\n\u00b6\n\n\nTo install or upgrade the SDK, refer to instructions \nhere\n.\n\n\nNew & Improved\n\u00b6\n\n\n\n\nAdded support for a CLI configuration file to specify default options for \ndvp\n commands. More details \nhere\n.\n\n\n\n\nImproved speed and scalability of plugin operations:\n\n\n\n\nReduced startup time for plugin operations from seconds to milliseconds.\n\n\nImproved memory utilization on the Delphix Engine to enable a large number of plugin operations to execute in parallel.\n\n\n\n\n\n\n\n\nAdded the ability for plugins to raise user visible messages with a custom message, action and output related to a failure during a plugin operation. Refer to the \nUser Visible Errors\n section for more details.\n\n\n\n\nImproved validation for type and range checks for autogenerated classes.\n\n\nImproved security for the plugin's runtime when executed on the Delphix Engine.\n\n\nRemoved the Delphix Engine feature flag \nPYTHON_TOOLKITS\n as the Delphix Engine supports plugins built on the SDK by default. The \nGetting Started\n section has been updated has well.\n\n\n\n\nBreaking Changes\n\u00b6\n\n\n\n\n\n\nThe following fields in the \nPlugin Config\n were renamed:\n\n\n\n\n\n\n\n\nPrevious\n\n\nUpdated\n\n\n\n\n\n\n\n\n\n\nname\n\n\nplugin_id\n\n\n\n\n\n\nprettyName\n\n\nname\n\n\n\n\n\n\n\n\nAdditionally, the \nplugin_id\n is now required to be a UUID with a format: \n[0-9a-fA-F]{8}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{12}\n.\n\n\nDetailed steps to detect and make changes.\n\n\n\n\n\n\nFixed\n\u00b6\n\n\n\n\nUpdated remote host operations to execute as the \nRemoteUser\n specfied instead of the primary environment user.\n\n\nFixed an incorrect user exception when the required plugin operation \nlinked.post_snapshot\n was missing.\n\n\nUpdated \nrun_expect\n to return an \nexit_code\n, \nstdout\n, \nstderr\n like other platform library functions.\n\n\nFixed \nrun_powershell\n to not automatically redirect \nstderr\n to \nstdout\n.\n\n\nEnsured that all exceptions raised by the \nStaged Linked Source Worker\n plugin operation are converted to faults for the user.\n\n\nEnabled the \nMountSpecification\n to be constructed with \nmounts\n that refer to different environments.\n\n\nSanitized the Python stack traces from exceptions during plugin execution and removed paths that reference where the plugin was built.\n\n\nRemoved a spurious build warning for \nDIRECT\n plugins that incorrectly suggested implementing  the \nStaged Linked Source Mount Specification\n plugin operation.\n\n\nRemoved a spurious message \nglobal name 'exit' is not defined\n which was displayed when a plugin library function failed.\n\n\nUpdated \nmanualDiscovery\n to be optional in the \nPlugin Config\n. The default value will be \nTrue\n.",
            "title": "Release - GA (v1.0.0)"
        },
        {
            "location": "/Release_Notes/1.0.0/1.0.0/#release-ga-v100",
            "text": "To install or upgrade the SDK, refer to instructions  here .",
            "title": "Release - GA (v1.0.0)"
        },
        {
            "location": "/Release_Notes/1.0.0/1.0.0/#new-improved",
            "text": "Added support for a CLI configuration file to specify default options for  dvp  commands. More details  here .   Improved speed and scalability of plugin operations:   Reduced startup time for plugin operations from seconds to milliseconds.  Improved memory utilization on the Delphix Engine to enable a large number of plugin operations to execute in parallel.     Added the ability for plugins to raise user visible messages with a custom message, action and output related to a failure during a plugin operation. Refer to the  User Visible Errors  section for more details.   Improved validation for type and range checks for autogenerated classes.  Improved security for the plugin's runtime when executed on the Delphix Engine.  Removed the Delphix Engine feature flag  PYTHON_TOOLKITS  as the Delphix Engine supports plugins built on the SDK by default. The  Getting Started  section has been updated has well.",
            "title": "New &amp; Improved"
        },
        {
            "location": "/Release_Notes/1.0.0/1.0.0/#breaking-changes",
            "text": "The following fields in the  Plugin Config  were renamed:     Previous  Updated      name  plugin_id    prettyName  name     Additionally, the  plugin_id  is now required to be a UUID with a format:  [0-9a-fA-F]{8}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{12} .  Detailed steps to detect and make changes.",
            "title": "Breaking Changes"
        },
        {
            "location": "/Release_Notes/1.0.0/1.0.0/#fixed",
            "text": "Updated remote host operations to execute as the  RemoteUser  specfied instead of the primary environment user.  Fixed an incorrect user exception when the required plugin operation  linked.post_snapshot  was missing.  Updated  run_expect  to return an  exit_code ,  stdout ,  stderr  like other platform library functions.  Fixed  run_powershell  to not automatically redirect  stderr  to  stdout .  Ensured that all exceptions raised by the  Staged Linked Source Worker  plugin operation are converted to faults for the user.  Enabled the  MountSpecification  to be constructed with  mounts  that refer to different environments.  Sanitized the Python stack traces from exceptions during plugin execution and removed paths that reference where the plugin was built.  Removed a spurious build warning for  DIRECT  plugins that incorrectly suggested implementing  the  Staged Linked Source Mount Specification  plugin operation.  Removed a spurious message  global name 'exit' is not defined  which was displayed when a plugin library function failed.  Updated  manualDiscovery  to be optional in the  Plugin Config . The default value will be  True .",
            "title": "Fixed"
        },
        {
            "location": "/Release_Notes/1.0.0/1.0.0_Breaking_Changes/",
            "text": "Breaking Changes - GA (v.1.0.0)\n\u00b6\n\n\nPlugin Config Fields Renamed\n\u00b6\n\n\nThe following fields in the \nPlugin Config\n were renamed:\n\n\n\n\n\n\n\n\nPrevious\n\n\nUpdated\n\n\n\n\n\n\n\n\n\n\nname\n\n\nplugin_id\n\n\n\n\n\n\nprettyName\n\n\nname\n\n\n\n\n\n\n\n\nAdditionally, the \nplugin_id\n is now required to be a UUID with format: \n[0-9a-fA-F]{8}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{12}\n. This will allow the plugins to be uniquely identified across plugin developers.\n\n\nWhat is affected\n\u00b6\n\n\nAll plugins built with v0.3.0 or v0.4.0 will be affected. The \nPlugin Config\n fields will have to be updated.\n\n\nHow does it fail\n\u00b6\n\n\ndvp build\n will fail with the following error message if the \nPlugin Config\n fields are not updated:\n\n\n$ dvp build\nError: Additional properties are not allowed \n(\n'prettyName'\n was unexpected\n)\n on \n[]\n\n\n{\n\n  \n\"pluginType\"\n: \n\"DIRECT\"\n, \n  \n\"name\"\n: \n\"My Plugin\"\n, \n  \n\"language\"\n: \n\"PYTHON27\"\n, \n  \n\"manualDiscovery\"\n: true, \n  \n\"hostTypes\"\n: \n[\n\n    \n\"UNIX\"\n\n  \n]\n, \n  \n\"version\"\n: \n\"0.1.0\"\n, \n  \n\"entryPoint\"\n: \n\"plugin_runner:plugin\"\n, \n  \n\"srcDir\"\n: \n\"src\"\n, \n  \n\"prettyName\"\n: \n\"My Plugin\"\n, \n  \n\"schemaFile\"\n: \n\"schema.json\"\n\n\n}\n\n\nError: \n'id'\n is a required property on \n[]\n\n\n{\n\n  \n\"pluginType\"\n: \n\"DIRECT\"\n, \n  \n\"name\"\n: \n\"My Plugin\"\n, \n  \n\"language\"\n: \n\"PYTHON27\"\n, \n  \n\"manualDiscovery\"\n: true, \n  \n\"hostTypes\"\n: \n[\n\n    \n\"UNIX\"\n\n  \n]\n, \n  \n\"version\"\n: \n\"0.1.0\"\n, \n  \n\"entryPoint\"\n: \n\"plugin_runner:plugin\"\n, \n  \n\"srcDir\"\n: \n\"src\"\n, \n  \n\"prettyName\"\n: \n\"My Plugin\"\n, \n  \n\"schemaFile\"\n: \n\"schema.json\"\n\n\n}\n\nValidation failed on plugin_config.yml. \n\n0\n Warning\n(\ns\n)\n. \n2\n Error\n(\ns\n)\n \n\nBUILD FAILED.\n\n\n\n\n\nHow to fix it\n\u00b6\n\n\nRename the \nPlugin Config\n fields. Make sure that the \nid\n is a UUID of the format \n[0-9a-fA-F]{8}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{12}\n. A UUID can be generated manually using an online generator or via Python:\n\n\n$ python\n>>> import uuid\n>>> uuid.uuid4\n()\n\nUUID\n(\n'4174f1b8-45df-43cc-8e4c-21d309c17861'\n)\n\n\n\n\n\n\n\n\nPrevious releases\n\n\n\n\nname: my_plugin\nprettyName: My Plugin\nversion: 0.1.0\nlanguage: PYTHON27\nhostTypes:\n- UNIX\npluginType: DIRECT\nmanualDiscovery: true\nentryPoint: plugin_runner:plugin\nsrcDir: src\nschemaFile: schema.json\n\n\n\n\n\n\n\n1.0.0\n\n\n\n\nid: 4174f1b8-45df-43cc-8e4c-21d309c17861\nname: My Plugin\nversion: 0.1.0\nlanguage: PYTHON27\nhostTypes:\n- UNIX\npluginType: DIRECT\nmanualDiscovery: true\nentryPoint: plugin_runner:plugin\nsrcDir: src\nschemaFile: schema.json",
            "title": "Breaking Changes - GA (v.1.0.0)"
        },
        {
            "location": "/Release_Notes/1.0.0/1.0.0_Breaking_Changes/#breaking-changes-ga-v100",
            "text": "",
            "title": "Breaking Changes - GA (v.1.0.0)"
        },
        {
            "location": "/Release_Notes/1.0.0/1.0.0_Breaking_Changes/#plugin-config-fields-renamed",
            "text": "The following fields in the  Plugin Config  were renamed:     Previous  Updated      name  plugin_id    prettyName  name     Additionally, the  plugin_id  is now required to be a UUID with format:  [0-9a-fA-F]{8}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{12} . This will allow the plugins to be uniquely identified across plugin developers.",
            "title": "Plugin Config Fields Renamed"
        },
        {
            "location": "/Release_Notes/1.0.0/1.0.0_Breaking_Changes/#what-is-affected",
            "text": "All plugins built with v0.3.0 or v0.4.0 will be affected. The  Plugin Config  fields will have to be updated.",
            "title": "What is affected"
        },
        {
            "location": "/Release_Notes/1.0.0/1.0.0_Breaking_Changes/#how-does-it-fail",
            "text": "dvp build  will fail with the following error message if the  Plugin Config  fields are not updated:  $ dvp build\nError: Additional properties are not allowed  ( 'prettyName'  was unexpected )  on  []  { \n   \"pluginType\" :  \"DIRECT\" , \n   \"name\" :  \"My Plugin\" , \n   \"language\" :  \"PYTHON27\" , \n   \"manualDiscovery\" : true, \n   \"hostTypes\" :  [ \n     \"UNIX\" \n   ] , \n   \"version\" :  \"0.1.0\" , \n   \"entryPoint\" :  \"plugin_runner:plugin\" , \n   \"srcDir\" :  \"src\" , \n   \"prettyName\" :  \"My Plugin\" , \n   \"schemaFile\" :  \"schema.json\"  } \n\nError:  'id'  is a required property on  []  { \n   \"pluginType\" :  \"DIRECT\" , \n   \"name\" :  \"My Plugin\" , \n   \"language\" :  \"PYTHON27\" , \n   \"manualDiscovery\" : true, \n   \"hostTypes\" :  [ \n     \"UNIX\" \n   ] , \n   \"version\" :  \"0.1.0\" , \n   \"entryPoint\" :  \"plugin_runner:plugin\" , \n   \"srcDir\" :  \"src\" , \n   \"prettyName\" :  \"My Plugin\" , \n   \"schemaFile\" :  \"schema.json\"  } \nValidation failed on plugin_config.yml.  0  Warning ( s ) .  2  Error ( s )  \n\nBUILD FAILED.",
            "title": "How does it fail"
        },
        {
            "location": "/Release_Notes/1.0.0/1.0.0_Breaking_Changes/#how-to-fix-it",
            "text": "Rename the  Plugin Config  fields. Make sure that the  id  is a UUID of the format  [0-9a-fA-F]{8}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{12} . A UUID can be generated manually using an online generator or via Python:  $ python\n>>> import uuid\n>>> uuid.uuid4 () \nUUID ( '4174f1b8-45df-43cc-8e4c-21d309c17861' )    Previous releases   name: my_plugin\nprettyName: My Plugin\nversion: 0.1.0\nlanguage: PYTHON27\nhostTypes:\n- UNIX\npluginType: DIRECT\nmanualDiscovery: true\nentryPoint: plugin_runner:plugin\nsrcDir: src\nschemaFile: schema.json   1.0.0   id: 4174f1b8-45df-43cc-8e4c-21d309c17861\nname: My Plugin\nversion: 0.1.0\nlanguage: PYTHON27\nhostTypes:\n- UNIX\npluginType: DIRECT\nmanualDiscovery: true\nentryPoint: plugin_runner:plugin\nsrcDir: src\nschemaFile: schema.json",
            "title": "How to fix it"
        },
        {
            "location": "/Release_Notes/0.4.0/0.4.0/",
            "text": "Release - Early Preview 2 (v0.4.0)\n\u00b6\n\n\nTo install or upgrade the SDK, refer to instructions \nhere\n.\n\n\nNew & Improved\n\u00b6\n\n\n\n\nAdded a new CLI command \ndownload-logs\n to enable downloading plugin generated logs from the Delphix Engine.\n\n\n\n\nAdded an optional argument named \ncheck\n to the following \nplatform library\n functions:\n\n\n\n\nrun_bash\n\n\nrun_powershell\n\n\n\n\nWith \ncheck=true\n, the platform library function checks the \nexit_code\n and raises an exception if it is non-zero.\n\n\n\n\n\n\nModified \ninit\n to auto-generate default implementations for all required plugin operations.\n\n\n\n\nImproved \nbuild\n validation for:\n\n\nRequired \nplugin operations\n.\n\n\nIncorrect \nplugin operation\n argument names.\n\n\nPlugin Config\n \nentryPoint\n: The \nentryPoint\n is now imported during the \nbuild\n as part of the validation.\n\n\nSchemas\n: Validated to conform to the \nJSON Schema Draft-07 Specification\n.\n\n\n\n\n\n\n\n\nImproved runtime validation and error messages for:\n\n\n\n\nObjects returned from \nplugin operations\n.\n\n\nPlatform Classes\n during instantiation.\n\n\nPlatform Library\n function arguments.\n\n\n\n\n\n\n\n\nAdded support for Docker based plugins by specifying \nrootSquashEnabled: false\n in the \nplugin config\n.\n\n\n\n\nAdded Job and thread information to plugin generated log messages to increase diagnosability and observability.\n\n\n\n\nBreaking Changes\n\u00b6\n\n\n\n\n\n\nA new argument \nsnapshot_parameters\n was added to the following \nstaged\n plugin operations:\n\n\n\n\nStaged Linked Source Pre-Snapshot\n\n\nStaged Linked Source Post-Snapshot\n\n\n\n\nThis argument will allow the end user to indicate to the plugin whether or not to initiate a full ingestion for a dSource. More details about the new argument are \nhere\n.\n\n\nDetailed steps to detect and make changes.\n\n\n\n\n\n\nProperties of the \nStagedSource\n class were modified:\n\n\n\n\nconnection\n was renamed to \nsource_connection\n.\n\n\nstaged_connection\n was added to allow connecting to the staging environment.\n\n\n\n\nThis will enable plugins to connect to both the source and staging environments. More details about these properties are \nhere\n.\n\n\nDetailed steps to detect and make changes.\n\n\n\n\n\n\nFixed\n\u00b6\n\n\n\n\nAllow access to nested package resources via \npkgutil.get_data\n.\n\n\nFixed Out of Memory exceptions.\n\n\n\n\nFixed missing or incorrectly populated properties for the following classes:\n\n\n\n\n\n\n\n\nClass\n\n\nProperties\n\n\n\n\n\n\n\n\n\n\nVirtualSource\n\n\nmounts\n\n\n\n\n\n\nRemoteUser\n\n\nname\n\n\n\n\n\n\nRemoteEnvironment\n\n\nname\n\n\n\n\n\n\nRemoteHost\n\n\nname\n \nbinary_path\n\n\n\n\n\n\n\n\n\n\n\n\nUpdated Job warnings during discovery to display the underlying Python exceptions if one is raised by the plugin operations.\n\n\n\n\nRecreate the plugin's log directory if a plugin is deleted and re-uploaded to the Delphix Engine.\n\n\nMark incorrectly provisioned VDBs as unusable and prevent subsequent Delphix Engine operations on such VDBs.\n\n\nBetter error messages when incorrect environment types are used for Platform Libraries.\n\n\nBetter error messages when a plugin's \nschema\n is updated and the plugin is re-uploaded to the Delphix Engine, with clear instructions on how to proceed.\n\n\nFixed \nbuild\n failures on Windows.",
            "title": "Release - Early Preview 2 (v0.4.0)"
        },
        {
            "location": "/Release_Notes/0.4.0/0.4.0/#release-early-preview-2-v040",
            "text": "To install or upgrade the SDK, refer to instructions  here .",
            "title": "Release - Early Preview 2 (v0.4.0)"
        },
        {
            "location": "/Release_Notes/0.4.0/0.4.0/#new-improved",
            "text": "Added a new CLI command  download-logs  to enable downloading plugin generated logs from the Delphix Engine.   Added an optional argument named  check  to the following  platform library  functions:   run_bash  run_powershell   With  check=true , the platform library function checks the  exit_code  and raises an exception if it is non-zero.    Modified  init  to auto-generate default implementations for all required plugin operations.   Improved  build  validation for:  Required  plugin operations .  Incorrect  plugin operation  argument names.  Plugin Config   entryPoint : The  entryPoint  is now imported during the  build  as part of the validation.  Schemas : Validated to conform to the  JSON Schema Draft-07 Specification .     Improved runtime validation and error messages for:   Objects returned from  plugin operations .  Platform Classes  during instantiation.  Platform Library  function arguments.     Added support for Docker based plugins by specifying  rootSquashEnabled: false  in the  plugin config .   Added Job and thread information to plugin generated log messages to increase diagnosability and observability.",
            "title": "New &amp; Improved"
        },
        {
            "location": "/Release_Notes/0.4.0/0.4.0/#breaking-changes",
            "text": "A new argument  snapshot_parameters  was added to the following  staged  plugin operations:   Staged Linked Source Pre-Snapshot  Staged Linked Source Post-Snapshot   This argument will allow the end user to indicate to the plugin whether or not to initiate a full ingestion for a dSource. More details about the new argument are  here .  Detailed steps to detect and make changes.    Properties of the  StagedSource  class were modified:   connection  was renamed to  source_connection .  staged_connection  was added to allow connecting to the staging environment.   This will enable plugins to connect to both the source and staging environments. More details about these properties are  here .  Detailed steps to detect and make changes.",
            "title": "Breaking Changes"
        },
        {
            "location": "/Release_Notes/0.4.0/0.4.0/#fixed",
            "text": "Allow access to nested package resources via  pkgutil.get_data .  Fixed Out of Memory exceptions.   Fixed missing or incorrectly populated properties for the following classes:     Class  Properties      VirtualSource  mounts    RemoteUser  name    RemoteEnvironment  name    RemoteHost  name   binary_path       Updated Job warnings during discovery to display the underlying Python exceptions if one is raised by the plugin operations.   Recreate the plugin's log directory if a plugin is deleted and re-uploaded to the Delphix Engine.  Mark incorrectly provisioned VDBs as unusable and prevent subsequent Delphix Engine operations on such VDBs.  Better error messages when incorrect environment types are used for Platform Libraries.  Better error messages when a plugin's  schema  is updated and the plugin is re-uploaded to the Delphix Engine, with clear instructions on how to proceed.  Fixed  build  failures on Windows.",
            "title": "Fixed"
        },
        {
            "location": "/Release_Notes/0.4.0/0.4.0_Breaking_Changes/",
            "text": "Breaking Changes - Early Preview 2 (v.0.4.0)\n\u00b6\n\n\nNew Argument \nsnapshot_parameters\n\u00b6\n\n\nA new argument \nsnapshot_parameters\n was added to the following \nstaged\n plugin operations:\n\n\n\n\nStaged Linked Source Pre-Snapshot\n\n\nStaged Linked Source Post-Snapshot\n\n\n\n\nThis argument will allow the end user to indicate to the plugin whether or not to initiate a full ingestion for a dSource. More details about the new argument are \nhere\n.\n\n\nWhat is affected\n\u00b6\n\n\nThis argument applies only to \nstaged\n plugins. The plugin's source code will have to be updated for the following staged plugin operations:\n\n\n\n\nStaged Linked Source Pre-Snapshot\n: This plugin operation is optional and will need to be updated if the plugin implements it.\n\n\nStaged Linked Source Post-Snapshot\n: This plugin operation is required and will need to be updated.\n\n\n\n\nHow does it fail\n\u00b6\n\n\nbuild\n will fail with the following error message if the new argument is not added to the affected staged plugin operations:\n\n\n$ dvp build\nError: Number of arguments \ndo\n not match in method staged_post_snapshot. Expected: \n[\n'staged_source'\n, \n'repository'\n, \n'source_config'\n, \n'snapshot_parameters'\n]\n, Found: \n[\n'repository'\n, \n'source_config'\n, \n'staged_source'\n]\n.\nError: Number of arguments \ndo\n not match in method staged_pre_snapshot. Expected: \n[\n'staged_source'\n, \n'repository'\n, \n'source_config'\n, \n'snapshot_parameters'\n]\n, Found: \n[\n'repository'\n, \n'source_config'\n, \n'staged_source'\n]\n.\n\n\n0\n Warning\n(\ns\n)\n. \n2\n Error\n(\ns\n)\n.\n\nBUILD FAILED.\n\n\n\n\n\nHow to fix it\n\u00b6\n\n\nUpdate the affected staged plugin operations to include the new argument \nsnapshot_parameters\n.\n\n\n\n\nPrevious releases\n\n\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \nlinked_pre_snapshot_prior\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \n# This was the function signature prior to 0.4.0\n\n  \npass\n\n\n\n@plugin.linked.post_snapshot\n()\n\n\ndef\n \nlinked_post_snapshot_prior\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \n# This was the function signature prior to 0.4.0\n\n  \nreturn\n \nSnapshotDefinition\n()\n\n\n\n\n\n\n\n\n0.4.0\n\n\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \nlinked_pre_snapshot_040\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n,\n \nsnapshot_parameters\n):\n\n  \n# Updated function signature in 0.4.0\n\n  \npass\n\n\n\n@plugin.linked.post_snapshot\n()\n\n\ndef\n \nlinked_post_snapshot_040\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n,\n \nsnapshot_parameters\n):\n\n  \n# Updated function signature in 0.4.0\n\n  \nreturn\n \nSnapshotDefinition\n()\n\n\n\n\n\n\nStagedSource Properties Modified\n\u00b6\n\n\nProperties of the \nStagedSource\n class were modified:\n\n\n\n\nconnection\n was renamed to \nsource_connection\n.\n\n\nstaged_connection\n was added to allow connecting to the staging environment.\n\n\n\n\nThis will enable plugins to connect to both the source and staging environments. More details about these properties are \nhere\n.\n\n\nWhat is affected\n\u00b6\n\n\nThis change applies only to \nstaged\n plugins.\n\n\nRequired Changes\n\u00b6\n\n\nThe plugin's source code will have to be updated for any staged plugin operations that accesses the \nconnection\n propery of a \nStagedSource\n object.\n\n\nOptional Changes\n\u00b6\n\n\nThe plugin can choose to use the new \nstaged_connection\n property to connect to the staging environment of a dSource.\n\n\nHow does it fail\n\u00b6\n\n\nAny Delphix Engine operation that calls a plugin operation that has not been fixed would fail with the following stack trace as part of the output of the user exception:\n\n\nAttributeError\n:\n \n'StagedSource'\n \nobject\n \nhas\n \nno\n \nattribute\n \n'connection'\n\n\n\n\n\n\nHow to fix it\n\u00b6\n\n\nUpdate any staged plugin operations that access the renamed property.\n\n\n\n\nPrevious releases\n\n\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \nlinked_pre_snapshot_prior\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \n# Property name was 'connection' was the name of the property for staged_source prior to 0.4.0\n\n  \nlibs\n.\nrun_bash\n(\nstaged_source\n.\nconnection\n,\n \n'date'\n)\n\n\n\n\n\n\n\n\n0.4.0\n\n\n\n\nfrom\n \ndlpx.virtualization.platform\n \nimport\n \nPlugin\n\n\nfrom\n \ndlpx.virtualization\n \nimport\n \nlibs\n\n\n\nplugin\n \n=\n \nPlugin\n()\n\n\n\n@plugin.linked.pre_snapshot\n()\n\n\ndef\n \nlinked_pre_snapshot_prior\n(\nstaged_source\n,\n \nrepository\n,\n \nsource_config\n):\n\n  \n# Property name was updated to 'source_connection' in 0.4.0\n\n  \nlibs\n.\nrun_bash\n(\nstaged_source\n.\nsource_connection\n,\n \n'date'\n)",
            "title": "Breaking Changes - Early Preview 2 (v.0.4.0)"
        },
        {
            "location": "/Release_Notes/0.4.0/0.4.0_Breaking_Changes/#breaking-changes-early-preview-2-v040",
            "text": "",
            "title": "Breaking Changes - Early Preview 2 (v.0.4.0)"
        },
        {
            "location": "/Release_Notes/0.4.0/0.4.0_Breaking_Changes/#new-argument-snapshot_parameters",
            "text": "A new argument  snapshot_parameters  was added to the following  staged  plugin operations:   Staged Linked Source Pre-Snapshot  Staged Linked Source Post-Snapshot   This argument will allow the end user to indicate to the plugin whether or not to initiate a full ingestion for a dSource. More details about the new argument are  here .",
            "title": "New Argument snapshot_parameters"
        },
        {
            "location": "/Release_Notes/0.4.0/0.4.0_Breaking_Changes/#what-is-affected",
            "text": "This argument applies only to  staged  plugins. The plugin's source code will have to be updated for the following staged plugin operations:   Staged Linked Source Pre-Snapshot : This plugin operation is optional and will need to be updated if the plugin implements it.  Staged Linked Source Post-Snapshot : This plugin operation is required and will need to be updated.",
            "title": "What is affected"
        },
        {
            "location": "/Release_Notes/0.4.0/0.4.0_Breaking_Changes/#how-does-it-fail",
            "text": "build  will fail with the following error message if the new argument is not added to the affected staged plugin operations:  $ dvp build\nError: Number of arguments  do  not match in method staged_post_snapshot. Expected:  [ 'staged_source' ,  'repository' ,  'source_config' ,  'snapshot_parameters' ] , Found:  [ 'repository' ,  'source_config' ,  'staged_source' ] .\nError: Number of arguments  do  not match in method staged_pre_snapshot. Expected:  [ 'staged_source' ,  'repository' ,  'source_config' ,  'snapshot_parameters' ] , Found:  [ 'repository' ,  'source_config' ,  'staged_source' ] . 0  Warning ( s ) .  2  Error ( s ) .\n\nBUILD FAILED.",
            "title": "How does it fail"
        },
        {
            "location": "/Release_Notes/0.4.0/0.4.0_Breaking_Changes/#how-to-fix-it",
            "text": "Update the affected staged plugin operations to include the new argument  snapshot_parameters .   Previous releases   from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.linked.pre_snapshot ()  def   linked_pre_snapshot_prior ( staged_source ,   repository ,   source_config ): \n   # This was the function signature prior to 0.4.0 \n   pass  @plugin.linked.post_snapshot ()  def   linked_post_snapshot_prior ( staged_source ,   repository ,   source_config ): \n   # This was the function signature prior to 0.4.0 \n   return   SnapshotDefinition ()    0.4.0   from   dlpx.virtualization.platform   import   Plugin  plugin   =   Plugin ()  @plugin.linked.pre_snapshot ()  def   linked_pre_snapshot_040 ( staged_source ,   repository ,   source_config ,   snapshot_parameters ): \n   # Updated function signature in 0.4.0 \n   pass  @plugin.linked.post_snapshot ()  def   linked_post_snapshot_040 ( staged_source ,   repository ,   source_config ,   snapshot_parameters ): \n   # Updated function signature in 0.4.0 \n   return   SnapshotDefinition ()",
            "title": "How to fix it"
        },
        {
            "location": "/Release_Notes/0.4.0/0.4.0_Breaking_Changes/#stagedsource-properties-modified",
            "text": "Properties of the  StagedSource  class were modified:   connection  was renamed to  source_connection .  staged_connection  was added to allow connecting to the staging environment.   This will enable plugins to connect to both the source and staging environments. More details about these properties are  here .",
            "title": "StagedSource Properties Modified"
        },
        {
            "location": "/Release_Notes/0.4.0/0.4.0_Breaking_Changes/#what-is-affected_1",
            "text": "This change applies only to  staged  plugins.",
            "title": "What is affected"
        },
        {
            "location": "/Release_Notes/0.4.0/0.4.0_Breaking_Changes/#required-changes",
            "text": "The plugin's source code will have to be updated for any staged plugin operations that accesses the  connection  propery of a  StagedSource  object.",
            "title": "Required Changes"
        },
        {
            "location": "/Release_Notes/0.4.0/0.4.0_Breaking_Changes/#optional-changes",
            "text": "The plugin can choose to use the new  staged_connection  property to connect to the staging environment of a dSource.",
            "title": "Optional Changes"
        },
        {
            "location": "/Release_Notes/0.4.0/0.4.0_Breaking_Changes/#how-does-it-fail_1",
            "text": "Any Delphix Engine operation that calls a plugin operation that has not been fixed would fail with the following stack trace as part of the output of the user exception:  AttributeError :   'StagedSource'   object   has   no   attribute   'connection'",
            "title": "How does it fail"
        },
        {
            "location": "/Release_Notes/0.4.0/0.4.0_Breaking_Changes/#how-to-fix-it_1",
            "text": "Update any staged plugin operations that access the renamed property.   Previous releases   from   dlpx.virtualization.platform   import   Plugin  from   dlpx.virtualization   import   libs  plugin   =   Plugin ()  @plugin.linked.pre_snapshot ()  def   linked_pre_snapshot_prior ( staged_source ,   repository ,   source_config ): \n   # Property name was 'connection' was the name of the property for staged_source prior to 0.4.0 \n   libs . run_bash ( staged_source . connection ,   'date' )    0.4.0   from   dlpx.virtualization.platform   import   Plugin  from   dlpx.virtualization   import   libs  plugin   =   Plugin ()  @plugin.linked.pre_snapshot ()  def   linked_pre_snapshot_prior ( staged_source ,   repository ,   source_config ): \n   # Property name was updated to 'source_connection' in 0.4.0 \n   libs . run_bash ( staged_source . source_connection ,   'date' )",
            "title": "How to fix it"
        }
    ]
}