{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! \u00b6 With this Delphix Virtualization SDK documentation we hope you will find all you need to know in order to develop your own plugins! Overview \u00b6 If you already know about plugins, and are looking for something specific, use the links to the left to find what you are looking for or search. If this is your first time here, and you are wondering what developing a Delphix plugin will do for you\u2014read on! What Does a Delphix Plugin do? \u00b6 The Delphix Engine is an appliance that lets you quickly and cheaply make virtual copies of large datasets. The engine has built-in support for interfacing with certain types of datasets, such as Oracle, SQL Server and ASE. When you develop a plugin, you enable end users to use your dataset type as if they were using a built-in dataset type, whether it\u2019s MongoDB, Cassandra, or something else. Your plugin will extend the Delphix Engine\u2019s capabilities by teaching it how to run essential virtual data operations on your datasets: How to stop and start them Where to store their data How to make virtual copies These plugin operations are the building blocks of the Delphix Engine. From these building blocks, the engine can provide all of the normal Delphix functionality to the datasets you connect to such as: Provisioning Refreshing Rewinding Replication Syncing Where to Start \u00b6 Read through the first few sections of this documentation, and we will walk you through how to get setup for development, then how to develop, build, and deploy your first plugin. Getting Started will show you how to setup the SDK. When you finish this section, you will have a full plugin development environment, and you will be ready to start building plugins. Building Your First Plugin will walk you step-by-step through the process of developing a very simple plugin. With it, you will learn the concepts and techniques that you will need to develop fully-fledged plugins. That does not mean this first plugin is useless\u2014you will be able to virtualize simple datasets with it. Once you complete these sections, use the rest of the documentation whenever you would like. Questions? \u00b6 If you have questions, bugs or feature requests reach out to us via the Virtualization SDK GitHub repository .","title":"Welcome!"},{"location":"#welcome","text":"With this Delphix Virtualization SDK documentation we hope you will find all you need to know in order to develop your own plugins!","title":"Welcome!"},{"location":"#overview","text":"If you already know about plugins, and are looking for something specific, use the links to the left to find what you are looking for or search. If this is your first time here, and you are wondering what developing a Delphix plugin will do for you\u2014read on!","title":"Overview"},{"location":"#what-does-a-delphix-plugin-do","text":"The Delphix Engine is an appliance that lets you quickly and cheaply make virtual copies of large datasets. The engine has built-in support for interfacing with certain types of datasets, such as Oracle, SQL Server and ASE. When you develop a plugin, you enable end users to use your dataset type as if they were using a built-in dataset type, whether it\u2019s MongoDB, Cassandra, or something else. Your plugin will extend the Delphix Engine\u2019s capabilities by teaching it how to run essential virtual data operations on your datasets: How to stop and start them Where to store their data How to make virtual copies These plugin operations are the building blocks of the Delphix Engine. From these building blocks, the engine can provide all of the normal Delphix functionality to the datasets you connect to such as: Provisioning Refreshing Rewinding Replication Syncing","title":"What Does a Delphix Plugin do?"},{"location":"#where-to-start","text":"Read through the first few sections of this documentation, and we will walk you through how to get setup for development, then how to develop, build, and deploy your first plugin. Getting Started will show you how to setup the SDK. When you finish this section, you will have a full plugin development environment, and you will be ready to start building plugins. Building Your First Plugin will walk you step-by-step through the process of developing a very simple plugin. With it, you will learn the concepts and techniques that you will need to develop fully-fledged plugins. That does not mean this first plugin is useless\u2014you will be able to virtualize simple datasets with it. Once you complete these sections, use the rest of the documentation whenever you would like.","title":"Where to Start"},{"location":"#questions","text":"If you have questions, bugs or feature requests reach out to us via the Virtualization SDK GitHub repository .","title":"Questions?"},{"location":"Getting_Started/","text":"Getting Started \u00b6 The Virtualization SDK is a Python package on PyPI . Install it in your local development environment so that you can build and upload a plugin. The SDK consists of three parts: The dlpx.virtulization.platform module The dlpx.virtualization.libs module A CLI The platform and libs modules expose objects and methods needed to develop a plugin. The CLI is used to build and upload a plugin. Requirements \u00b6 macOS 10.14+, Ubuntu 16.04+, or Windows 10 Python 2.7 (Python 3 is not supported) Java 7+ Delphix Engine 6.0.2.0 or above Installation \u00b6 To install the latest version of the SDK run: $ pip install dvp Use a Virtual Environment We highly recommended that you develop plugins inside of a virtual environment. To learn more about virtual environments, refer to Virtualenv's documentation . The virtual environment needs to use Python 2.7. This is configured when creating the virtualenv: $ virtualenv -p /path/to/python2.7/binary ENV To install a specific version of the SDK run: $ pip install dvp==<version> To upgrade an existing installation of the SDK run: $ pip install dvp --upgrade API Build Version The version of the SDK defines the version of the Virtualization Platform API your plugin will be built against. Basic Usage \u00b6 Our CLI reference describes commands, provides examples, and a help section. To build your plugin: $ dvp build -c <plugin_config> -a <artifact_file> This will generate an upload artifact at <artifact_file> . That file can then be uploaded with: $ dvp upload -e <delphix_engine_address> -u <delphix_admin_user> -a <artifact_file> You will be prompted for the Delphix Engine user's password. You can also use a CLI Configuration File to set default values for CLI command options. Questions? \u00b6 If you have questions, bugs or feature requests reach out to us via the Virtualization SDK GitHub repository .","title":"Getting Started"},{"location":"Getting_Started/#getting-started","text":"The Virtualization SDK is a Python package on PyPI . Install it in your local development environment so that you can build and upload a plugin. The SDK consists of three parts: The dlpx.virtulization.platform module The dlpx.virtualization.libs module A CLI The platform and libs modules expose objects and methods needed to develop a plugin. The CLI is used to build and upload a plugin.","title":"Getting Started"},{"location":"Getting_Started/#requirements","text":"macOS 10.14+, Ubuntu 16.04+, or Windows 10 Python 2.7 (Python 3 is not supported) Java 7+ Delphix Engine 6.0.2.0 or above","title":"Requirements"},{"location":"Getting_Started/#installation","text":"To install the latest version of the SDK run: $ pip install dvp Use a Virtual Environment We highly recommended that you develop plugins inside of a virtual environment. To learn more about virtual environments, refer to Virtualenv's documentation . The virtual environment needs to use Python 2.7. This is configured when creating the virtualenv: $ virtualenv -p /path/to/python2.7/binary ENV To install a specific version of the SDK run: $ pip install dvp==<version> To upgrade an existing installation of the SDK run: $ pip install dvp --upgrade API Build Version The version of the SDK defines the version of the Virtualization Platform API your plugin will be built against.","title":"Installation"},{"location":"Getting_Started/#basic-usage","text":"Our CLI reference describes commands, provides examples, and a help section. To build your plugin: $ dvp build -c <plugin_config> -a <artifact_file> This will generate an upload artifact at <artifact_file> . That file can then be uploaded with: $ dvp upload -e <delphix_engine_address> -u <delphix_admin_user> -a <artifact_file> You will be prompted for the Delphix Engine user's password. You can also use a CLI Configuration File to set default values for CLI command options.","title":"Basic Usage"},{"location":"Getting_Started/#questions","text":"If you have questions, bugs or feature requests reach out to us via the Virtualization SDK GitHub repository .","title":"Questions?"},{"location":"Best_Practices/CLI_Configuration_File/","text":"CLI Configuration File \u00b6 The CLI configuration file can be used to set default values for CLI command options. Location \u00b6 The configuration file is located in the user's home directory under .dvp/config . <USER_HOME> \u2514\u2500\u2500 .dvp \u2514\u2500\u2500 config Your user's home directory will depend on the operating system, but can be referred to using ~ in Unix-based operating systems or %UserProfile% in Windows. Supported Options \u00b6 Use default profile Only the values listed in the default profile are used unless they are overridden by values passed in from a command line option with the same name. The CLI configuration file supports the following options: engine \u00b6 Specifies the Delphix Engine which can be used as part of the dvp upload or dvp download-logs command. engine = engine.example.com user \u00b6 Specifies the user to a Delphix Engine which is used as part of the dvp upload or dvp download-logs command. user = admin password \u00b6 Specifies the password for the user to a Delphix Engine which is used as part of the dvp upload or dvp download-logs command. password = userpassword Example \u00b6 The following example uses all of the supported options for the CLI configuration file: [default] engine = engine.example.com user = admin password = userpassword","title":"CLI Configuration File"},{"location":"Best_Practices/CLI_Configuration_File/#cli-configuration-file","text":"The CLI configuration file can be used to set default values for CLI command options.","title":"CLI Configuration File"},{"location":"Best_Practices/CLI_Configuration_File/#location","text":"The configuration file is located in the user's home directory under .dvp/config . <USER_HOME> \u2514\u2500\u2500 .dvp \u2514\u2500\u2500 config Your user's home directory will depend on the operating system, but can be referred to using ~ in Unix-based operating systems or %UserProfile% in Windows.","title":"Location"},{"location":"Best_Practices/CLI_Configuration_File/#supported-options","text":"Use default profile Only the values listed in the default profile are used unless they are overridden by values passed in from a command line option with the same name. The CLI configuration file supports the following options:","title":"Supported Options"},{"location":"Best_Practices/CLI_Configuration_File/#engine","text":"Specifies the Delphix Engine which can be used as part of the dvp upload or dvp download-logs command. engine = engine.example.com","title":"engine"},{"location":"Best_Practices/CLI_Configuration_File/#user","text":"Specifies the user to a Delphix Engine which is used as part of the dvp upload or dvp download-logs command. user = admin","title":"user"},{"location":"Best_Practices/CLI_Configuration_File/#password","text":"Specifies the password for the user to a Delphix Engine which is used as part of the dvp upload or dvp download-logs command. password = userpassword","title":"password"},{"location":"Best_Practices/CLI_Configuration_File/#example","text":"The following example uses all of the supported options for the CLI configuration file: [default] engine = engine.example.com user = admin password = userpassword","title":"Example"},{"location":"Best_Practices/Code_Sharing/","text":"Code Sharing \u00b6 All Python modules inside of srcDir can be imported just as they would be if the plugin was executing locally. When a plugin operation is executed srcDir is the current working directory so all imports need to be relative to srcDir regardless of the path of the module doing the import. Please refer to Python's documentation on modules to learn more about modules and imports. Example \u00b6 Assume we have the following file structure: postgres \u251c\u2500\u2500 plugin_config.yml \u251c\u2500\u2500 schema.json \u2514\u2500\u2500 src \u251c\u2500\u2500 operations \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 discovery.py \u251c\u2500\u2500 plugin_runner.py \u251c\u2500\u2500 resources \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 execute_sql.sh \u2502 \u251c\u2500\u2500 list_installs.sh \u2502 \u2514\u2500\u2500 list_schemas.sql \u2514\u2500\u2500 utils \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 execution_util.py Any module in the plugin could import execution_util.py with from utils import execution_util . Gotcha Since the platform uses Python 2.7, every directory needs to have an __init__.py file in it otherwise the modules and resources in the folder will not be found at runtime. For more information on __init__.py files refer to Python's documentation on packages . Note that the srcDir in the plugin config file ( src in this example) does not need an __init__.py file. Assume schema.json contains: { \"repositoryDefinition\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\" } }, \"nameField\": \"name\", \"identityFields\": [\"name\"] }, \"sourceConfigDefinition\": { \"type\": \"object\", \"required\": [\"name\"], \"additionalProperties\": false, \"properties\": { \"name\": { \"type\": \"string\" } }, \"nameField\": \"name\", \"identityFields\": [\"name\"] } } To keep the code cleaner, this plugin does two things: Splits discovery logic into its own module: discovery.py . Uses two helper funtions execute_sql and execute_shell in utils/execution_util.py to abstract all remote execution. plugin_runner.py \u00b6 When the platform needs to execute a plugin operation, it always calls into the function decorated by the entryPoint object. The rest of the control flow is determined by the plugin. In order to split logic, the decorated function must delegate into the appropriate module. Below is an example of plugin_runner.py delegating into discovery.py to handle repository and source config discovery: from operations import discovery from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . discovery . repository () def repository_discovery ( source_connection ): return discovery . find_installs ( source_connection ); @plugin . discovery . source_config () def source_config_discovery ( source_connection , repository ): return discovery . find_schemas ( source_connection , repository ) Note discovery.py is in the operations package so it is imported with from operations import discovery . discovery.py \u00b6 In discovery.py the plugin delegates even further to split business logic away from remote execution. utils/execution_util.py deals with remote execution and error handling so discovery.py can focus on business logic. Note that discovery.py still needs to know the format of the return value from each script. from dlpx.virtualization import libs from generated.definitions import RepositoryDefinition , SourceConfigDefinition from utils import execution_util def find_installs ( source_connection ): installs = execution_util . execute_shell ( source_connection , 'list_installs.sh' ) # Assume 'installs' is a comma separated list of the names of Postgres installations. install_names = installs . split ( ',' ) return [ RepositoryDefinition ( name = name ) for name in install_names ] def find_schemas ( source_connection , repository ): schemas = execution_util . execute_sql ( source_connection , repository . name , 'list_schemas.sql' ) # Assume 'schemas' is a comma separated list of the schema names. schema_names = schemas . split ( ',' ) return [ SourceConfigDefinition ( name = name ) for name in schema_names ] Note Even though discovery.py is in the operations package, the import for execution_util is still relative to the srcDir specified in the plugin config file. execution_util is in the utils package so it is imported with from utils import execution_util . execution_util.py \u00b6 execution_util.py has two methods execute_sql and execute_shell . execute_sql takes the name of a SQL script in resources/ and executes it with resources/execute_sql.sh . execute_shell takes the name of a shell script in resources/ and executes it. import pkgutil from dlpx.virtualization import libs def execute_sql ( source_connection , install_name , script_name ): psql_script = pkgutil . get_data ( \"resources\" , \"execute_sql.sh\" ) sql_script = pkgutil . get_data ( \"resources\" , script_name ) result = libs . run_bash ( source_connection , psql_script , variables = { \"SCRIPT\" : sql_script }, check = True ) return result . stdout def execute_shell ( source_connection , script_name ): script = pkgutil . get_data ( \"resources\" , script_name ) result = libs . run_bash ( source_connection , script , check = True ) return result . stdout Note Both execute_sql and execute_shell use the check parameter which will cause an error to be raised if the exit code is non-zero. For more information refer to the run_bash documentation .","title":"Code Sharing"},{"location":"Best_Practices/Code_Sharing/#code-sharing","text":"All Python modules inside of srcDir can be imported just as they would be if the plugin was executing locally. When a plugin operation is executed srcDir is the current working directory so all imports need to be relative to srcDir regardless of the path of the module doing the import. Please refer to Python's documentation on modules to learn more about modules and imports.","title":"Code Sharing"},{"location":"Best_Practices/Code_Sharing/#example","text":"Assume we have the following file structure: postgres \u251c\u2500\u2500 plugin_config.yml \u251c\u2500\u2500 schema.json \u2514\u2500\u2500 src \u251c\u2500\u2500 operations \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 discovery.py \u251c\u2500\u2500 plugin_runner.py \u251c\u2500\u2500 resources \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 execute_sql.sh \u2502 \u251c\u2500\u2500 list_installs.sh \u2502 \u2514\u2500\u2500 list_schemas.sql \u2514\u2500\u2500 utils \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 execution_util.py Any module in the plugin could import execution_util.py with from utils import execution_util . Gotcha Since the platform uses Python 2.7, every directory needs to have an __init__.py file in it otherwise the modules and resources in the folder will not be found at runtime. For more information on __init__.py files refer to Python's documentation on packages . Note that the srcDir in the plugin config file ( src in this example) does not need an __init__.py file. Assume schema.json contains: { \"repositoryDefinition\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\" } }, \"nameField\": \"name\", \"identityFields\": [\"name\"] }, \"sourceConfigDefinition\": { \"type\": \"object\", \"required\": [\"name\"], \"additionalProperties\": false, \"properties\": { \"name\": { \"type\": \"string\" } }, \"nameField\": \"name\", \"identityFields\": [\"name\"] } } To keep the code cleaner, this plugin does two things: Splits discovery logic into its own module: discovery.py . Uses two helper funtions execute_sql and execute_shell in utils/execution_util.py to abstract all remote execution.","title":"Example"},{"location":"Best_Practices/Code_Sharing/#plugin_runnerpy","text":"When the platform needs to execute a plugin operation, it always calls into the function decorated by the entryPoint object. The rest of the control flow is determined by the plugin. In order to split logic, the decorated function must delegate into the appropriate module. Below is an example of plugin_runner.py delegating into discovery.py to handle repository and source config discovery: from operations import discovery from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . discovery . repository () def repository_discovery ( source_connection ): return discovery . find_installs ( source_connection ); @plugin . discovery . source_config () def source_config_discovery ( source_connection , repository ): return discovery . find_schemas ( source_connection , repository ) Note discovery.py is in the operations package so it is imported with from operations import discovery .","title":"plugin_runner.py"},{"location":"Best_Practices/Code_Sharing/#discoverypy","text":"In discovery.py the plugin delegates even further to split business logic away from remote execution. utils/execution_util.py deals with remote execution and error handling so discovery.py can focus on business logic. Note that discovery.py still needs to know the format of the return value from each script. from dlpx.virtualization import libs from generated.definitions import RepositoryDefinition , SourceConfigDefinition from utils import execution_util def find_installs ( source_connection ): installs = execution_util . execute_shell ( source_connection , 'list_installs.sh' ) # Assume 'installs' is a comma separated list of the names of Postgres installations. install_names = installs . split ( ',' ) return [ RepositoryDefinition ( name = name ) for name in install_names ] def find_schemas ( source_connection , repository ): schemas = execution_util . execute_sql ( source_connection , repository . name , 'list_schemas.sql' ) # Assume 'schemas' is a comma separated list of the schema names. schema_names = schemas . split ( ',' ) return [ SourceConfigDefinition ( name = name ) for name in schema_names ] Note Even though discovery.py is in the operations package, the import for execution_util is still relative to the srcDir specified in the plugin config file. execution_util is in the utils package so it is imported with from utils import execution_util .","title":"discovery.py"},{"location":"Best_Practices/Code_Sharing/#execution_utilpy","text":"execution_util.py has two methods execute_sql and execute_shell . execute_sql takes the name of a SQL script in resources/ and executes it with resources/execute_sql.sh . execute_shell takes the name of a shell script in resources/ and executes it. import pkgutil from dlpx.virtualization import libs def execute_sql ( source_connection , install_name , script_name ): psql_script = pkgutil . get_data ( \"resources\" , \"execute_sql.sh\" ) sql_script = pkgutil . get_data ( \"resources\" , script_name ) result = libs . run_bash ( source_connection , psql_script , variables = { \"SCRIPT\" : sql_script }, check = True ) return result . stdout def execute_shell ( source_connection , script_name ): script = pkgutil . get_data ( \"resources\" , script_name ) result = libs . run_bash ( source_connection , script , check = True ) return result . stdout Note Both execute_sql and execute_shell use the check parameter which will cause an error to be raised if the exit code is non-zero. For more information refer to the run_bash documentation .","title":"execution_util.py"},{"location":"Best_Practices/Managing_Scripts_For_Remote_Execution/","text":"Managing Scripts for Remote Execution \u00b6 To execute a PowerShell or Bash script or Expect script on a remote host, you must provide the script as a string to run_powershell or run_bash or run_expect . While you can keep these strings as literals in your Python code, best practice is to keep them as resource files in your source directory and access them with pkgutil . pkgutil is part of the standard Python library. The method that is applicable to resources is pkgutil.get_data . Basic Usage \u00b6 Given the following plugin structure: \u251c\u2500\u2500 plugin_config.yml \u251c\u2500\u2500 schema.json \u2514\u2500\u2500 src \u251c\u2500\u2500 plugin_runner.py \u2514\u2500\u2500 resources \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 get_date.sh Assume SnapshotDefinition is: \"snapshotDefinition\": { \"type\" : \"object\", \"additionalProperties\" : false, \"properties\" : { \"name\": {\"type\": \"string\"}, \"date\": {\"type\": \"string\"} } } and src/resources/get_date.sh contains: 1 2 #!/usr/bin/env bash date If get_date.sh is needed in post_snapshot , it can be retrieved and executed: import pkgutil from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform.exceptions import UserError from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin . linked . post_snapshot () def post_snapshot ( direct_source , repository , source_config ): # Retrieve script contents script_content = pkgutil . get_data ( 'resources' , 'get_date.sh' ) # Execute script on remote host response = libs . run_bash ( direct_source . connection , script_content ) # Fail operation if the timestamp couldn't be retrieved if response . exit_code != 0 : raise UserError ( 'Failed to get date' , 'Make sure the user has the required permissions' , ' {} \\n {} ' . format ( response . stdout , rsponse . stderr )) return SnapshotDefinition ( name = 'Snapshot' , date = response . stdout ) Python's Working Directory This assumes that src/ is Python's current working directory. This is the behavior of the Virtualization Platform. Resources need to be in a Python module pkgutil.get_data cannot retrieve the contents of a resource that is not in a Python package. This means that a resource that is in the first level of your source directory will not be retrievable with pkgutil . Resources must be in a subdirectory of your source directory, and that subdirectory must contain an __init__.py file. Multi-level Packages \u00b6 Given the following plugin structure: \u251c\u2500\u2500 plugin_config.yml \u251c\u2500\u2500 schema.json \u2514\u2500\u2500 src \u251c\u2500\u2500 plugin_runner.py \u2514\u2500\u2500 resources \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 database \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 execute_sql.sh \u2514\u2500\u2500 platform \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 get_date.sh The contents of src/resources/platform/get_date.sh can be retrieved with: script_content = pkgutil . get_data ( 'resources.platform' , 'get_date.sh' )","title":"Managing Scripts for Remote Execution"},{"location":"Best_Practices/Managing_Scripts_For_Remote_Execution/#managing-scripts-for-remote-execution","text":"To execute a PowerShell or Bash script or Expect script on a remote host, you must provide the script as a string to run_powershell or run_bash or run_expect . While you can keep these strings as literals in your Python code, best practice is to keep them as resource files in your source directory and access them with pkgutil . pkgutil is part of the standard Python library. The method that is applicable to resources is pkgutil.get_data .","title":"Managing Scripts for Remote Execution"},{"location":"Best_Practices/Managing_Scripts_For_Remote_Execution/#basic-usage","text":"Given the following plugin structure: \u251c\u2500\u2500 plugin_config.yml \u251c\u2500\u2500 schema.json \u2514\u2500\u2500 src \u251c\u2500\u2500 plugin_runner.py \u2514\u2500\u2500 resources \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 get_date.sh Assume SnapshotDefinition is: \"snapshotDefinition\": { \"type\" : \"object\", \"additionalProperties\" : false, \"properties\" : { \"name\": {\"type\": \"string\"}, \"date\": {\"type\": \"string\"} } } and src/resources/get_date.sh contains: 1 2 #!/usr/bin/env bash date If get_date.sh is needed in post_snapshot , it can be retrieved and executed: import pkgutil from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform.exceptions import UserError from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin . linked . post_snapshot () def post_snapshot ( direct_source , repository , source_config ): # Retrieve script contents script_content = pkgutil . get_data ( 'resources' , 'get_date.sh' ) # Execute script on remote host response = libs . run_bash ( direct_source . connection , script_content ) # Fail operation if the timestamp couldn't be retrieved if response . exit_code != 0 : raise UserError ( 'Failed to get date' , 'Make sure the user has the required permissions' , ' {} \\n {} ' . format ( response . stdout , rsponse . stderr )) return SnapshotDefinition ( name = 'Snapshot' , date = response . stdout ) Python's Working Directory This assumes that src/ is Python's current working directory. This is the behavior of the Virtualization Platform. Resources need to be in a Python module pkgutil.get_data cannot retrieve the contents of a resource that is not in a Python package. This means that a resource that is in the first level of your source directory will not be retrievable with pkgutil . Resources must be in a subdirectory of your source directory, and that subdirectory must contain an __init__.py file.","title":"Basic Usage"},{"location":"Best_Practices/Managing_Scripts_For_Remote_Execution/#multi-level-packages","text":"Given the following plugin structure: \u251c\u2500\u2500 plugin_config.yml \u251c\u2500\u2500 schema.json \u2514\u2500\u2500 src \u251c\u2500\u2500 plugin_runner.py \u2514\u2500\u2500 resources \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 database \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 execute_sql.sh \u2514\u2500\u2500 platform \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 get_date.sh The contents of src/resources/platform/get_date.sh can be retrieved with: script_content = pkgutil . get_data ( 'resources.platform' , 'get_date.sh' )","title":"Multi-level Packages"},{"location":"Best_Practices/Sensitive_Data/","text":"Dealing With Sensitive Data \u00b6 Often, a plugin will need to handle sensitive user-provided data. The most common example of this is a database password. Plugins must be careful to handle sensitive data appropriately. Three tips for handling sensitive data are: Tell the Delphix Engine which parts of your data are sensitive. When passing sensitive data to remote plugin library functions (such as run_bash ), use environment variables. Avoid logging, or otherwise writing out the sensitive data. Each of these tips are explained below. Marking Your Data As Sensitive \u00b6 Because the Delphix Engine manages the storing and retrieving of plugin-defined data, it needs to know which pieces of data are sensitive. The plugin does this in its schemas , by using the special password keyword. The following example of a schema defines an object with three properties, one of which is sensitive and tagged with the password keyword: { \"type\" : \"object\" , \"properties\" : { \"db_connectionPort\" : { \"type\" : \"string\" }, \"db_username\" : { \"type\" : \"string\" }, \"db_password\" : { \"type\" : \"string\" , \"format\" : \"password\" } } } This tells the Delphix Engine to take special precautions with this password property, as follows: The Delphix Engine will encrypt the password before storing it, and decrypt it only as necessary to pass back to the plugin. The Delphix Engine will not write this password anywhere (for example, it will not appear in any system logs). The Delphix Engine's UI and CLI will not display the password. Clients of the Delphix Engine's public API will not be able to access the password. Using Environment Variables For Remote Data Passing \u00b6 Sometimes, a plugin will need to pass sensitive data to a remote environment. For example, perhaps a database command needs to be run on a staging environment , and that database command will need to use a password. Example \u00b6 Let us take a look at a very simple example where we need to shutdown a database called \"inventory\" on a target environment by using the db_cmd shutdown inventory command. This command will ask for a password on stdin , and for our example our password is \"hunter2\". If we were running this command by hand, it might look like this: $ db_cmd shutdown inventory Connecting to database instance... Please enter database password: At this point, we would type in \"hunter2\", and the command would proceed to shut down the database. Since a plugin cannot type in the password by hand, it will do something like this instead: $ echo \"hunter2\" | db_cmd shutdown inventory Don't Do This \u00b6 First, let us take a look at how not to do this! Here is a bit of plugin python code that will run the above command. from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . virtual . stop () def my_virtual_stop ( virtual_source , repository , source_config ): # THIS IS INSECURE! DO NOT DO THIS! full_command = \"echo {} | db_cmd shutdown {} \" . format ( password , db_name ) libs . run_bash ( virtual_source . connection , full_command ) This constructs a Python string containing exactly the desired command from above. However, this is not recommended. The problem here is that there is a cleartext password in the Python string. But, this Python string is not treated as sensitive by the Virtualization Platform. For example, suppose the Virtualization Platform cannot make a connection to the target environment. In which case, it will raise an error containing the Python string, so that people will know what command failed. But, in our example, that would result in the password being part of the cleartext error message. Using Environment Variables \u00b6 The Delphix Engine provides a better way to pass sensitive data to remote bash (or powershell) calls: environment variables. Let us look at a different way to run the same command as above. from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . virtual . stop () # Use environment variables to pass sensitive data to remote commands environment_vars = { \"DATABASE_PASSWORD\" : password } full_command = \"echo $DATABASE_PASSWORD | db_cmd shutdown {} \" . format ( db_name ) libs . run_bash ( virtual_source . connection , full_command , variables = environment_vars ) Note We are no longer putting the cleartext password into the Python command string. Instead, we are instructing the Virtualization Platform to put the password into an environment variable on the target environment. The Python command string merely mentions the name of the environment variable, and does not contain the password itself. Once the command runs on the target environment, Bash will substitute in the password, and the database shutdown will run as expected. Unlike with the command string, the Virtualization Platform does treat environment variables as sensitive information, and will not include them in error messages or internal logs, etc. Don't Write Out Sensitive Data \u00b6 Plugin writers are strongly advised to never write out unencrypted sensitive data. This is common-sense general advice that applies to all areas of programming, not just for plugins. However, there are a couple of special concerns for plugins. The Virtualization Platform provides logging capabilities to plugins. The generated logs are unencrypted and not treated as sensitive. Therefore, it is important for plugins to never log sensitive data . In addition, remember that your plugin is not treated as sensitive by the Virtualization Platform. Plugin code is distributed unencrypted, and is viewable in cleartext by Delphix Engine users. Sensitive data such as passwords should never be hard-coded in your plugin code.","title":"Dealing With Sensitive Data"},{"location":"Best_Practices/Sensitive_Data/#dealing-with-sensitive-data","text":"Often, a plugin will need to handle sensitive user-provided data. The most common example of this is a database password. Plugins must be careful to handle sensitive data appropriately. Three tips for handling sensitive data are: Tell the Delphix Engine which parts of your data are sensitive. When passing sensitive data to remote plugin library functions (such as run_bash ), use environment variables. Avoid logging, or otherwise writing out the sensitive data. Each of these tips are explained below.","title":"Dealing With Sensitive Data"},{"location":"Best_Practices/Sensitive_Data/#marking-your-data-as-sensitive","text":"Because the Delphix Engine manages the storing and retrieving of plugin-defined data, it needs to know which pieces of data are sensitive. The plugin does this in its schemas , by using the special password keyword. The following example of a schema defines an object with three properties, one of which is sensitive and tagged with the password keyword: { \"type\" : \"object\" , \"properties\" : { \"db_connectionPort\" : { \"type\" : \"string\" }, \"db_username\" : { \"type\" : \"string\" }, \"db_password\" : { \"type\" : \"string\" , \"format\" : \"password\" } } } This tells the Delphix Engine to take special precautions with this password property, as follows: The Delphix Engine will encrypt the password before storing it, and decrypt it only as necessary to pass back to the plugin. The Delphix Engine will not write this password anywhere (for example, it will not appear in any system logs). The Delphix Engine's UI and CLI will not display the password. Clients of the Delphix Engine's public API will not be able to access the password.","title":"Marking Your Data As Sensitive"},{"location":"Best_Practices/Sensitive_Data/#using-environment-variables-for-remote-data-passing","text":"Sometimes, a plugin will need to pass sensitive data to a remote environment. For example, perhaps a database command needs to be run on a staging environment , and that database command will need to use a password.","title":"Using Environment Variables For Remote Data Passing"},{"location":"Best_Practices/Sensitive_Data/#example","text":"Let us take a look at a very simple example where we need to shutdown a database called \"inventory\" on a target environment by using the db_cmd shutdown inventory command. This command will ask for a password on stdin , and for our example our password is \"hunter2\". If we were running this command by hand, it might look like this: $ db_cmd shutdown inventory Connecting to database instance... Please enter database password: At this point, we would type in \"hunter2\", and the command would proceed to shut down the database. Since a plugin cannot type in the password by hand, it will do something like this instead: $ echo \"hunter2\" | db_cmd shutdown inventory","title":"Example"},{"location":"Best_Practices/Sensitive_Data/#dont-do-this","text":"First, let us take a look at how not to do this! Here is a bit of plugin python code that will run the above command. from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . virtual . stop () def my_virtual_stop ( virtual_source , repository , source_config ): # THIS IS INSECURE! DO NOT DO THIS! full_command = \"echo {} | db_cmd shutdown {} \" . format ( password , db_name ) libs . run_bash ( virtual_source . connection , full_command ) This constructs a Python string containing exactly the desired command from above. However, this is not recommended. The problem here is that there is a cleartext password in the Python string. But, this Python string is not treated as sensitive by the Virtualization Platform. For example, suppose the Virtualization Platform cannot make a connection to the target environment. In which case, it will raise an error containing the Python string, so that people will know what command failed. But, in our example, that would result in the password being part of the cleartext error message.","title":"Don't Do This"},{"location":"Best_Practices/Sensitive_Data/#using-environment-variables","text":"The Delphix Engine provides a better way to pass sensitive data to remote bash (or powershell) calls: environment variables. Let us look at a different way to run the same command as above. from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . virtual . stop () # Use environment variables to pass sensitive data to remote commands environment_vars = { \"DATABASE_PASSWORD\" : password } full_command = \"echo $DATABASE_PASSWORD | db_cmd shutdown {} \" . format ( db_name ) libs . run_bash ( virtual_source . connection , full_command , variables = environment_vars ) Note We are no longer putting the cleartext password into the Python command string. Instead, we are instructing the Virtualization Platform to put the password into an environment variable on the target environment. The Python command string merely mentions the name of the environment variable, and does not contain the password itself. Once the command runs on the target environment, Bash will substitute in the password, and the database shutdown will run as expected. Unlike with the command string, the Virtualization Platform does treat environment variables as sensitive information, and will not include them in error messages or internal logs, etc.","title":"Using Environment Variables"},{"location":"Best_Practices/Sensitive_Data/#dont-write-out-sensitive-data","text":"Plugin writers are strongly advised to never write out unencrypted sensitive data. This is common-sense general advice that applies to all areas of programming, not just for plugins. However, there are a couple of special concerns for plugins. The Virtualization Platform provides logging capabilities to plugins. The generated logs are unencrypted and not treated as sensitive. Therefore, it is important for plugins to never log sensitive data . In addition, remember that your plugin is not treated as sensitive by the Virtualization Platform. Plugin code is distributed unencrypted, and is viewable in cleartext by Delphix Engine users. Sensitive data such as passwords should never be hard-coded in your plugin code.","title":"Don't Write Out Sensitive Data"},{"location":"Best_Practices/Unicode_Data/","text":"Working with Unicode Data \u00b6 To use unicode characters in the plugin code, the following lines should be included at top of the plugin code: #!/usr/bin/env python # -*- coding: utf-8 -*- Otherwise, there may be errors when building the plugin using dvp build or during the execution of a plugin operation. Example \u00b6 #!/usr/bin/env python # -*- coding: utf-8 -*- from dlpx.virtualization.platform import Plugin from dlpx.virtualization import libs from generated.definitions import RepositoryDefinition plugin = Plugin () @plugin . discovery . repository () def repository_discovery ( source_connection ): # Create a repository with name \u2603 command = 'echo \u2603' result = libs . run_bash ( source_connection , command ) return [ RepositoryDefinition ( name = result . stdout )]","title":"Working with Unicode Data"},{"location":"Best_Practices/Unicode_Data/#working-with-unicode-data","text":"To use unicode characters in the plugin code, the following lines should be included at top of the plugin code: #!/usr/bin/env python # -*- coding: utf-8 -*- Otherwise, there may be errors when building the plugin using dvp build or during the execution of a plugin operation.","title":"Working with Unicode Data"},{"location":"Best_Practices/Unicode_Data/#example","text":"#!/usr/bin/env python # -*- coding: utf-8 -*- from dlpx.virtualization.platform import Plugin from dlpx.virtualization import libs from generated.definitions import RepositoryDefinition plugin = Plugin () @plugin . discovery . repository () def repository_discovery ( source_connection ): # Create a repository with name \u2603 command = 'echo \u2603' result = libs . run_bash ( source_connection , command ) return [ RepositoryDefinition ( name = result . stdout )]","title":"Example"},{"location":"Best_Practices/User_Visible_Errors/","text":"User Visible Errors \u00b6 Plugin authors can choose to fail a plugin operation by raising an exception of type UserError with a custom message, action and output for the end user. Fields \u00b6 Field Type Description message String Description of the failure to show the end user. action String Optional . List of actions that the end user could take to fix the problem. If not provided, it defaults to Contact the plugin author to correct the error. output String Optional . Output or stack trace from the failure to give the end user more information so that they can self diagnose. If not provided, it defaults to the stack trace of the failure. Example \u00b6 import pkgutil from dlpx.virtualization.platform import Plugin from generated.definitions import SourceConfigDefinition from dlpx.virtualization.platform.exceptions import UserError plugin = Plugin () @plugin . virtual . start () def start ( virtual_source , repository , source_config ): script_content = pkgutil . get_data ( 'resources' , 'start_database.sh' ) response = libs . run_bash ( virtual_source . connection , script_content ) # Fail operation if the database could not be started if response . exit_code != 0 : raise UserError ( 'Failed to start the database' , 'Make sure the user has appropriate permissions' , ' {} \\n {} ' . format ( response . stdout , response . stderr )) The UI would show the end user if the plugin operation above fails:","title":"User Visible Errors"},{"location":"Best_Practices/User_Visible_Errors/#user-visible-errors","text":"Plugin authors can choose to fail a plugin operation by raising an exception of type UserError with a custom message, action and output for the end user.","title":"User Visible Errors"},{"location":"Best_Practices/User_Visible_Errors/#fields","text":"Field Type Description message String Description of the failure to show the end user. action String Optional . List of actions that the end user could take to fix the problem. If not provided, it defaults to Contact the plugin author to correct the error. output String Optional . Output or stack trace from the failure to give the end user more information so that they can self diagnose. If not provided, it defaults to the stack trace of the failure.","title":"Fields"},{"location":"Best_Practices/User_Visible_Errors/#example","text":"import pkgutil from dlpx.virtualization.platform import Plugin from generated.definitions import SourceConfigDefinition from dlpx.virtualization.platform.exceptions import UserError plugin = Plugin () @plugin . virtual . start () def start ( virtual_source , repository , source_config ): script_content = pkgutil . get_data ( 'resources' , 'start_database.sh' ) response = libs . run_bash ( virtual_source . connection , script_content ) # Fail operation if the database could not be started if response . exit_code != 0 : raise UserError ( 'Failed to start the database' , 'Make sure the user has appropriate permissions' , ' {} \\n {} ' . format ( response . stdout , response . stderr )) The UI would show the end user if the plugin operation above fails:","title":"Example"},{"location":"Best_Practices/Working_with_Powershell/","text":"Error handling in Powershell \u00b6 Info Commands run via run_powershell are executed as a script. The exit code returned by run_powershell as part of the RunPowershellResult is determined by the exit code from the script. PowerShell gives you a few ways to handle errors in your scripts: Set $ErrorActionPreference. This only applies to PowerShell Cmdlets. For scripts or other executables such as sqlcmd, PowerShell will return with exit code 0 even if there is an error, regardless of the value of $ErrorActionPrefe rence. The allowable values for $ErrorActionPreference are: Continue (default) \u2013 Continue even if there is an error. SilentlyContinue \u2013 Acts like Continue with the exception that errors are not displayed Inquire \u2013 Prompts the user in case of error Stop - Stops execution after the first error Use exception handling by using traps and try/catch blocks or if statements to detect errors and return with non-zero exit codes Use custom error handling that can be invoked after launching each command in the script to correctly detect errors. Examples \u00b6 The following example will show you how setting $ErrorActionPreference will return exit codes In the below code, ls nothing123 is expected to fail. ls nothing123 Write-Host \"Test\" Here is the output when the above commands runs on a remote host and the script will return the value of $? to be True eventhough the script failed. ```PS C:\\Users\\dtully\\test> ./test1.ps1 ls : Cannot find path 'C:\\Users\\dtully\\test\\nothing123' because it does not exist. At C:\\Users\\dtully\\test\\test1.ps1:1 char:1 + ls nothing123 + ~~~~~~~~~~~~~ + CategoryInfo : ObjectNotFound: (C:\\Users\\dtully\\test\\nothing123:String) [Get-ChildItem], ItemNotFoundEx ception + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetChildItemCommand PS C:\\Users\\dtully\\test> Write-Host $? True Now lets set $ErrorActionPreference=Stop. ```Windows $ErrorActionPreference = \"Stop\" ls nothing123 Write-Host \"Test\" Now when we run the command again we see the return value of $? to be False. PS C:\\Users\\dtully\\test> ./test1.ps1 ls : Cannot find path 'C:\\Users\\dtully\\test\\nothing123' because it does not exist. At C:\\Users\\dtully\\test\\test1.ps1:2 char:1 + ls nothing123 + ~~~~~~~~~~~~~ + CategoryInfo : ObjectNotFound: (C:\\Users\\dtully\\test\\nothing123:String) [Get-ChildItem], ItemNotFoundException + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetChildItemCommand PS C:\\Users\\dtully\\test> Write-Host $? False The following example shows how you can use the function verifySuccess to detect whether the previous command failed, and if it did print, print an error message and return with an exit code of 1. function die { Write-Error \"Error: $($args[0])\" exit 1 } function verifySuccess { if (!$?) { die \"$($args[0])\" } } Write-Output \"I'd rather be in Hawaii\" verifySuccess \"WRITE_OUTPUT_FAILED\" & C:\\Program Files\\Delphix\\scripts\\myscript.ps1 verifySuccess \"MY_SCRIPT_FAILED\"","title":"Working with Powershell"},{"location":"Best_Practices/Working_with_Powershell/#error-handling-in-powershell","text":"Info Commands run via run_powershell are executed as a script. The exit code returned by run_powershell as part of the RunPowershellResult is determined by the exit code from the script. PowerShell gives you a few ways to handle errors in your scripts: Set $ErrorActionPreference. This only applies to PowerShell Cmdlets. For scripts or other executables such as sqlcmd, PowerShell will return with exit code 0 even if there is an error, regardless of the value of $ErrorActionPrefe rence. The allowable values for $ErrorActionPreference are: Continue (default) \u2013 Continue even if there is an error. SilentlyContinue \u2013 Acts like Continue with the exception that errors are not displayed Inquire \u2013 Prompts the user in case of error Stop - Stops execution after the first error Use exception handling by using traps and try/catch blocks or if statements to detect errors and return with non-zero exit codes Use custom error handling that can be invoked after launching each command in the script to correctly detect errors.","title":"Error handling in Powershell"},{"location":"Best_Practices/Working_with_Powershell/#examples","text":"The following example will show you how setting $ErrorActionPreference will return exit codes In the below code, ls nothing123 is expected to fail. ls nothing123 Write-Host \"Test\" Here is the output when the above commands runs on a remote host and the script will return the value of $? to be True eventhough the script failed. ```PS C:\\Users\\dtully\\test> ./test1.ps1 ls : Cannot find path 'C:\\Users\\dtully\\test\\nothing123' because it does not exist. At C:\\Users\\dtully\\test\\test1.ps1:1 char:1 + ls nothing123 + ~~~~~~~~~~~~~ + CategoryInfo : ObjectNotFound: (C:\\Users\\dtully\\test\\nothing123:String) [Get-ChildItem], ItemNotFoundEx ception + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetChildItemCommand PS C:\\Users\\dtully\\test> Write-Host $? True Now lets set $ErrorActionPreference=Stop. ```Windows $ErrorActionPreference = \"Stop\" ls nothing123 Write-Host \"Test\" Now when we run the command again we see the return value of $? to be False. PS C:\\Users\\dtully\\test> ./test1.ps1 ls : Cannot find path 'C:\\Users\\dtully\\test\\nothing123' because it does not exist. At C:\\Users\\dtully\\test\\test1.ps1:2 char:1 + ls nothing123 + ~~~~~~~~~~~~~ + CategoryInfo : ObjectNotFound: (C:\\Users\\dtully\\test\\nothing123:String) [Get-ChildItem], ItemNotFoundException + FullyQualifiedErrorId : PathNotFound,Microsoft.PowerShell.Commands.GetChildItemCommand PS C:\\Users\\dtully\\test> Write-Host $? False The following example shows how you can use the function verifySuccess to detect whether the previous command failed, and if it did print, print an error message and return with an exit code of 1. function die { Write-Error \"Error: $($args[0])\" exit 1 } function verifySuccess { if (!$?) { die \"$($args[0])\" } } Write-Output \"I'd rather be in Hawaii\" verifySuccess \"WRITE_OUTPUT_FAILED\" & C:\\Program Files\\Delphix\\scripts\\myscript.ps1 verifySuccess \"MY_SCRIPT_FAILED\"","title":"Examples"},{"location":"Building_Your_First_Plugin/Data_Ingestion/","text":"Data Ingestion \u00b6 How Does Delphix Ingest Data? \u00b6 As previously discussed, the Delphix Engine uses the discovery process to learn about datasets that live on a source environment . In this section we will learn how the Delphix Engine uses a two-step process to ingest a dataset. Linking \u00b6 The first step is called linking . This is simply the creation of a new dataset on the Delphix Engine, which is associated with the dataset on the source environment. This new linked dataset is called a dSource . Syncing \u00b6 Immediately after linking, the new dSource is synced for the first time. Syncing is a process by which data from the source environment is copied onto the Delphix Engine. Subsequent syncs may then be periodically performed in order to keep the dSource up-to-date. The details of how this is done varies significantly from plugin to plugin. For example, some plugins will simply copy files from the filesystem. Other plugins might contact a DBMS and instruct it to send backup or replication streams. There are many possibilities here, but they all break down into two main strategies that the plugin author can choose from: direct and staging. With the direct strategy, the plugin is not in charge of the data copying. Instead the Delphix Engine directly pulls raw data from the source environment. The plugin merely provides the location of the data. This is a very simple strategy, and is also quite limiting. For our first plugin, we will be using the more flexible staging strategy. With this strategy, the Delphix Engine uses NFS for Unix environments (or iSCSI on Windows environments) to mount storage onto a staging environment . Our plugin will then be in full control of how to get data from the source environment onto this storage mount. With the staging strategy, there are two types of syncs: sync and resync. A sync is used to ingestion incremental changes while a resync is used to re-ingest all the data for the dSource. For databases, this could mean re-ingesting from a full database backup to reset the dSource. A sync and a resync execute the same plugin operations and are differentiated by a boolean flag in the snapshot_parameters argument passed into linked.pre_snapshot and linked.post_snapshot . A regular sync is the default and is executed as part of policy driven syncs. A resync is only executed during initial ingestion or if the Delphix user manually starts one. The customer can manually trigger a resync via the UI by selecting the dSource, going to more options and selecting Resynchronize dSource . Gotcha Although it is not common, it is entirely possible that the staging environment is the same as the source environment. Be careful not to assume otherwise in your plugins. Our Syncing Strategy \u00b6 For our purposes here in this intro plugin, we will use a simple strategy. We won't do anything with the resync snapshot parameter and simply copy files from the filesystem on the source environment onto the NFS mount on the staging environment. We will do this by running the Unix tool rsync from our staging environment, and rely on passwordless SSH to connect to the source environment. Info This plugin is assuming that rsync is installed on the staging host, and that the staging host user is able to SSH into the source host without having to type in a password. A more full-featured plugin would test these assumptions, usually as part of discovery. In the special case mentioned above, where the staging environment is the same as the source environment, we could likely do something more efficient. However, for simplicity's sake, we won't do that here. Defining Your Linked Source Data Format \u00b6 In order to be able to successfully do the copying required, plugins might need to get some information from the end-user of your plugin. In our case, we need to tell rsync how to access the files. This means we need to know the source environment's IP address (or domain name), the username we need to connect as, and finally the location where the files live. Again, we will be using a JSON schema to define the data format. The user will be presented with a UI that lets them provide all the information our schema specifies. Open up schema.json in your editor/IDE. Locate the LinkedSourceDefinition and replace it with the following schema: \"linkedSourceDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"sourceAddress\" , \"username\" , \"mountLocation\" ], \"properties\" : { \"sourceAddress\" : { \"type\" : \"string\" , \"prettyName\" : \"Host from which to copy\" , \"description\" : \"IP or FQDN of host from which to copy\" }, \"username\" : { \"type\" : \"string\" , \"prettyName\" : \"Username on Source Host\" , \"description\" : \"Username for making SSH connection to source host\" }, \"mountLocation\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" , \"prettyName\" : \"Mount Location on Staging Host\" , \"description\" : \"Where to mount storage onto the staging host while syncing\" } } } , Info As will be explained later, this schema will be used to generate Python code. All names in the autogenerated Python code will use lower_case_with_underscores as attribute names as per Python variable naming conventions. That is, if we were to use mountLocation as the schema property name, it would be called mount_location in the generated Python code. With this schema, the user will be required to provide the source username, the source's IP address, and the staging mount location as part of the linking process. Implementing Syncing in Your Plugin \u00b6 There are three things we must do to implement syncing. First, we need to tell the Delphix Engine where to mount storage onto the staging environment. Next we need to actually do the work of copying data onto that mounted storage. Finally, we need to generate any snapshot-related data. Mount Specification \u00b6 Before syncing can begin, the Delphix Engine needs to mount some storage onto the staging host. Since different plugins can have different requirements about where exactly this mount lives, it is up to the plugin to specify this location. As mentioned above, our simple plugin will get this location from the user. Open up the plugin_runner.py file and find the linked_mount_specification function (which was generated by dvp init ). Replace it with the following code: @plugin.linked.mount_specification() def linked_mount_specification(staged_source, repository): mount_location = staged_source.parameters.mount_location mount = Mount(staged_source.staged_connection.environment, mount_location) return MountSpecification([mount]) Let's take this line-by-line to see what's going on here. @plugin.linked.mount_specification() This decorator announces that the following function is the code that handles the mount_specification operation. This is what allows the Delphix Engine to know which function to call when it's time to learn where to mount. Every operation definition will begin with a similar decorator. def linked_mount_specification(staged_source, repository): This begins a Python function definition. We chose to call it linked_mount_specification , but we could have chosen any name at all. This function accepts two arguments, one giving information about the linked source, and one giving information about the associated repository. mount_location = staged_source.parameters.mount_location The staged_source input argument contains an attribute called parameters . This in turn contains all of the properties defined by the linkedSourceDefinition schema. So, in our case, that means it will contain attributes called source_address , username , and mount_location . Note how any attribute defined in camelCase in the schema is converted to variable_with_underscores . This line simply retrieves the user-provided mount location and saves it in a local variable. mount = Mount(staged_source.staged_connection.environment, mount_location) This line constructs a new object from the Mount class . This class holds details about how Delphix Engine storage is mounted onto remote environments. Here, we create a mount object that says to mount onto the staging environment, at the location specified by the user. return MountSpecification([mount]) On the line just before this one, we created an object that describes a single mount. Now, we must return a full mount specification . In general, a mount specification is a collection of mounts. But, in our case, we just have one single mount. Therefore, we use an array with only one item it in -- namely, the one single mount object we created just above. Data Copying \u00b6 As explained here , the Delphix Engine will always run the plugin's preSnapshot operation just before taking a snapshot of the dsource. That means our preSnapshot operation has to get the NFS share into the desired state. For us, that means that's the time to do our data copy. Unlike the previous operations we've seen so far, the pre-snapshot operation will not be autogenerated by dvp init . So, we will need to add one ourselves. Open up the plugin_runner.py file. First, we'll add a new import line near the top of the file, so that we can use Delphix's platform libraries and raise user visible errors (explained below). from dlpx.virtualization import libs from dlpx.virtualization.platform.exceptions import UserError Next, we'll add a new function: @plugin . linked . pre_snapshot () def copy_data_from_source ( staged_source , repository , source_config , snapshot_parameters ): stage_mount_path = staged_source . mount . mount_path data_location = \" {} @ {} : {} \" . format ( staged_source . parameters . username , staged_source . parameters . source_address , source_config . path ) rsync_command = \"rsync -r {} {} \" . format ( data_location , stage_mount_path ) result = libs . run_bash ( staged_source . staged_connection , rsync_command ) if result . exit_code != 0 : raise UserError ( \"Could not copy files.\" , \"Ensure that passwordless SSH works for {} .\" . format ( staged_source . parameters . source_address ), result . stderr ) Let's walk through this function and see what's going on stage_mount_path = staged_source . mount . mount_path The staged_source argument contains information about the current mount location. Here we save that to a local variable for convenience. data_location = \" {} @ {} : {} \" . format ( staged_source . parameters . username , staged_source . parameters . source_address , source_config . path ) This code creates a Python string that represents the location of the data that we want to ingest. This is in the form <user>@<host>:<path> . For example jdoe@sourcehost.mycompany.com:/bin . As before with mountLocation , we have defined our schemas such that these three pieces of information were provided by the user. Here we're just putting them into a format that rsync will understand. rsync_command = \"rsync -r {} {} \" . format ( data_location , stage_mount_path ) This line is the actual Bash command that we'll be running on the staging host. This will look something like rsync -r user@host:/source/path /staging/mount/path . result = libs . run_bash ( staged_source . staged_connection , rsync_command ) This is an example of a platform library function, where we ask the Virtualization Platform to do some work on our behalf. In this case, we're asking the platform to run our Bash command on the staging environment. For full details on the run_bash platform library function and others, see this reference . if result . exit_code != 0 : raise UserError ( \"Could not copy files.\" , \"Ensure that passwordless SSH works for {} .\" . format ( staged_source . parameters . source_address ), result . stderr ) Finally, we check to see if our Bash command actually worked okay. If not, we raise an error message, and describe one possible problem for the user to investigate. For more details on raising user visible errors, see this reference . Saving Snapshot Data \u00b6 Whenever the Delphix Engine takes a snapshot of a dSource or VDB, the plugin has the chance to save any information it likes alongside that snapshot. Later, if the snapshot is ever used to provision a new VDB, the plugin can use the previously-saved information to help get the new VDB ready for use. The format of this data is controlled by the plugin's snapshotDefinition schema. In our case, we don't have any data we need to save. So, there's not much to do here. We will not modify the blank schema that was created by dvp init . We do still need to provide python function for the engine to call, but we don't have to do much. In fact, the default implementation that was generated by dvp init will work just fine for our purposes: @plugin . linked . post_snapshot () def linked_post_snapshot ( staged_source , repository , source_config , snapshot_parameters ): return SnapshotDefinition () The only thing this code is doing is creating a new object using our (empty) snapshot definition, and returning that new empty object. How to Link and Sync in the Delphix Engine \u00b6 Let's try it out and make sure this works! Prerequisites You should already have a repository and source config set up from the previous page. You can optionally set up a new staging environment. Or, you can simply re-use your source environment for staging. Procedure Note Recall that, for simplicity's sake, this plugin requires that passwordless SSH is set up between your staging and source environments. You may want to verify this before continuing. As before, use dvp build and dvp upload to get your latest plugin changes installed onto the Delphix Engine. Go to Manage > Environments , select your source environment, and then go to the Databases tab. Find Repository for our First Plugin , and your source config underneath it. From your source config click Add dSource . This will begin the linking process. The first screen you see should ask for the properties that you recently added to your linkedSourceDefinition . Walk through the remainder of the screens and hit Submit . This will kick off the initial link and first sync. You can confirm that your new dSource was added successfully by going to Manage > Datasets . After you have finished entering this information, the initial sync process will begin. This is what will call your pre-snapshot operation, thus copying data. Gotcha Manually creating a dSource sets your plugin\u2019s linked source schema in stone, and you will have to recreate the dSource in order to modify your schema. We will cover how to deal with this correctly later, in the upgrade section . For now, if you need to change your plugin's linked source schema, you will have to first delete any dSources you have manually added.","title":"Data Ingestion"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#data-ingestion","text":"","title":"Data Ingestion"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#how-does-delphix-ingest-data","text":"As previously discussed, the Delphix Engine uses the discovery process to learn about datasets that live on a source environment . In this section we will learn how the Delphix Engine uses a two-step process to ingest a dataset.","title":"How Does Delphix Ingest Data?"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#linking","text":"The first step is called linking . This is simply the creation of a new dataset on the Delphix Engine, which is associated with the dataset on the source environment. This new linked dataset is called a dSource .","title":"Linking"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#syncing","text":"Immediately after linking, the new dSource is synced for the first time. Syncing is a process by which data from the source environment is copied onto the Delphix Engine. Subsequent syncs may then be periodically performed in order to keep the dSource up-to-date. The details of how this is done varies significantly from plugin to plugin. For example, some plugins will simply copy files from the filesystem. Other plugins might contact a DBMS and instruct it to send backup or replication streams. There are many possibilities here, but they all break down into two main strategies that the plugin author can choose from: direct and staging. With the direct strategy, the plugin is not in charge of the data copying. Instead the Delphix Engine directly pulls raw data from the source environment. The plugin merely provides the location of the data. This is a very simple strategy, and is also quite limiting. For our first plugin, we will be using the more flexible staging strategy. With this strategy, the Delphix Engine uses NFS for Unix environments (or iSCSI on Windows environments) to mount storage onto a staging environment . Our plugin will then be in full control of how to get data from the source environment onto this storage mount. With the staging strategy, there are two types of syncs: sync and resync. A sync is used to ingestion incremental changes while a resync is used to re-ingest all the data for the dSource. For databases, this could mean re-ingesting from a full database backup to reset the dSource. A sync and a resync execute the same plugin operations and are differentiated by a boolean flag in the snapshot_parameters argument passed into linked.pre_snapshot and linked.post_snapshot . A regular sync is the default and is executed as part of policy driven syncs. A resync is only executed during initial ingestion or if the Delphix user manually starts one. The customer can manually trigger a resync via the UI by selecting the dSource, going to more options and selecting Resynchronize dSource . Gotcha Although it is not common, it is entirely possible that the staging environment is the same as the source environment. Be careful not to assume otherwise in your plugins.","title":"Syncing"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#our-syncing-strategy","text":"For our purposes here in this intro plugin, we will use a simple strategy. We won't do anything with the resync snapshot parameter and simply copy files from the filesystem on the source environment onto the NFS mount on the staging environment. We will do this by running the Unix tool rsync from our staging environment, and rely on passwordless SSH to connect to the source environment. Info This plugin is assuming that rsync is installed on the staging host, and that the staging host user is able to SSH into the source host without having to type in a password. A more full-featured plugin would test these assumptions, usually as part of discovery. In the special case mentioned above, where the staging environment is the same as the source environment, we could likely do something more efficient. However, for simplicity's sake, we won't do that here.","title":"Our Syncing Strategy"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#defining-your-linked-source-data-format","text":"In order to be able to successfully do the copying required, plugins might need to get some information from the end-user of your plugin. In our case, we need to tell rsync how to access the files. This means we need to know the source environment's IP address (or domain name), the username we need to connect as, and finally the location where the files live. Again, we will be using a JSON schema to define the data format. The user will be presented with a UI that lets them provide all the information our schema specifies. Open up schema.json in your editor/IDE. Locate the LinkedSourceDefinition and replace it with the following schema: \"linkedSourceDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"sourceAddress\" , \"username\" , \"mountLocation\" ], \"properties\" : { \"sourceAddress\" : { \"type\" : \"string\" , \"prettyName\" : \"Host from which to copy\" , \"description\" : \"IP or FQDN of host from which to copy\" }, \"username\" : { \"type\" : \"string\" , \"prettyName\" : \"Username on Source Host\" , \"description\" : \"Username for making SSH connection to source host\" }, \"mountLocation\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" , \"prettyName\" : \"Mount Location on Staging Host\" , \"description\" : \"Where to mount storage onto the staging host while syncing\" } } } , Info As will be explained later, this schema will be used to generate Python code. All names in the autogenerated Python code will use lower_case_with_underscores as attribute names as per Python variable naming conventions. That is, if we were to use mountLocation as the schema property name, it would be called mount_location in the generated Python code. With this schema, the user will be required to provide the source username, the source's IP address, and the staging mount location as part of the linking process.","title":"Defining Your Linked Source Data Format"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#implementing-syncing-in-your-plugin","text":"There are three things we must do to implement syncing. First, we need to tell the Delphix Engine where to mount storage onto the staging environment. Next we need to actually do the work of copying data onto that mounted storage. Finally, we need to generate any snapshot-related data.","title":"Implementing Syncing in Your Plugin"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#mount-specification","text":"Before syncing can begin, the Delphix Engine needs to mount some storage onto the staging host. Since different plugins can have different requirements about where exactly this mount lives, it is up to the plugin to specify this location. As mentioned above, our simple plugin will get this location from the user. Open up the plugin_runner.py file and find the linked_mount_specification function (which was generated by dvp init ). Replace it with the following code: @plugin.linked.mount_specification() def linked_mount_specification(staged_source, repository): mount_location = staged_source.parameters.mount_location mount = Mount(staged_source.staged_connection.environment, mount_location) return MountSpecification([mount]) Let's take this line-by-line to see what's going on here. @plugin.linked.mount_specification() This decorator announces that the following function is the code that handles the mount_specification operation. This is what allows the Delphix Engine to know which function to call when it's time to learn where to mount. Every operation definition will begin with a similar decorator. def linked_mount_specification(staged_source, repository): This begins a Python function definition. We chose to call it linked_mount_specification , but we could have chosen any name at all. This function accepts two arguments, one giving information about the linked source, and one giving information about the associated repository. mount_location = staged_source.parameters.mount_location The staged_source input argument contains an attribute called parameters . This in turn contains all of the properties defined by the linkedSourceDefinition schema. So, in our case, that means it will contain attributes called source_address , username , and mount_location . Note how any attribute defined in camelCase in the schema is converted to variable_with_underscores . This line simply retrieves the user-provided mount location and saves it in a local variable. mount = Mount(staged_source.staged_connection.environment, mount_location) This line constructs a new object from the Mount class . This class holds details about how Delphix Engine storage is mounted onto remote environments. Here, we create a mount object that says to mount onto the staging environment, at the location specified by the user. return MountSpecification([mount]) On the line just before this one, we created an object that describes a single mount. Now, we must return a full mount specification . In general, a mount specification is a collection of mounts. But, in our case, we just have one single mount. Therefore, we use an array with only one item it in -- namely, the one single mount object we created just above.","title":"Mount Specification"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#data-copying","text":"As explained here , the Delphix Engine will always run the plugin's preSnapshot operation just before taking a snapshot of the dsource. That means our preSnapshot operation has to get the NFS share into the desired state. For us, that means that's the time to do our data copy. Unlike the previous operations we've seen so far, the pre-snapshot operation will not be autogenerated by dvp init . So, we will need to add one ourselves. Open up the plugin_runner.py file. First, we'll add a new import line near the top of the file, so that we can use Delphix's platform libraries and raise user visible errors (explained below). from dlpx.virtualization import libs from dlpx.virtualization.platform.exceptions import UserError Next, we'll add a new function: @plugin . linked . pre_snapshot () def copy_data_from_source ( staged_source , repository , source_config , snapshot_parameters ): stage_mount_path = staged_source . mount . mount_path data_location = \" {} @ {} : {} \" . format ( staged_source . parameters . username , staged_source . parameters . source_address , source_config . path ) rsync_command = \"rsync -r {} {} \" . format ( data_location , stage_mount_path ) result = libs . run_bash ( staged_source . staged_connection , rsync_command ) if result . exit_code != 0 : raise UserError ( \"Could not copy files.\" , \"Ensure that passwordless SSH works for {} .\" . format ( staged_source . parameters . source_address ), result . stderr ) Let's walk through this function and see what's going on stage_mount_path = staged_source . mount . mount_path The staged_source argument contains information about the current mount location. Here we save that to a local variable for convenience. data_location = \" {} @ {} : {} \" . format ( staged_source . parameters . username , staged_source . parameters . source_address , source_config . path ) This code creates a Python string that represents the location of the data that we want to ingest. This is in the form <user>@<host>:<path> . For example jdoe@sourcehost.mycompany.com:/bin . As before with mountLocation , we have defined our schemas such that these three pieces of information were provided by the user. Here we're just putting them into a format that rsync will understand. rsync_command = \"rsync -r {} {} \" . format ( data_location , stage_mount_path ) This line is the actual Bash command that we'll be running on the staging host. This will look something like rsync -r user@host:/source/path /staging/mount/path . result = libs . run_bash ( staged_source . staged_connection , rsync_command ) This is an example of a platform library function, where we ask the Virtualization Platform to do some work on our behalf. In this case, we're asking the platform to run our Bash command on the staging environment. For full details on the run_bash platform library function and others, see this reference . if result . exit_code != 0 : raise UserError ( \"Could not copy files.\" , \"Ensure that passwordless SSH works for {} .\" . format ( staged_source . parameters . source_address ), result . stderr ) Finally, we check to see if our Bash command actually worked okay. If not, we raise an error message, and describe one possible problem for the user to investigate. For more details on raising user visible errors, see this reference .","title":"Data Copying"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#saving-snapshot-data","text":"Whenever the Delphix Engine takes a snapshot of a dSource or VDB, the plugin has the chance to save any information it likes alongside that snapshot. Later, if the snapshot is ever used to provision a new VDB, the plugin can use the previously-saved information to help get the new VDB ready for use. The format of this data is controlled by the plugin's snapshotDefinition schema. In our case, we don't have any data we need to save. So, there's not much to do here. We will not modify the blank schema that was created by dvp init . We do still need to provide python function for the engine to call, but we don't have to do much. In fact, the default implementation that was generated by dvp init will work just fine for our purposes: @plugin . linked . post_snapshot () def linked_post_snapshot ( staged_source , repository , source_config , snapshot_parameters ): return SnapshotDefinition () The only thing this code is doing is creating a new object using our (empty) snapshot definition, and returning that new empty object.","title":"Saving Snapshot Data"},{"location":"Building_Your_First_Plugin/Data_Ingestion/#how-to-link-and-sync-in-the-delphix-engine","text":"Let's try it out and make sure this works! Prerequisites You should already have a repository and source config set up from the previous page. You can optionally set up a new staging environment. Or, you can simply re-use your source environment for staging. Procedure Note Recall that, for simplicity's sake, this plugin requires that passwordless SSH is set up between your staging and source environments. You may want to verify this before continuing. As before, use dvp build and dvp upload to get your latest plugin changes installed onto the Delphix Engine. Go to Manage > Environments , select your source environment, and then go to the Databases tab. Find Repository for our First Plugin , and your source config underneath it. From your source config click Add dSource . This will begin the linking process. The first screen you see should ask for the properties that you recently added to your linkedSourceDefinition . Walk through the remainder of the screens and hit Submit . This will kick off the initial link and first sync. You can confirm that your new dSource was added successfully by going to Manage > Datasets . After you have finished entering this information, the initial sync process will begin. This is what will call your pre-snapshot operation, thus copying data. Gotcha Manually creating a dSource sets your plugin\u2019s linked source schema in stone, and you will have to recreate the dSource in order to modify your schema. We will cover how to deal with this correctly later, in the upgrade section . For now, if you need to change your plugin's linked source schema, you will have to first delete any dSources you have manually added.","title":"How to Link and Sync in the Delphix Engine"},{"location":"Building_Your_First_Plugin/Discovery/","text":"Discovery \u00b6 What is Discovery? \u00b6 In order to ingest data from a source environment, the Delphix Engine first needs to learn information about the data: Where does it live? How can it be accessed? What is it called? Discovery is the process by which the Delphix Engine learns about remote data. Discovery can be either: automatic \u2014 where the plugin finds the remote data on its own manual \u2014 where the user tells us about the remote data For our first plugin, we will be using a mix of these two techniques. Source Configs and Repositories \u00b6 What are Source Configs and Repositories? \u00b6 A source config is a collection of information that Delphix uses to represent a dataset. Different plugins will have different ideas about what a \"dataset\" is (an entire database? a set of config files? an application?). For our first plugin, it is simply a directory tree on the filesystem of the remote environment. A repository represents what you might call \"data dependencies\" -- anything installed on the remote host that the dataset depends on. For example, if you are working with a Postgres database, then your repository will represent an installation of a particular version of the Postgres DBMS. In this plugin, we do not have any special dependencies, except for the simple existence of the unix system on which the directory lives. We will be using automatic discovery for our repositories, and manual discovery for our source configs. This is the default configuration that is created by dvp init , so there is nothing further we need to do here. Defining Your Data Formats \u00b6 Because each plugin will have different ideas about what a repository or source config represents, different plugins will have different sets of information that they need to collect and store. Delphix needs to know the format of this information. How many pieces of information are collected? What are they called? Are they strings? Numbers? For our first plugin, we do not need a lot of information. We use no special information about our repositories (except some way for the user to identify them). For source configs, all we need to know is the path to the directory from which we will be ingesting data. The plugin needs to describe all of this to the Delphix Engine, and it does so using schemas . Recall that when we ran dvp init , a file full of bare-bones schemas was created. As we build up our first toolkit, we will be augmenting these schemas to serve our needs. Repository Schema \u00b6 Open up the schema.json file in your editor/IDE and locate repositoryDefinition , it should look like this: { \"repositoryDefinition\" : { \"type\" : \"object\" , \"properties\" : { \"name\" : { \"type\" : \"string\" } }, \"nameField\" : \"name\" , \"identityFields\" : [ \"name\" ] } } Since we do not have any special dependencies, we can just leave it as-is. For detailed information about exactly how repository schemas work, see the reference page . In brief, what we are doing here is saying that each of our repositories will have a single property called name , which will be used both as a unique identifier and as the user-visible name of the repository. Source Config Schema \u00b6 For source configs, the bare-bones schema is not going to be good enough. Recall that for us, a source config represents a directory tree on a remote environment. Locate the sourceConfigDefinition inside the schema.json file and modify the definition so it looks like this: \"sourceConfigDefinition\" : { \"type\" : \"object\" , \"required\" : [ \"name\" , \"path\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" , \"prettyName\" : \"Dataset Name\" , \"description\" : \"User-visible name for this dataset\" }, \"path\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" , \"prettyName\" : \"Path\" , \"description\" : \"Full path to data location on the remote environment\" } }, \"nameField\" : \"name\" , \"identityFields\" : [ \"path\" ] } , Now, we have two properties, a property name serving as the user-visible name of the source config and path which tells us where the data lives on the remote host. Note we are using path as the unique identifier. Because we are using manual discovery, the end user is going to be responsible for filling in values for name and path . So, we have added some things to our schema that we did not need for repositories. The prettyName and description entries will be used by the UI to tell the user what these fields mean. Because we set additionalProperties to false , this will prevent users from supplying properties other than name and path . Finally, we have specified that the path property must be a well-formatted Unix path. This allows the UI to enforce that the format is correct before the user is allowed to proceed. (Note this only enforces the format, and does not actually check to see if the path really exists on some remote environment!) Refer to the reference page for Schemas for more details about these entries, and for other things that you can do in these schemas. Implementing Discovery in Your Plugin \u00b6 About Python Code \u00b6 As described in the overview section, plugins customize the behavior of the Delphix Engine by providing Python code. Each customizable piece of behavior is called a \"plugin operation\". The plugin provides separate Python functions for each of the operations that it wants to customize. Right now, we are concerned with discovery. There are two customizable operations related to automatic discovery, one for repositories and one for source configs. In both cases, the job of the Python method is to automatically collect whatever information the schemas (see above) require, and to return that information to the Delphix Engine. The Delphix Engine will run these customized operations whenever a new environment is added, or when an existing environment is rediscovered. A Look at the Generated Code \u00b6 Recall that the dvp init command we ran created a file called src/plugin_runner.py . Open this file in your editor/IDE. You will see that this file already contains a bunch of Python code. Let's take a look at the first three blocks of code in this file. from dlpx.virtualization.platform import Mount , MountSpecification , Plugin from generated.definitions import ( RepositoryDefinition , SourceConfigDefinition , SnapshotDefinition , ) These import lines make certain functionality available to our Python code. Some of this functionality will be used just below, as we implement discovery. Others will be used later on, as we implement ingestion and provisioning. Later, you'll add more import s to unlock more functionality. plugin = Plugin () This line creates a Python object which allows us to define our plugin types. We have the ability to do this because of the import Plugin statement above. This object is stored in a variable we have elected to call plugin . We are free to call this variable anything we want, so long as we also change the entryPoint line in the plugin_config.yml file. For this example, we will just leave it as plugin . # # Below is an example of the repository discovery operation. # # NOTE: The decorators are defined on the 'plugin' object created above. # # Mark the function below as the operation that does repository discovery. @plugin . discovery . repository () def repository_discovery ( source_connection ): # # This is an object generated from the repositoryDefinition schema. # In order to use it locally you must run the 'build -g' command provided # by the SDK tools from the plugin's root directory. # return [ RepositoryDefinition ( name = '1e87dc30-3cdb-4f0a-9634-07ce017d20d1' )] This is our first plugin operation . In this case, it's defining what will happen when the Delphix Engine wants to discover repositories on an environment. Let's take a look at this code line-by-line @plugin . discovery . repository () def repository_discovery ( source_connection ): This begins the definition of a function called repository_discovery . We are using a Python decorator which signals to the Delphix Engine that this is the function which should be called when it is time to do repository discovery. The actual name of the function doesn't matter here. Note that we are using our plugin variable here as part of the decorator. The Delphix Engine will pass us information about the source environment in an argument called source_connection . Warning The name of this input argument matters. That is, you'll always need to have an argument called source_connection here. Each plugin operation has its own set of required argument names. For details on which arguments apply to which operations, see the reference section . return [ RepositoryDefinition ( name = '1e87dc30-3cdb-4f0a-9634-07ce017d20d1' )] This creates and returns a Python object that corresponds to the format defined by our repository schema. Because out repository has exactly one string property called name , therefore this Python object has one property called name . Notice that the code generator has filled in the value of name with a random string. This results in a plugin operation that works, but which will not be very helpful for the user. We'll change this later. The rest of the file contains more plugin operations, and we'll be modifying them later. Repository Discovery \u00b6 Now, we need to modify the provided repository discovery operation. This operation will examine a remote environment, find any repositories, and return information about them to the Delphix Engine. As a reminder, our only external dependency on the remote environment is simply the existence of a filesystem. Since every Unix host has a filesystem, that means we will have exactly one repository per remote environment. Therefore, our repository discovery operation can be very simple. In fact, as we saw above, the default-generated repository_discovery function does almost exactly what we want -- it returns one single repository for any Unix host that it is asked to work with. The only problem with it is that it uses unhelpful name. That's really easy to change! Replace or modify repository_discovery so it looks like this: @plugin . discovery . repository () def repository_discovery ( source_connection ): repository = RepositoryDefinition ( 'Repository for our First Plugin' ) return [ repository ] Tip Be careful to always use consistent indentation in Python code! Source Config Discovery \u00b6 For source configs, we will rely solely on manual discovery. Therefore, the user will tell us which directories they want to ingest from. We still have to define a source config discovery operation -- it just won't need to do much. The job of this operation is to return only source configs associated with the given repository . This function will be called once per repository. In our case, that means it will only be called once. Because we want to supply no automatically-discovered source configs, this function should simply returns an empty list. In fact, dvp init has already generated a function for us that does exactly this. @plugin . discovery . source_config () def source_config_discovery ( source_connection , repository ): return [] If we wanted to do automatic discovery of source configs, we'd modify this function. But, for our purposes now, the existing code is fine and we don't need to change anything. How to Run Discovery in the Delphix Engine \u00b6 Let us make sure discovery works! Run the dvp build commands, as before. This will build the plugin, with all of the new changes, and create an artifact. Run dvp upload -e <engine> -u <user> , as before. This will get all the new changes onto the Delphix Engine. Once the new plugin is uploaded, add a remote unix environment to your engine. To do this, go to Manage > Environments , chose Add Environment from the menu, answer the questions, and Submit . (If you already have an environment set up, you can just refresh it instead). To keep an eye on this discovery process, you may need to open the Actions tab on the UI. If any errors happen, they will be reported here. After the automatic discovery process completes, go to the Databases tab. You will see an entry for Repository For Our First Plugin . This is the repository you created in your Python code. Notice that it says No databases found on installation . This is because we chose not to do automatic source config discovery. However, because we have allowed manual source config discovery, you can add your own entries by clicking the plus sign ( Add Database ). Complete the information in the Add Database dialog and click Add. This should all look familiar. It is precisely what we defined in our source config schema. As expected, there are two entries, one for our name property, and one for path . For example, in the above screenshot, we are specifying that we want to sync the /bin directory from the remote host, and we want to call it Binaries . You can pick any directory and name that you want. Once you have added one or more source configs, you will be able to sync. This is covered on the next page. Warning Once you have automatically or manually created source configs, you will not be allowed to modify your plugin's source config schema. We will cover how to deal with this later in the upgrade section . For now, if you need to change your plugin's source config schema: You will have to delete any source configs you have manually added. Delete the plugin and its corresponding objects (dSources, Virtual Sources, etc) if the source configs were manually discovered.","title":"Discovery"},{"location":"Building_Your_First_Plugin/Discovery/#discovery","text":"","title":"Discovery"},{"location":"Building_Your_First_Plugin/Discovery/#what-is-discovery","text":"In order to ingest data from a source environment, the Delphix Engine first needs to learn information about the data: Where does it live? How can it be accessed? What is it called? Discovery is the process by which the Delphix Engine learns about remote data. Discovery can be either: automatic \u2014 where the plugin finds the remote data on its own manual \u2014 where the user tells us about the remote data For our first plugin, we will be using a mix of these two techniques.","title":"What is Discovery?"},{"location":"Building_Your_First_Plugin/Discovery/#source-configs-and-repositories","text":"","title":"Source Configs and Repositories"},{"location":"Building_Your_First_Plugin/Discovery/#what-are-source-configs-and-repositories","text":"A source config is a collection of information that Delphix uses to represent a dataset. Different plugins will have different ideas about what a \"dataset\" is (an entire database? a set of config files? an application?). For our first plugin, it is simply a directory tree on the filesystem of the remote environment. A repository represents what you might call \"data dependencies\" -- anything installed on the remote host that the dataset depends on. For example, if you are working with a Postgres database, then your repository will represent an installation of a particular version of the Postgres DBMS. In this plugin, we do not have any special dependencies, except for the simple existence of the unix system on which the directory lives. We will be using automatic discovery for our repositories, and manual discovery for our source configs. This is the default configuration that is created by dvp init , so there is nothing further we need to do here.","title":"What are Source Configs and Repositories?"},{"location":"Building_Your_First_Plugin/Discovery/#defining-your-data-formats","text":"Because each plugin will have different ideas about what a repository or source config represents, different plugins will have different sets of information that they need to collect and store. Delphix needs to know the format of this information. How many pieces of information are collected? What are they called? Are they strings? Numbers? For our first plugin, we do not need a lot of information. We use no special information about our repositories (except some way for the user to identify them). For source configs, all we need to know is the path to the directory from which we will be ingesting data. The plugin needs to describe all of this to the Delphix Engine, and it does so using schemas . Recall that when we ran dvp init , a file full of bare-bones schemas was created. As we build up our first toolkit, we will be augmenting these schemas to serve our needs.","title":"Defining Your Data Formats"},{"location":"Building_Your_First_Plugin/Discovery/#repository-schema","text":"Open up the schema.json file in your editor/IDE and locate repositoryDefinition , it should look like this: { \"repositoryDefinition\" : { \"type\" : \"object\" , \"properties\" : { \"name\" : { \"type\" : \"string\" } }, \"nameField\" : \"name\" , \"identityFields\" : [ \"name\" ] } } Since we do not have any special dependencies, we can just leave it as-is. For detailed information about exactly how repository schemas work, see the reference page . In brief, what we are doing here is saying that each of our repositories will have a single property called name , which will be used both as a unique identifier and as the user-visible name of the repository.","title":"Repository Schema"},{"location":"Building_Your_First_Plugin/Discovery/#source-config-schema","text":"For source configs, the bare-bones schema is not going to be good enough. Recall that for us, a source config represents a directory tree on a remote environment. Locate the sourceConfigDefinition inside the schema.json file and modify the definition so it looks like this: \"sourceConfigDefinition\" : { \"type\" : \"object\" , \"required\" : [ \"name\" , \"path\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" , \"prettyName\" : \"Dataset Name\" , \"description\" : \"User-visible name for this dataset\" }, \"path\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" , \"prettyName\" : \"Path\" , \"description\" : \"Full path to data location on the remote environment\" } }, \"nameField\" : \"name\" , \"identityFields\" : [ \"path\" ] } , Now, we have two properties, a property name serving as the user-visible name of the source config and path which tells us where the data lives on the remote host. Note we are using path as the unique identifier. Because we are using manual discovery, the end user is going to be responsible for filling in values for name and path . So, we have added some things to our schema that we did not need for repositories. The prettyName and description entries will be used by the UI to tell the user what these fields mean. Because we set additionalProperties to false , this will prevent users from supplying properties other than name and path . Finally, we have specified that the path property must be a well-formatted Unix path. This allows the UI to enforce that the format is correct before the user is allowed to proceed. (Note this only enforces the format, and does not actually check to see if the path really exists on some remote environment!) Refer to the reference page for Schemas for more details about these entries, and for other things that you can do in these schemas.","title":"Source Config Schema"},{"location":"Building_Your_First_Plugin/Discovery/#implementing-discovery-in-your-plugin","text":"","title":"Implementing Discovery in Your Plugin"},{"location":"Building_Your_First_Plugin/Discovery/#about-python-code","text":"As described in the overview section, plugins customize the behavior of the Delphix Engine by providing Python code. Each customizable piece of behavior is called a \"plugin operation\". The plugin provides separate Python functions for each of the operations that it wants to customize. Right now, we are concerned with discovery. There are two customizable operations related to automatic discovery, one for repositories and one for source configs. In both cases, the job of the Python method is to automatically collect whatever information the schemas (see above) require, and to return that information to the Delphix Engine. The Delphix Engine will run these customized operations whenever a new environment is added, or when an existing environment is rediscovered.","title":"About Python Code"},{"location":"Building_Your_First_Plugin/Discovery/#a-look-at-the-generated-code","text":"Recall that the dvp init command we ran created a file called src/plugin_runner.py . Open this file in your editor/IDE. You will see that this file already contains a bunch of Python code. Let's take a look at the first three blocks of code in this file. from dlpx.virtualization.platform import Mount , MountSpecification , Plugin from generated.definitions import ( RepositoryDefinition , SourceConfigDefinition , SnapshotDefinition , ) These import lines make certain functionality available to our Python code. Some of this functionality will be used just below, as we implement discovery. Others will be used later on, as we implement ingestion and provisioning. Later, you'll add more import s to unlock more functionality. plugin = Plugin () This line creates a Python object which allows us to define our plugin types. We have the ability to do this because of the import Plugin statement above. This object is stored in a variable we have elected to call plugin . We are free to call this variable anything we want, so long as we also change the entryPoint line in the plugin_config.yml file. For this example, we will just leave it as plugin . # # Below is an example of the repository discovery operation. # # NOTE: The decorators are defined on the 'plugin' object created above. # # Mark the function below as the operation that does repository discovery. @plugin . discovery . repository () def repository_discovery ( source_connection ): # # This is an object generated from the repositoryDefinition schema. # In order to use it locally you must run the 'build -g' command provided # by the SDK tools from the plugin's root directory. # return [ RepositoryDefinition ( name = '1e87dc30-3cdb-4f0a-9634-07ce017d20d1' )] This is our first plugin operation . In this case, it's defining what will happen when the Delphix Engine wants to discover repositories on an environment. Let's take a look at this code line-by-line @plugin . discovery . repository () def repository_discovery ( source_connection ): This begins the definition of a function called repository_discovery . We are using a Python decorator which signals to the Delphix Engine that this is the function which should be called when it is time to do repository discovery. The actual name of the function doesn't matter here. Note that we are using our plugin variable here as part of the decorator. The Delphix Engine will pass us information about the source environment in an argument called source_connection . Warning The name of this input argument matters. That is, you'll always need to have an argument called source_connection here. Each plugin operation has its own set of required argument names. For details on which arguments apply to which operations, see the reference section . return [ RepositoryDefinition ( name = '1e87dc30-3cdb-4f0a-9634-07ce017d20d1' )] This creates and returns a Python object that corresponds to the format defined by our repository schema. Because out repository has exactly one string property called name , therefore this Python object has one property called name . Notice that the code generator has filled in the value of name with a random string. This results in a plugin operation that works, but which will not be very helpful for the user. We'll change this later. The rest of the file contains more plugin operations, and we'll be modifying them later.","title":"A Look at the Generated Code"},{"location":"Building_Your_First_Plugin/Discovery/#repository-discovery","text":"Now, we need to modify the provided repository discovery operation. This operation will examine a remote environment, find any repositories, and return information about them to the Delphix Engine. As a reminder, our only external dependency on the remote environment is simply the existence of a filesystem. Since every Unix host has a filesystem, that means we will have exactly one repository per remote environment. Therefore, our repository discovery operation can be very simple. In fact, as we saw above, the default-generated repository_discovery function does almost exactly what we want -- it returns one single repository for any Unix host that it is asked to work with. The only problem with it is that it uses unhelpful name. That's really easy to change! Replace or modify repository_discovery so it looks like this: @plugin . discovery . repository () def repository_discovery ( source_connection ): repository = RepositoryDefinition ( 'Repository for our First Plugin' ) return [ repository ] Tip Be careful to always use consistent indentation in Python code!","title":"Repository Discovery"},{"location":"Building_Your_First_Plugin/Discovery/#source-config-discovery","text":"For source configs, we will rely solely on manual discovery. Therefore, the user will tell us which directories they want to ingest from. We still have to define a source config discovery operation -- it just won't need to do much. The job of this operation is to return only source configs associated with the given repository . This function will be called once per repository. In our case, that means it will only be called once. Because we want to supply no automatically-discovered source configs, this function should simply returns an empty list. In fact, dvp init has already generated a function for us that does exactly this. @plugin . discovery . source_config () def source_config_discovery ( source_connection , repository ): return [] If we wanted to do automatic discovery of source configs, we'd modify this function. But, for our purposes now, the existing code is fine and we don't need to change anything.","title":"Source Config Discovery"},{"location":"Building_Your_First_Plugin/Discovery/#how-to-run-discovery-in-the-delphix-engine","text":"Let us make sure discovery works! Run the dvp build commands, as before. This will build the plugin, with all of the new changes, and create an artifact. Run dvp upload -e <engine> -u <user> , as before. This will get all the new changes onto the Delphix Engine. Once the new plugin is uploaded, add a remote unix environment to your engine. To do this, go to Manage > Environments , chose Add Environment from the menu, answer the questions, and Submit . (If you already have an environment set up, you can just refresh it instead). To keep an eye on this discovery process, you may need to open the Actions tab on the UI. If any errors happen, they will be reported here. After the automatic discovery process completes, go to the Databases tab. You will see an entry for Repository For Our First Plugin . This is the repository you created in your Python code. Notice that it says No databases found on installation . This is because we chose not to do automatic source config discovery. However, because we have allowed manual source config discovery, you can add your own entries by clicking the plus sign ( Add Database ). Complete the information in the Add Database dialog and click Add. This should all look familiar. It is precisely what we defined in our source config schema. As expected, there are two entries, one for our name property, and one for path . For example, in the above screenshot, we are specifying that we want to sync the /bin directory from the remote host, and we want to call it Binaries . You can pick any directory and name that you want. Once you have added one or more source configs, you will be able to sync. This is covered on the next page. Warning Once you have automatically or manually created source configs, you will not be allowed to modify your plugin's source config schema. We will cover how to deal with this later in the upgrade section . For now, if you need to change your plugin's source config schema: You will have to delete any source configs you have manually added. Delete the plugin and its corresponding objects (dSources, Virtual Sources, etc) if the source configs were manually discovered.","title":"How to Run Discovery in the Delphix Engine"},{"location":"Building_Your_First_Plugin/Initial_Setup/","text":"Initial Setup \u00b6 Before we begin to start writing plugin code, we will need to do some setup work. We will be using the dvp tool, which is described in the Getting Started section. The quoted examples in this section assume you're working on a Unix-like system. Sanity check \u00b6 First a reminder that it's highly recommended that you develop your plugin in a virtual environment . Next, make sure you have a Delphix Engine ready to use, as described in the Prerequisites section on the previous page. Finally, let's quickly make sure that dvp is working! Type dvp -h and you should see something like the following: (venv)$ dvp -h Usage: dvp [OPTIONS] COMMAND [ARGS]... The tools of the Delphix Virtualization SDK that help develop, build, and upload a plugin. Options: --version Show the version and exit. -v, --verbose Enable verbose mode. Can be repeated up to three times for increased verbosity. -q, --quiet Enable quiet mode. Can be repeated up to three times for increased suppression. -h, --help Show this message and exit. Commands: build Build the plugin code and generate upload artifact file... download-logs Download plugin logs from a target Delphix Engine to a... init Create a plugin in the root directory. upload Upload the generated upload artifact (the plugin JSON file)... If this looks good, you are ready to begin! If, instead, you see something like the following, go back to Getting Started and make sure you setup everything correctly before continuing. (venv)$ dvp -bash: dvp: command not found Creating a Bare Plugin \u00b6 To start, we will create a new directory where our new plugin code will live. (venv)$ mkdir first_plugin (venv)$ cd first_plugin Now that we are in our new plugin directory, we can use the dvp tool to create a plugin for us. This plugin will be a mere skeleton -- it will not do anything useful until we modify it in the subsequent pages. (venv) first_plugin$ dvp init -n first_plugin -s STAGED -p WINDOWS The -n argument here means \"plugin name.\" We are using the name first_plugin . The -s argument tells which syncing strategy we want to use. The -p argument tells which host platform our plugin supports. You can type dvp init -h for more information about the options available. After running this command, you should see that files have been created for you: (venv) first_plugin$ ls plugin_config.yml schema.json src These files are described below: File Description plugin_config.yml The plugin config file, which provides a list of plugin properties schema.json Contains schemas which provide custom datatype definitions src/plugin_runner.py A Python file which will eventually contain code that handles plugin operations Open these files in your editor/IDE and take a look at them. At this point they will not have a lot of content, but we will add to them as we go through the next few pages. Building The New Plugin \u00b6 The new files we created above have to get built to produce a single artifact . This is done with the dvp tool. (venv) first_plugin$ dvp build After the build, you should see that the build process has created a new file called artifact.json . (venv) first_plugin$ ls artifact.json plugin_config.yml schema.json src Uploading The New Plugin \u00b6 Now using the dvp tool we can upload the artifact onto our Delphix Engine. (venv) first_plugin$ dvp upload -e engine.company.com -u admin The -e argument specifies the engine on which to install the plugin, and the -u argument gives the Delphix Engine user. You will be prompted for a password. Once the upload is finished, you can verify the installation from the Manage > Toolkits screen in the Delphix Engine UI.","title":"Initial Setup"},{"location":"Building_Your_First_Plugin/Initial_Setup/#initial-setup","text":"Before we begin to start writing plugin code, we will need to do some setup work. We will be using the dvp tool, which is described in the Getting Started section. The quoted examples in this section assume you're working on a Unix-like system.","title":"Initial Setup"},{"location":"Building_Your_First_Plugin/Initial_Setup/#sanity-check","text":"First a reminder that it's highly recommended that you develop your plugin in a virtual environment . Next, make sure you have a Delphix Engine ready to use, as described in the Prerequisites section on the previous page. Finally, let's quickly make sure that dvp is working! Type dvp -h and you should see something like the following: (venv)$ dvp -h Usage: dvp [OPTIONS] COMMAND [ARGS]... The tools of the Delphix Virtualization SDK that help develop, build, and upload a plugin. Options: --version Show the version and exit. -v, --verbose Enable verbose mode. Can be repeated up to three times for increased verbosity. -q, --quiet Enable quiet mode. Can be repeated up to three times for increased suppression. -h, --help Show this message and exit. Commands: build Build the plugin code and generate upload artifact file... download-logs Download plugin logs from a target Delphix Engine to a... init Create a plugin in the root directory. upload Upload the generated upload artifact (the plugin JSON file)... If this looks good, you are ready to begin! If, instead, you see something like the following, go back to Getting Started and make sure you setup everything correctly before continuing. (venv)$ dvp -bash: dvp: command not found","title":"Sanity check"},{"location":"Building_Your_First_Plugin/Initial_Setup/#creating-a-bare-plugin","text":"To start, we will create a new directory where our new plugin code will live. (venv)$ mkdir first_plugin (venv)$ cd first_plugin Now that we are in our new plugin directory, we can use the dvp tool to create a plugin for us. This plugin will be a mere skeleton -- it will not do anything useful until we modify it in the subsequent pages. (venv) first_plugin$ dvp init -n first_plugin -s STAGED -p WINDOWS The -n argument here means \"plugin name.\" We are using the name first_plugin . The -s argument tells which syncing strategy we want to use. The -p argument tells which host platform our plugin supports. You can type dvp init -h for more information about the options available. After running this command, you should see that files have been created for you: (venv) first_plugin$ ls plugin_config.yml schema.json src These files are described below: File Description plugin_config.yml The plugin config file, which provides a list of plugin properties schema.json Contains schemas which provide custom datatype definitions src/plugin_runner.py A Python file which will eventually contain code that handles plugin operations Open these files in your editor/IDE and take a look at them. At this point they will not have a lot of content, but we will add to them as we go through the next few pages.","title":"Creating a Bare Plugin"},{"location":"Building_Your_First_Plugin/Initial_Setup/#building-the-new-plugin","text":"The new files we created above have to get built to produce a single artifact . This is done with the dvp tool. (venv) first_plugin$ dvp build After the build, you should see that the build process has created a new file called artifact.json . (venv) first_plugin$ ls artifact.json plugin_config.yml schema.json src","title":"Building The New Plugin"},{"location":"Building_Your_First_Plugin/Initial_Setup/#uploading-the-new-plugin","text":"Now using the dvp tool we can upload the artifact onto our Delphix Engine. (venv) first_plugin$ dvp upload -e engine.company.com -u admin The -e argument specifies the engine on which to install the plugin, and the -u argument gives the Delphix Engine user. You will be prompted for a password. Once the upload is finished, you can verify the installation from the Manage > Toolkits screen in the Delphix Engine UI.","title":"Uploading The New Plugin"},{"location":"Building_Your_First_Plugin/Overview/","text":"Overview \u00b6 In the following few pages, we will walk through an example of making a simple, working plugin. Our plugin will virtualize simple directory trees on Unix systems. The actual contents of these directories could be anything: configuration files, documents, image libraries, etc. Our plugin will not care about the contents and will treat it as a directory tree full of files. Data Flow in the Delphix Engine \u00b6 Here we will briefly overview how data moves through the Delphix Engine. Ingestion \u00b6 It all begins with Delphix ingesting data\u2014copying some data from what we call a source environment onto the Delphix Engine. Plugins can use either of two basic strategies to do this copying: direct linking , where the Delphix Engine pulls data directly from the source environment. staged linking , where the plugin is responsible for pulling data from the source environment. Our plugin will use the staged linking strategy. With staged linking, Delphix exposes and mounts storage to a staging environment . This would be an NFS share for Unix environments and iSCSI disks for Windows environments. You can use either the source environment or a different environment for staging. We will write our plugin to handle both approaches. Once Delphix mounts the storage share onto the staging environment, the plugin needs to arrange for the relevant data to be copied from the source environment onto the storage share, which is backed by Delphix Engine storage. When this initial copy is complete, Delphix will take a snapshot of the backing storage. This same basic operation will be repeated when Delphix mounts an NFS share: The plugin copies data onto it, then Delphix snapshots the result. Provisioning \u00b6 Provisioning is when you take a Delphix Engine snapshot and create a virtual dataset from it. First the snapshot is cloned onto the Delphix Engine, then this newly-cloned data is mounted as a virtual dataset onto a target environment . While this new virtual dataset gets updated by its end users, the original snapshot is persistent. You can use it in a few ways: Provision other virtual datasets from it Rewind the virtual dataset back to the state it represents Create a physical database from it in what we call V2P: Virtual to Physical Parts of a Plugin \u00b6 A plugin consists of three main parts. We will cover them briefly here, and then fill in more details later in the tutorial. Plugin Config \u00b6 Plugin config is where the plugin describes itself to the Delphix Engine. What is the plugin called? What version of the plugin is being used? What type(s) of environments does the plugin work with? What features does the plugin offer?... Plugin Operations \u00b6 The plugin will need to provide operations. These are Python functions, each of which implements one small piece of functionality. This is how the plugin customizes Delphix behavior to work with the kind of dataset you\u2019re building the plugin for. One operation will handle setting up a newly-configured virtual dataset. Another will handle copying data from a source environment, and so on. Later we\u2019ll provide examples for our first plugin. See Plugin Operations for full details on the operations that are available, which are required, and what each one is required to do. Schemas \u00b6 As part of normal operations, plugins need to generate and access certain pieces of information in order to do their job. For example, plugins that work with Postgres might need to know which port number to connect to, or which credentials to use. Defining your plugin\u2019s schemas will enable it to give the Delphix Engine the details it needs to run the operations we\u2019ve built into it. Different datasets can have very different needs. The schemas you provide for your plugin will tell Delphix how to operate with your dataset. Prerequisites \u00b6 To complete the tutorial that follows, make sure you check off the things on this list: Download the SDK and get it working A running Delphix Engine version 6.0.2.0 or above. Add at least one Unix host\u2014but preferably three\u2014to the Delphix Engine as remote environments. Have a tool at hand for editing text files\u2014mostly Python and JSON. A simple text editor would work fine, or you can use a full-fledged IDE.","title":"Overview"},{"location":"Building_Your_First_Plugin/Overview/#overview","text":"In the following few pages, we will walk through an example of making a simple, working plugin. Our plugin will virtualize simple directory trees on Unix systems. The actual contents of these directories could be anything: configuration files, documents, image libraries, etc. Our plugin will not care about the contents and will treat it as a directory tree full of files.","title":"Overview"},{"location":"Building_Your_First_Plugin/Overview/#data-flow-in-the-delphix-engine","text":"Here we will briefly overview how data moves through the Delphix Engine.","title":"Data Flow in the Delphix Engine"},{"location":"Building_Your_First_Plugin/Overview/#ingestion","text":"It all begins with Delphix ingesting data\u2014copying some data from what we call a source environment onto the Delphix Engine. Plugins can use either of two basic strategies to do this copying: direct linking , where the Delphix Engine pulls data directly from the source environment. staged linking , where the plugin is responsible for pulling data from the source environment. Our plugin will use the staged linking strategy. With staged linking, Delphix exposes and mounts storage to a staging environment . This would be an NFS share for Unix environments and iSCSI disks for Windows environments. You can use either the source environment or a different environment for staging. We will write our plugin to handle both approaches. Once Delphix mounts the storage share onto the staging environment, the plugin needs to arrange for the relevant data to be copied from the source environment onto the storage share, which is backed by Delphix Engine storage. When this initial copy is complete, Delphix will take a snapshot of the backing storage. This same basic operation will be repeated when Delphix mounts an NFS share: The plugin copies data onto it, then Delphix snapshots the result.","title":"Ingestion"},{"location":"Building_Your_First_Plugin/Overview/#provisioning","text":"Provisioning is when you take a Delphix Engine snapshot and create a virtual dataset from it. First the snapshot is cloned onto the Delphix Engine, then this newly-cloned data is mounted as a virtual dataset onto a target environment . While this new virtual dataset gets updated by its end users, the original snapshot is persistent. You can use it in a few ways: Provision other virtual datasets from it Rewind the virtual dataset back to the state it represents Create a physical database from it in what we call V2P: Virtual to Physical","title":"Provisioning"},{"location":"Building_Your_First_Plugin/Overview/#parts-of-a-plugin","text":"A plugin consists of three main parts. We will cover them briefly here, and then fill in more details later in the tutorial.","title":"Parts of a Plugin"},{"location":"Building_Your_First_Plugin/Overview/#plugin-config","text":"Plugin config is where the plugin describes itself to the Delphix Engine. What is the plugin called? What version of the plugin is being used? What type(s) of environments does the plugin work with? What features does the plugin offer?...","title":"Plugin Config"},{"location":"Building_Your_First_Plugin/Overview/#plugin-operations","text":"The plugin will need to provide operations. These are Python functions, each of which implements one small piece of functionality. This is how the plugin customizes Delphix behavior to work with the kind of dataset you\u2019re building the plugin for. One operation will handle setting up a newly-configured virtual dataset. Another will handle copying data from a source environment, and so on. Later we\u2019ll provide examples for our first plugin. See Plugin Operations for full details on the operations that are available, which are required, and what each one is required to do.","title":"Plugin Operations"},{"location":"Building_Your_First_Plugin/Overview/#schemas","text":"As part of normal operations, plugins need to generate and access certain pieces of information in order to do their job. For example, plugins that work with Postgres might need to know which port number to connect to, or which credentials to use. Defining your plugin\u2019s schemas will enable it to give the Delphix Engine the details it needs to run the operations we\u2019ve built into it. Different datasets can have very different needs. The schemas you provide for your plugin will tell Delphix how to operate with your dataset.","title":"Schemas"},{"location":"Building_Your_First_Plugin/Overview/#prerequisites","text":"To complete the tutorial that follows, make sure you check off the things on this list: Download the SDK and get it working A running Delphix Engine version 6.0.2.0 or above. Add at least one Unix host\u2014but preferably three\u2014to the Delphix Engine as remote environments. Have a tool at hand for editing text files\u2014mostly Python and JSON. A simple text editor would work fine, or you can use a full-fledged IDE.","title":"Prerequisites"},{"location":"Building_Your_First_Plugin/Provisioning/","text":"Provisioning \u00b6 What is Provisioning? \u00b6 Once Delphix has a snapshot of a dataset (for example of a dSource), it is possible to quickly clone that snapshot to create a new virtual dataset . This new virtual dataset will be made available for use on a target environment . This process is called provisioning . Our Provisioning Strategy \u00b6 For many plugins, there is a lot of work that needs to be done before a newly-provisioned virtual dataset can be made useful. For example, it might need to be registered with a running DBMS. Or, maybe some data inside the dataset needs to be changed so it behaves properly on the target environment. In our case, however, there is very little to do. All we really require is that the files in the virtual dataset are accessible at some path on the target environment. Since the Delphix Engine takes care of mounting the data, we only need to worry about controlling where that data is mounted. Defining our Provision-Related Data Formats \u00b6 We have already seen four custom data formats: for repositories, source configs, snapshots and linked sources. The final one is used for virtual sources . Recall that, for our plugin, a VDB is just a directory full of files. There is no special procedure needed to enable it, no DBMS to coordinate with, etc. All we need to do is make the files available on the target environment. So, the only question for the user is \"Where should these files live?\" Open up schema.json , locate the virtualSourceDefintion section, and change it to look like this: \"virtualSourceDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"mountLocation\" ], \"properties\" : { \"mountLocation\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" , \"prettyName\" : \"Mount Location on Target Host\" , \"description\" : \"Where to mount VDB onto the target host\" } } } , This should look familiar from the source config schema that we did earlier. We only have one property, and it represents the mount location on the target environment. Implementing Provisioning \u00b6 There are numerous ways for a plugin to customize the provisioning process. For our example plugin, we just need to do a few things: Tell Delphix where to mount the virtual dataset. Create a sourceConfig to represent each newly-provisioned virtual dataset. Modify an existing sourceConfig , if necessary, when the virtual dataset is refreshed or rewound. Construct snapshot-related data any time a snapshot is taken of the virtual dataset. Controlling Mounting \u00b6 As we saw previously with linked sources, we need to tell Delphix where to mount the dataset. Open up plugin_runner.py and find the plugin.virtual.mount_specification decorator. Change that function so that it looks like this: @plugin . virtual . mount_specification () def vdb_mount_spec ( virtual_source , repository ): mount_location = virtual_source . parameters . mount_location mount = Mount ( virtual_source . connection . environment , mount_location ) return MountSpecification ([ mount ]) As we did with linked sources, we just look up what the user told us, and then package that up and return it to Delphix. Creating a Source Config for a new VDB \u00b6 Just like we saw earlier with linked datasets , each virtual dataset will need its own source config so that the Delphix Engine can interact with it. Our plugin is in charge of creating that source config at provision time As a reminder, here is what our schema looks like for source configs: \"sourceConfigDefinition\" : { \"type\" : \"object\" , \"required\" : [ \"name\" , \"path\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" , \"prettyName\" : \"Dataset Name\" , \"description\" : \"User-visible name for this dataset\" }, \"path\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" , \"prettyName\" : \"Path\" , \"description\" : \"Full path to data location on the remote environment\" } }, \"nameField\" : \"name\" , \"identityFields\" : [ \"path\" ] } , Thus, for each newly-cloned virtual dataset, we create a new source config object with a name and a path. This is done by the configure plugin operation. In addition to generating a new source config, the configure operation is also tasked with getting the newly-cloned dataset ready for use on the target environment. What this means exactly will vary from plugin to plugin. For our simple plugin, the dataset does not require any setup work, and so we only have to worry about the source config. Find the plugin.virtual.configure decorator and change the function to look like this: @plugin . virtual . configure () def configure_new_vdb ( virtual_source , snapshot , repository ): mount_location = virtual_source . parameters . mount_location name = \"VDB mounted at {} \" . format ( mount_location ) return SourceConfigDefinition ( path = mount_location , name = name ) Modifying a Source Config after Rewind or Refresh \u00b6 Just as a new VDB might need to be configured, a refreshed or rewound VDB might need to be \"reconfigured\" to handle the new post-refresh (or post-rewind) state of the VDB. So, just as there is a configure operation, there is also a reconfigure operation. The main difference between the two is that configure must create a source config, but reconfigure needs to modify a pre-existing source config. In our simple plugin, there is no special work to do at reconfigure time, and there is no reason to modify anything about the source config. We just need to write a reconfigure operation that returns the existing source config without making any changes. Find the plugin.virtual.reconfigure decorator and modify the function as follows. @plugin . virtual . reconfigure () def reconfigure_existing_vdb ( virtual_source , repository , source_config , snapshot ): return source_config Saving Snapshot Data \u00b6 As with our linked sources, we don't actually have anything we need to save when VDB snapshots are taken. And, again, dvp init has created a post-snapshot operation that will work just fine for us without modification: @plugin . virtual . post_snapshot () def virtual_post_snapshot ( virtual_source , repository , source_config ): return SnapshotDefinition () How To Provision in the Delphix Engine \u00b6 Finally, let us try it out to make sure provisioning works! Again, use dvp build and dvp upload to get your new changes onto your Delphix Engine. Click Manage > Datasets . Select the dSource you created in the last page. You should see at least one snapshot, and maybe more than one if you have manually taken a snapshot, or if you have a snapshot policy in place. Select one of these snapshots and click the Provision vFiles icon. This will open the Provision VDB wizard. Complete the steps and select Submit . During VDB provisioning one of the things you will have to do is to provide the data required by your virtual source schema. In our case, that means you will be asked to provide a value for mountLocation . You will also be asked to choose a target environment on which the new VDB will live. After the wizard finishes, you will see a job appear in the Actions tab on the right-hand side of the screen. When that job completes, your new VDB should be ready. To ensure everything has worked correctly, log into to your target environment. From there, you can examine the directory you specified as the mountLocation . What you should see is a copy of the directory that you linked to with your dSource.","title":"Provisioning"},{"location":"Building_Your_First_Plugin/Provisioning/#provisioning","text":"","title":"Provisioning"},{"location":"Building_Your_First_Plugin/Provisioning/#what-is-provisioning","text":"Once Delphix has a snapshot of a dataset (for example of a dSource), it is possible to quickly clone that snapshot to create a new virtual dataset . This new virtual dataset will be made available for use on a target environment . This process is called provisioning .","title":"What is Provisioning?"},{"location":"Building_Your_First_Plugin/Provisioning/#our-provisioning-strategy","text":"For many plugins, there is a lot of work that needs to be done before a newly-provisioned virtual dataset can be made useful. For example, it might need to be registered with a running DBMS. Or, maybe some data inside the dataset needs to be changed so it behaves properly on the target environment. In our case, however, there is very little to do. All we really require is that the files in the virtual dataset are accessible at some path on the target environment. Since the Delphix Engine takes care of mounting the data, we only need to worry about controlling where that data is mounted.","title":"Our Provisioning Strategy"},{"location":"Building_Your_First_Plugin/Provisioning/#defining-our-provision-related-data-formats","text":"We have already seen four custom data formats: for repositories, source configs, snapshots and linked sources. The final one is used for virtual sources . Recall that, for our plugin, a VDB is just a directory full of files. There is no special procedure needed to enable it, no DBMS to coordinate with, etc. All we need to do is make the files available on the target environment. So, the only question for the user is \"Where should these files live?\" Open up schema.json , locate the virtualSourceDefintion section, and change it to look like this: \"virtualSourceDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"mountLocation\" ], \"properties\" : { \"mountLocation\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" , \"prettyName\" : \"Mount Location on Target Host\" , \"description\" : \"Where to mount VDB onto the target host\" } } } , This should look familiar from the source config schema that we did earlier. We only have one property, and it represents the mount location on the target environment.","title":"Defining our Provision-Related Data Formats"},{"location":"Building_Your_First_Plugin/Provisioning/#implementing-provisioning","text":"There are numerous ways for a plugin to customize the provisioning process. For our example plugin, we just need to do a few things: Tell Delphix where to mount the virtual dataset. Create a sourceConfig to represent each newly-provisioned virtual dataset. Modify an existing sourceConfig , if necessary, when the virtual dataset is refreshed or rewound. Construct snapshot-related data any time a snapshot is taken of the virtual dataset.","title":"Implementing Provisioning"},{"location":"Building_Your_First_Plugin/Provisioning/#controlling-mounting","text":"As we saw previously with linked sources, we need to tell Delphix where to mount the dataset. Open up plugin_runner.py and find the plugin.virtual.mount_specification decorator. Change that function so that it looks like this: @plugin . virtual . mount_specification () def vdb_mount_spec ( virtual_source , repository ): mount_location = virtual_source . parameters . mount_location mount = Mount ( virtual_source . connection . environment , mount_location ) return MountSpecification ([ mount ]) As we did with linked sources, we just look up what the user told us, and then package that up and return it to Delphix.","title":"Controlling Mounting"},{"location":"Building_Your_First_Plugin/Provisioning/#creating-a-source-config-for-a-new-vdb","text":"Just like we saw earlier with linked datasets , each virtual dataset will need its own source config so that the Delphix Engine can interact with it. Our plugin is in charge of creating that source config at provision time As a reminder, here is what our schema looks like for source configs: \"sourceConfigDefinition\" : { \"type\" : \"object\" , \"required\" : [ \"name\" , \"path\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" , \"prettyName\" : \"Dataset Name\" , \"description\" : \"User-visible name for this dataset\" }, \"path\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" , \"prettyName\" : \"Path\" , \"description\" : \"Full path to data location on the remote environment\" } }, \"nameField\" : \"name\" , \"identityFields\" : [ \"path\" ] } , Thus, for each newly-cloned virtual dataset, we create a new source config object with a name and a path. This is done by the configure plugin operation. In addition to generating a new source config, the configure operation is also tasked with getting the newly-cloned dataset ready for use on the target environment. What this means exactly will vary from plugin to plugin. For our simple plugin, the dataset does not require any setup work, and so we only have to worry about the source config. Find the plugin.virtual.configure decorator and change the function to look like this: @plugin . virtual . configure () def configure_new_vdb ( virtual_source , snapshot , repository ): mount_location = virtual_source . parameters . mount_location name = \"VDB mounted at {} \" . format ( mount_location ) return SourceConfigDefinition ( path = mount_location , name = name )","title":"Creating a Source Config for a new VDB"},{"location":"Building_Your_First_Plugin/Provisioning/#modifying-a-source-config-after-rewind-or-refresh","text":"Just as a new VDB might need to be configured, a refreshed or rewound VDB might need to be \"reconfigured\" to handle the new post-refresh (or post-rewind) state of the VDB. So, just as there is a configure operation, there is also a reconfigure operation. The main difference between the two is that configure must create a source config, but reconfigure needs to modify a pre-existing source config. In our simple plugin, there is no special work to do at reconfigure time, and there is no reason to modify anything about the source config. We just need to write a reconfigure operation that returns the existing source config without making any changes. Find the plugin.virtual.reconfigure decorator and modify the function as follows. @plugin . virtual . reconfigure () def reconfigure_existing_vdb ( virtual_source , repository , source_config , snapshot ): return source_config","title":"Modifying a Source Config after Rewind or Refresh"},{"location":"Building_Your_First_Plugin/Provisioning/#saving-snapshot-data","text":"As with our linked sources, we don't actually have anything we need to save when VDB snapshots are taken. And, again, dvp init has created a post-snapshot operation that will work just fine for us without modification: @plugin . virtual . post_snapshot () def virtual_post_snapshot ( virtual_source , repository , source_config ): return SnapshotDefinition ()","title":"Saving Snapshot Data"},{"location":"Building_Your_First_Plugin/Provisioning/#how-to-provision-in-the-delphix-engine","text":"Finally, let us try it out to make sure provisioning works! Again, use dvp build and dvp upload to get your new changes onto your Delphix Engine. Click Manage > Datasets . Select the dSource you created in the last page. You should see at least one snapshot, and maybe more than one if you have manually taken a snapshot, or if you have a snapshot policy in place. Select one of these snapshots and click the Provision vFiles icon. This will open the Provision VDB wizard. Complete the steps and select Submit . During VDB provisioning one of the things you will have to do is to provide the data required by your virtual source schema. In our case, that means you will be asked to provide a value for mountLocation . You will also be asked to choose a target environment on which the new VDB will live. After the wizard finishes, you will see a job appear in the Actions tab on the right-hand side of the screen. When that job completes, your new VDB should be ready. To ensure everything has worked correctly, log into to your target environment. From there, you can examine the directory you specified as the mountLocation . What you should see is a copy of the directory that you linked to with your dSource.","title":"How To Provision in the Delphix Engine"},{"location":"References/CLI/","text":"CLI \u00b6 The CLI is installed with the SDK. To install the SDK, refer to the Getting Started section. You can also use a CLI Configuration File to set default values for CLI command options. Help \u00b6 Every command has a -h flag including the CLI itself. This will print the help menu. Examples \u00b6 Get the CLI's help menu. $ dvp -h Usage: dvp [OPTIONS] COMMAND [ARGS]... The tools of the Delphix Virtualization SDK that help develop, build, and upload a plugin. Options: --version Show the version and exit. -v, --verbose Enable verbose mode. Can be repeated up to three times for increased verbosity. -q, --quiet Enable quiet mode. Can be repeated up to three times for increased suppression. -h, --help Show this message and exit. Commands: build Build the plugin code and generate upload artifact file... download-logs Download plugin logs from a target Delphix Engine to a... init Create a plugin in the root directory. upload Upload the generated upload artifact (the plugin JSON file)... Get the build command's help menu. $ dvp build -h Usage: dvp build [OPTIONS] Build the plugin code and generate upload artifact file using the configuration provided in the plugin config file. Options: -c, --plugin-config FILE Set the path to plugin config file.This file contains the configuration required to build the plugin. [default: plugin_config.yml] -a, --upload-artifact FILE Set the upload artifact.The upload artifact file generated by build process will be writtento this file and later used by upload command. [default: artifact.json] -g, --generate-only Only generate the Python classes from the schema definitions. Do not do a full build or create an upload artifact. [default: False] -h, --help Show this message and exit. Verbosity \u00b6 To change the verbosity level of the CLI you can specify up to three -v (to increase) or -q (to decrease) the amount that is printed to the console. This is an option on the CLI itself and can be used with any command. Option Output -qqq None -qq Critical -q Error -v Info -vv Debug -vvv All Examples \u00b6 Print everything to the console. $ dvp -vvv build Print nothing to the console. $ dvp -qqq build Commands \u00b6 init \u00b6 Description \u00b6 Create a plugin in the root directory. The plugin will be valid but have no functionality. Options \u00b6 Option Description Required Default -r, --root-dir DIRECTORY Set the plugin root directory. N os.cwd() -n, --plugin-name TEXT Set the name of the plugin that will be used to identify it. N id -s, --ingestion-strategy [DIRECT|STAGED] Set the ingestion strategy of the plugin. A \"direct\" plugin ingests without a staging server while a \"staged\" plugin requires a staging server. N DIRECT -t, --host-type [UNIX|WINDOWS] Set the host platform supported by the plugin. N UNIX Examples \u00b6 Create a UNIX plugin in the current working directory with the DIRECT ingestion strategy. Here the name of the plugin will be equal to the id that is generated. $ dvp init Create a UNIX plugin in the current working directory with the DIRECT ingestion strategy and use postgres as the display name. $ dvp init -n postgres Create a UNIX plugin called mongodb in a custom location with the STAGED ingestion strategy. $ dvp init -n mongodb -s STAGED -r /our/plugin/directory Create a WINDOWS plugin called mssql in the current working directory with the DIRECT ingestion strategy. $ dvp init -n mssql -t WINDOWS build \u00b6 Description \u00b6 Build the plugin code and generate upload artifact file using the configuration provided in the plugin config file. Options \u00b6 Option Description Required Default -c, --plugin-config FILE Set the path to plugin config file.This file contains the configuration required to build the plugin. N plugin_config.yml -a, --upload-artifact FILE Set the upload artifact.The upload artifact file generated by build process will be written to this file and later used by upload command. N artifact.json -g, --generate-only Only generate the Python classes from the schema definitions. Do not do a full build or create an upload artifact. N False Examples \u00b6 Do a full build of the plugin and write the upload artifact to ./artifact.json . This assumes current working directory contains a plugin config file named plugin_config.yml . $ dvp build Do a partial build and just generate the Python classes from the schema definitions. This assumes current working directory contains ad plugin config file named plugin_config.yml . $ dvp build -g Do a full build of a plugin and write the artifact file to a custom location. $ dvp build -c config.yml -a build/artifact.json upload \u00b6 Description \u00b6 Upload the generated upload artifact (the plugin JSON file) that was built to a target Delphix Engine. Note that the upload artifact should be the file created after running the build command and will fail if it's not readable or valid. Options \u00b6 Option Description Required Default -e, --delphix-engine TEXT Upload plugin to the provided engine. This should be either the hostname or IP address. Y None -u, --user TEXT Authenticate to the Delphix Engine with the provided user. Y None -a, --upload-artifact FILE Path to the upload artifact that was generated through build. N artifact.json --wait Block and wait for the upload job to finish on the Delphix Engine. N None --password TEXT Authenticate using the provided password. If ommitted, the password will be requested through a secure prompt. N None Examples \u00b6 Upload artifact build/artifact.json to engine.example.com using the user admin . Since the password option is ommitted, a secure password prompt is used instead. $ dvp upload -a build/artifact -e engine.example.com -u admin Password: download-logs \u00b6 Description \u00b6 Download plugin logs from a Delphix Engine to a local directory. Options \u00b6 Option Description Required Default -e, --delphix-engine TEXT Download plugin logs from the provided Delphix engine. This should be either the hostname or IP address. Y None -c, --plugin-config FILE Set the path to plugin config file. This file contains the plugin name to download logs for. N plugin_config.yml -u, --user TEXT Authenticate to the Delphix Engine with the provided user. Y None -d, --directory DIRECTORY Specify the directory of where to download the plugin logs. N os.cwd() --password TEXT Authenticate using the provided password. If ommitted, the password will be requested through a secure prompt. N None Examples \u00b6 Download plugin logs from engine.example.com using the user admin . Since the password option is ommitted, a secure password prompt is used instead. $ dvp download-logs -e engine.example.com -u admin Password:","title":"CLI"},{"location":"References/CLI/#cli","text":"The CLI is installed with the SDK. To install the SDK, refer to the Getting Started section. You can also use a CLI Configuration File to set default values for CLI command options.","title":"CLI"},{"location":"References/CLI/#help","text":"Every command has a -h flag including the CLI itself. This will print the help menu.","title":"Help"},{"location":"References/CLI/#examples","text":"Get the CLI's help menu. $ dvp -h Usage: dvp [OPTIONS] COMMAND [ARGS]... The tools of the Delphix Virtualization SDK that help develop, build, and upload a plugin. Options: --version Show the version and exit. -v, --verbose Enable verbose mode. Can be repeated up to three times for increased verbosity. -q, --quiet Enable quiet mode. Can be repeated up to three times for increased suppression. -h, --help Show this message and exit. Commands: build Build the plugin code and generate upload artifact file... download-logs Download plugin logs from a target Delphix Engine to a... init Create a plugin in the root directory. upload Upload the generated upload artifact (the plugin JSON file)... Get the build command's help menu. $ dvp build -h Usage: dvp build [OPTIONS] Build the plugin code and generate upload artifact file using the configuration provided in the plugin config file. Options: -c, --plugin-config FILE Set the path to plugin config file.This file contains the configuration required to build the plugin. [default: plugin_config.yml] -a, --upload-artifact FILE Set the upload artifact.The upload artifact file generated by build process will be writtento this file and later used by upload command. [default: artifact.json] -g, --generate-only Only generate the Python classes from the schema definitions. Do not do a full build or create an upload artifact. [default: False] -h, --help Show this message and exit.","title":"Examples"},{"location":"References/CLI/#verbosity","text":"To change the verbosity level of the CLI you can specify up to three -v (to increase) or -q (to decrease) the amount that is printed to the console. This is an option on the CLI itself and can be used with any command. Option Output -qqq None -qq Critical -q Error -v Info -vv Debug -vvv All","title":"Verbosity"},{"location":"References/CLI/#examples_1","text":"Print everything to the console. $ dvp -vvv build Print nothing to the console. $ dvp -qqq build","title":"Examples"},{"location":"References/CLI/#commands","text":"","title":"Commands"},{"location":"References/CLI/#init","text":"","title":"init"},{"location":"References/CLI/#description","text":"Create a plugin in the root directory. The plugin will be valid but have no functionality.","title":"Description"},{"location":"References/CLI/#options","text":"Option Description Required Default -r, --root-dir DIRECTORY Set the plugin root directory. N os.cwd() -n, --plugin-name TEXT Set the name of the plugin that will be used to identify it. N id -s, --ingestion-strategy [DIRECT|STAGED] Set the ingestion strategy of the plugin. A \"direct\" plugin ingests without a staging server while a \"staged\" plugin requires a staging server. N DIRECT -t, --host-type [UNIX|WINDOWS] Set the host platform supported by the plugin. N UNIX","title":"Options"},{"location":"References/CLI/#examples_2","text":"Create a UNIX plugin in the current working directory with the DIRECT ingestion strategy. Here the name of the plugin will be equal to the id that is generated. $ dvp init Create a UNIX plugin in the current working directory with the DIRECT ingestion strategy and use postgres as the display name. $ dvp init -n postgres Create a UNIX plugin called mongodb in a custom location with the STAGED ingestion strategy. $ dvp init -n mongodb -s STAGED -r /our/plugin/directory Create a WINDOWS plugin called mssql in the current working directory with the DIRECT ingestion strategy. $ dvp init -n mssql -t WINDOWS","title":"Examples"},{"location":"References/CLI/#build","text":"","title":"build"},{"location":"References/CLI/#description_1","text":"Build the plugin code and generate upload artifact file using the configuration provided in the plugin config file.","title":"Description"},{"location":"References/CLI/#options_1","text":"Option Description Required Default -c, --plugin-config FILE Set the path to plugin config file.This file contains the configuration required to build the plugin. N plugin_config.yml -a, --upload-artifact FILE Set the upload artifact.The upload artifact file generated by build process will be written to this file and later used by upload command. N artifact.json -g, --generate-only Only generate the Python classes from the schema definitions. Do not do a full build or create an upload artifact. N False","title":"Options"},{"location":"References/CLI/#examples_3","text":"Do a full build of the plugin and write the upload artifact to ./artifact.json . This assumes current working directory contains a plugin config file named plugin_config.yml . $ dvp build Do a partial build and just generate the Python classes from the schema definitions. This assumes current working directory contains ad plugin config file named plugin_config.yml . $ dvp build -g Do a full build of a plugin and write the artifact file to a custom location. $ dvp build -c config.yml -a build/artifact.json","title":"Examples"},{"location":"References/CLI/#upload","text":"","title":"upload"},{"location":"References/CLI/#description_2","text":"Upload the generated upload artifact (the plugin JSON file) that was built to a target Delphix Engine. Note that the upload artifact should be the file created after running the build command and will fail if it's not readable or valid.","title":"Description"},{"location":"References/CLI/#options_2","text":"Option Description Required Default -e, --delphix-engine TEXT Upload plugin to the provided engine. This should be either the hostname or IP address. Y None -u, --user TEXT Authenticate to the Delphix Engine with the provided user. Y None -a, --upload-artifact FILE Path to the upload artifact that was generated through build. N artifact.json --wait Block and wait for the upload job to finish on the Delphix Engine. N None --password TEXT Authenticate using the provided password. If ommitted, the password will be requested through a secure prompt. N None","title":"Options"},{"location":"References/CLI/#examples_4","text":"Upload artifact build/artifact.json to engine.example.com using the user admin . Since the password option is ommitted, a secure password prompt is used instead. $ dvp upload -a build/artifact -e engine.example.com -u admin Password:","title":"Examples"},{"location":"References/CLI/#download-logs","text":"","title":"download-logs"},{"location":"References/CLI/#description_3","text":"Download plugin logs from a Delphix Engine to a local directory.","title":"Description"},{"location":"References/CLI/#options_3","text":"Option Description Required Default -e, --delphix-engine TEXT Download plugin logs from the provided Delphix engine. This should be either the hostname or IP address. Y None -c, --plugin-config FILE Set the path to plugin config file. This file contains the plugin name to download logs for. N plugin_config.yml -u, --user TEXT Authenticate to the Delphix Engine with the provided user. Y None -d, --directory DIRECTORY Specify the directory of where to download the plugin logs. N os.cwd() --password TEXT Authenticate using the provided password. If ommitted, the password will be requested through a secure prompt. N None","title":"Options"},{"location":"References/CLI/#examples_5","text":"Download plugin logs from engine.example.com using the user admin . Since the password option is ommitted, a secure password prompt is used instead. $ dvp download-logs -e engine.example.com -u admin Password:","title":"Examples"},{"location":"References/Classes/","text":"Classes \u00b6 DirectSource \u00b6 Represents a Linked Source object and its properties when using a Direct Linking strategy. from dlpx.virtualization.platform import DirectSource direct_source = DirectSource ( guid , connection , parameters ) Fields \u00b6 Field Type Description guid String Unique Identifier for the source. connection RemoteConnection Connection for the source environment. parameters LinkedSourceDefinition User input as per the LinkedSource Schema . StagedSource \u00b6 Represents a Linked Source object and its properties when using a Staged Linking strategy. from dlpx.virtualization.platform import StagedSource staged_source = StagedSource ( guid , source_connection , parameters , mount , staged_connection ) Fields \u00b6 Field Type Description guid String Unique Identifier for the source. source_connection RemoteConnection Connection for the source environment. parameters LinkedSourceDefinition User input as per the LinkedSource Schema . mount Mount Mount point associated with the source. staged_connection RemoteConnection Connection for the staging environment. VirtualSource \u00b6 Represents a Virtual Source object and its properties. from dlpx.virtualization.platform import VirtualSource virtual_source = VirtualSource ( guid , connection , parameters , mounts ) Fields \u00b6 Field Type Description guid String Unique Identifier for the source. connection RemoteConnection Connection for the source environment. parameters VirtualSourceDefinition User input as per the VirtualSource Schema . mounts list[ Mount ] Mount points associated with the source. RemoteConnection \u00b6 Represents a connection to a source. from dlpx.virtualization.common import RemoteConnection connection = RemoteConnection ( environment , user ) Fields \u00b6 Field Type Description environment RemoteEnvironment Environment for the connection. user RemoteUser User for the connection. Status \u00b6 An enum used to represent the state of a linked or virtual source and whether it is functioning as expected. from dlpx.virtualization.platform import Status status = Status . ACTIVE Values \u00b6 Value Description ACTIVE Source is healthy and functioning as expected. INACTIVE Source is not functioning as expected. Mount \u00b6 Represents a mount exported and mounted to a remote host. from dlpx.virtualization.platform import Mount mount = Mount ( environment , path ) Fields \u00b6 Field Type Description remote_environment RemoteEnvironment or Reference Environment for the connection. mount_path String The path on the remote host that has the mounted data set. shared_path String Optional. The path of the subdirectory of the data set to mount to the remote host. OwnershipSpecification \u00b6 Represents how to set the ownership for a data set. This only applies to Unix Hosts. from dlpx.virtualization.platform import OwnershipSpecification ownership_specification = OwnershipSpecification ( uid , gid ) Fields \u00b6 Field Type Description uid Integer The user id to set the ownership of the data set to. gid Integer The group id to set the ownership of the data set to. MountSpecification \u00b6 Represents properties for the mount associated with an exported data set. from dlpx.virtualization.platform import MountSpecification mount_specification = MountSpecification ([ mount ], ownership_specification ) Fields \u00b6 Field Type Description mounts list[ Mount ] The list of mounts to export the data sets to. ownership_specification OwnershipSpecification Optional. Control the ownership attributes for the data set. It defaults to the environment user of the remote environment if it is not specified. SnapshotParametersDefinition \u00b6 User provided parameters for the snapshot operation. It includes a boolean property named resync that can be used to indicate to the plugin whether or not to initiate a full ingestion of the dSource. The parameters are only set during a manual snapshot. When using a sync policy, resync defaults to false . from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . linked . pre_snapshot () def linked_pre_snapshot ( staged_source , repository , source_config , snapshot_parameters ): if snapshot_parameter . resync : print ( snapshot_parameter . resync ) This class will be generated during build and is located with the autogenerated classes. As it is passed into the operation, importing it is not neccessary. Fields \u00b6 Field Type Description resync Boolean Determines if this snapshot should ingest the dSource from scratch. RemoteEnvironment \u00b6 Represents a remote environment. from dlpx.virtualization.common import RemoteEnvironment environment = RemoteEnvironment ( name , reference , host ) Fields \u00b6 Field Type Description name String Name of the environment. reference String Unique identifier for the environment. host RemoteHost Host that belongs to the environment. RemoteHost \u00b6 Represents a remote host, can be Unix or Windows. from dlpx.virtualization.common import RemoteHost host = RemoteHost ( name , reference , binary_path , scratch_path ) Fields \u00b6 Field Type Description name String Host address. reference String Unique identifier for the host. binary_path String Path to Delphix provided binaries on the host, which are present in the toolkit pushed to the remote host like dlpx_db_exec , dlpx_pfexec , etc. This property is only available for Unix hosts. scratch_path String Path to scratch path on the host. RemoteUser \u00b6 Represents a user on a remote host. from dlpx.virtualization.common import RemoteUser user = RemoteUser ( name , reference ) Fields \u00b6 Field Type Description name String User name. reference String Unique identifier for the user.","title":"Classes"},{"location":"References/Classes/#classes","text":"","title":"Classes"},{"location":"References/Classes/#directsource","text":"Represents a Linked Source object and its properties when using a Direct Linking strategy. from dlpx.virtualization.platform import DirectSource direct_source = DirectSource ( guid , connection , parameters )","title":"DirectSource"},{"location":"References/Classes/#fields","text":"Field Type Description guid String Unique Identifier for the source. connection RemoteConnection Connection for the source environment. parameters LinkedSourceDefinition User input as per the LinkedSource Schema .","title":"Fields"},{"location":"References/Classes/#stagedsource","text":"Represents a Linked Source object and its properties when using a Staged Linking strategy. from dlpx.virtualization.platform import StagedSource staged_source = StagedSource ( guid , source_connection , parameters , mount , staged_connection )","title":"StagedSource"},{"location":"References/Classes/#fields_1","text":"Field Type Description guid String Unique Identifier for the source. source_connection RemoteConnection Connection for the source environment. parameters LinkedSourceDefinition User input as per the LinkedSource Schema . mount Mount Mount point associated with the source. staged_connection RemoteConnection Connection for the staging environment.","title":"Fields"},{"location":"References/Classes/#virtualsource","text":"Represents a Virtual Source object and its properties. from dlpx.virtualization.platform import VirtualSource virtual_source = VirtualSource ( guid , connection , parameters , mounts )","title":"VirtualSource"},{"location":"References/Classes/#fields_2","text":"Field Type Description guid String Unique Identifier for the source. connection RemoteConnection Connection for the source environment. parameters VirtualSourceDefinition User input as per the VirtualSource Schema . mounts list[ Mount ] Mount points associated with the source.","title":"Fields"},{"location":"References/Classes/#remoteconnection","text":"Represents a connection to a source. from dlpx.virtualization.common import RemoteConnection connection = RemoteConnection ( environment , user )","title":"RemoteConnection"},{"location":"References/Classes/#fields_3","text":"Field Type Description environment RemoteEnvironment Environment for the connection. user RemoteUser User for the connection.","title":"Fields"},{"location":"References/Classes/#status","text":"An enum used to represent the state of a linked or virtual source and whether it is functioning as expected. from dlpx.virtualization.platform import Status status = Status . ACTIVE","title":"Status"},{"location":"References/Classes/#values","text":"Value Description ACTIVE Source is healthy and functioning as expected. INACTIVE Source is not functioning as expected.","title":"Values"},{"location":"References/Classes/#mount","text":"Represents a mount exported and mounted to a remote host. from dlpx.virtualization.platform import Mount mount = Mount ( environment , path )","title":"Mount"},{"location":"References/Classes/#fields_4","text":"Field Type Description remote_environment RemoteEnvironment or Reference Environment for the connection. mount_path String The path on the remote host that has the mounted data set. shared_path String Optional. The path of the subdirectory of the data set to mount to the remote host.","title":"Fields"},{"location":"References/Classes/#ownershipspecification","text":"Represents how to set the ownership for a data set. This only applies to Unix Hosts. from dlpx.virtualization.platform import OwnershipSpecification ownership_specification = OwnershipSpecification ( uid , gid )","title":"OwnershipSpecification"},{"location":"References/Classes/#fields_5","text":"Field Type Description uid Integer The user id to set the ownership of the data set to. gid Integer The group id to set the ownership of the data set to.","title":"Fields"},{"location":"References/Classes/#mountspecification","text":"Represents properties for the mount associated with an exported data set. from dlpx.virtualization.platform import MountSpecification mount_specification = MountSpecification ([ mount ], ownership_specification )","title":"MountSpecification"},{"location":"References/Classes/#fields_6","text":"Field Type Description mounts list[ Mount ] The list of mounts to export the data sets to. ownership_specification OwnershipSpecification Optional. Control the ownership attributes for the data set. It defaults to the environment user of the remote environment if it is not specified.","title":"Fields"},{"location":"References/Classes/#snapshotparametersdefinition","text":"User provided parameters for the snapshot operation. It includes a boolean property named resync that can be used to indicate to the plugin whether or not to initiate a full ingestion of the dSource. The parameters are only set during a manual snapshot. When using a sync policy, resync defaults to false . from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . linked . pre_snapshot () def linked_pre_snapshot ( staged_source , repository , source_config , snapshot_parameters ): if snapshot_parameter . resync : print ( snapshot_parameter . resync ) This class will be generated during build and is located with the autogenerated classes. As it is passed into the operation, importing it is not neccessary.","title":"SnapshotParametersDefinition"},{"location":"References/Classes/#fields_7","text":"Field Type Description resync Boolean Determines if this snapshot should ingest the dSource from scratch.","title":"Fields"},{"location":"References/Classes/#remoteenvironment","text":"Represents a remote environment. from dlpx.virtualization.common import RemoteEnvironment environment = RemoteEnvironment ( name , reference , host )","title":"RemoteEnvironment"},{"location":"References/Classes/#fields_8","text":"Field Type Description name String Name of the environment. reference String Unique identifier for the environment. host RemoteHost Host that belongs to the environment.","title":"Fields"},{"location":"References/Classes/#remotehost","text":"Represents a remote host, can be Unix or Windows. from dlpx.virtualization.common import RemoteHost host = RemoteHost ( name , reference , binary_path , scratch_path )","title":"RemoteHost"},{"location":"References/Classes/#fields_9","text":"Field Type Description name String Host address. reference String Unique identifier for the host. binary_path String Path to Delphix provided binaries on the host, which are present in the toolkit pushed to the remote host like dlpx_db_exec , dlpx_pfexec , etc. This property is only available for Unix hosts. scratch_path String Path to scratch path on the host.","title":"Fields"},{"location":"References/Classes/#remoteuser","text":"Represents a user on a remote host. from dlpx.virtualization.common import RemoteUser user = RemoteUser ( name , reference )","title":"RemoteUser"},{"location":"References/Classes/#fields_10","text":"Field Type Description name String User name. reference String Unique identifier for the user.","title":"Fields"},{"location":"References/Decorators/","text":"Decorators \u00b6 The Virtualization SDK exposes decorators to be able to annotate functions that correspond to each Plugin Operation . In the example below, it first instantiates a Plugin() object, that can then be used to tag plugin operations. from dlpx.virtualization.platform import Plugin # Initialize a plugin object plugin = Plugin () # Use the decorator to annotate the function that corresponds to the \"Virtual Source Start\" Plugin Operation @plugin . virtual_source . start () def my_start ( virtual_source , repository , source_config ): print \"running start\" Info Decorators exposed by the Virtualization SDK are inherently python function calls and needs parentheses () appended at the end. Assuming the name of the object, is plugin as above, the table below lists the corresponding decorators for each plugin operation. Plugin Operation Decorator Repository Discovey @plugin.discovery.repository() Source Config Discovey @plugin.discovery.source_config() Direct Linked Source Pre-Snapshot @plugin.linked.pre_snapshot() Direct Linked Source Post-Snapshot @plugin.linked.post_snapshot() Staged Linked Source Pre-Snapshot @plugin.linked.pre_snapshot() Staged Linked Source Post-Snapshot @plugin.linked.post_snapshot() Staged Linked Source Start-Staging @plugin.linked.start_staging() Staged Linked Source Stop-Staging @plugin.linked.stop_staging() Staged Linked Source Status @plugin.linked.status() Staged Linked Source Worker @plugin.linked.worker() Staged Linked Source Mount Specification @plugin.linked.mount_specification() Virtual Source Configure @plugin.virtual.configure() Virtual Source Unconfigure @plugin.virtual.unconfigure() Virtual Source Reconfigure @plugin.virtual.reconfigure() Virtual Source Start @plugin.virtual.start() Virtual Source Stop @plugin.virtual.stop() VirtualSource Pre-Snapshot @plugin.virtual.pre_snapshot() Virtual Source Post-Snapshot @plugin.virtual.post_snapshot() Virtual Source Mount Specification @plugin.virtual.mount_specification() Virtual Source Status @plugin.virtual.status() Repository Data Migration @plugin.upgrade.repository(migration_id) Source Config Data Migration @plugin.upgrade.source_config(migration_id) Linked Source Data Migration @plugin.upgrade.linked_source(migration_id) Virtual Source Data Migration @plugin.upgrade.virtual_source(migration_id) Snapshot Data Migration @plugin.upgrade.snapshot(migration_id) Warning A plugin should only implement the direct operations or the staged operations based on the plugin type","title":"Decorators"},{"location":"References/Decorators/#decorators","text":"The Virtualization SDK exposes decorators to be able to annotate functions that correspond to each Plugin Operation . In the example below, it first instantiates a Plugin() object, that can then be used to tag plugin operations. from dlpx.virtualization.platform import Plugin # Initialize a plugin object plugin = Plugin () # Use the decorator to annotate the function that corresponds to the \"Virtual Source Start\" Plugin Operation @plugin . virtual_source . start () def my_start ( virtual_source , repository , source_config ): print \"running start\" Info Decorators exposed by the Virtualization SDK are inherently python function calls and needs parentheses () appended at the end. Assuming the name of the object, is plugin as above, the table below lists the corresponding decorators for each plugin operation. Plugin Operation Decorator Repository Discovey @plugin.discovery.repository() Source Config Discovey @plugin.discovery.source_config() Direct Linked Source Pre-Snapshot @plugin.linked.pre_snapshot() Direct Linked Source Post-Snapshot @plugin.linked.post_snapshot() Staged Linked Source Pre-Snapshot @plugin.linked.pre_snapshot() Staged Linked Source Post-Snapshot @plugin.linked.post_snapshot() Staged Linked Source Start-Staging @plugin.linked.start_staging() Staged Linked Source Stop-Staging @plugin.linked.stop_staging() Staged Linked Source Status @plugin.linked.status() Staged Linked Source Worker @plugin.linked.worker() Staged Linked Source Mount Specification @plugin.linked.mount_specification() Virtual Source Configure @plugin.virtual.configure() Virtual Source Unconfigure @plugin.virtual.unconfigure() Virtual Source Reconfigure @plugin.virtual.reconfigure() Virtual Source Start @plugin.virtual.start() Virtual Source Stop @plugin.virtual.stop() VirtualSource Pre-Snapshot @plugin.virtual.pre_snapshot() Virtual Source Post-Snapshot @plugin.virtual.post_snapshot() Virtual Source Mount Specification @plugin.virtual.mount_specification() Virtual Source Status @plugin.virtual.status() Repository Data Migration @plugin.upgrade.repository(migration_id) Source Config Data Migration @plugin.upgrade.source_config(migration_id) Linked Source Data Migration @plugin.upgrade.linked_source(migration_id) Virtual Source Data Migration @plugin.upgrade.virtual_source(migration_id) Snapshot Data Migration @plugin.upgrade.snapshot(migration_id) Warning A plugin should only implement the direct operations or the staged operations based on the plugin type","title":"Decorators"},{"location":"References/Glossary/","text":"Glossary \u00b6 Artifact \u00b6 A single file that is the result of a build . It is this artifact which is distributed to users, and which is installed onto engines. Automatic Discovery \u00b6 Discovery which is done by the Delphix Engine (with help from a plugin) itself, with no need for the end user to provide any information. Building \u00b6 The process of creating an artifact from the collection of files that make up the plugin's source code. Data Migration \u00b6 A python function which is called as part of the upgrade process. It handles transforming data from an older format to a newer format. More details here . Data Migration ID \u00b6 Each data migration is tagged with a unique ID. This allows the Delphix Engine to know which data migrations need to be run, in which order, when upgrading to a new plugin version. More details here . Decorator \u00b6 A Python construct which is used by plugins to \"tag\" certain functions, so that the Delphix Engine knows which function corresponds to which plugin operation. Direct Linking \u00b6 A strategy that involves data being ingested directly from the source environment onto the Delphix Engine, without the assistance of a staging environment. Discovery \u00b6 The process by which the Delphix Engine learns about how a particular environment can be used for ingesting or virtualizing datasets. dSource \u00b6 See Linked Dataset Environment \u00b6 A remote system that the Delphix Engine can interact with. An environment can be used as a source , staging or target environment (or any combination of those). For example, a Linux machine that the Delphix Engine can connect to is an environment. Environment User \u00b6 A set of user credentials that the Delphix Engine can use to interact with an Environmnet . For example, a username and password to login to a Linux machine. Linked Dataset \u00b6 A dataset on the Delphix Engine which holds an ingested copy of a pre-existing external dataset from a source environment. A linked dataset is often called a dSource . Linked Source \u00b6 An object on the Delphix Engine that holds information related to a linked dataset . Linking \u00b6 The process by which the Delphix Engine connects a new dSource to a pre-existing dataset on a source environment. Logging \u00b6 Logging is when a plugin writes out some human-readable information to a log file. The log file can then be examined, typically in order to debug a problem with the plugin. Plugin Config \u00b6 A YAML file containing a list of plugin properties: What is the plugin's name? What version of the plugin is this? Etc. More details here . Manual Discovery \u00b6 Discovery which the end user does by manually entering the necessary information into the Delphix Engine. Mount Specification \u00b6 A collection of information, provided by the plugin, which give all the details about how and where virtual datasets should be mounted onto target environments . This term is often shortened to \"Mount Spec\". Password Properties \u00b6 In schemas , any string property can be tagged with \"format\": \"password\" . This will let the Delphix Engine know that the property contains sensitive information. Any such values will only be stored in encrypted format, and the UI will not display the values on screen. Platform Libraries \u00b6 A set of Python functions that are provided by the Virtualization Platform. Plugins use these library functions to request that the Virtualization Platform do some task on behalf of the plugin. For example, running a Bash command on an environment, or making an log entry. Plugin \u00b6 A tool that customizes the Delphix Engine so it knows how to interact with a particular kind of dataset. Plugin Operation \u00b6 A piece of functionality that provided by a plugin in order to customize Delphix Engine behavior to work with a particular kind of dataset. A plugin operation is implemented as a Python function. For example, a MySQL plugin might provide an operation called \"stop\" which knows how to stop a MySQL database. Provisioning \u00b6 The process of making a virtual copy of a dataset and making it available for use on a target environment. Replication \u00b6 Delphix allows end users to replicate data objects between Delphix Engines by creating a replication profile. Data objects that belong to a plugin can also be part of the replication profile. Refer to the Delphix Engine Documentation for more details. Repository \u00b6 Information that represents a set of dependencies that a dataset requires in order to be functional. For example, a particular Postgres database might require an installed Postgres 9.6 DBMS, and so its associated repository would contain all the information required to interact with that DBMS. Schema \u00b6 A formal description of a data type. Plugins use JSON format for their schemas . Snapshot \u00b6 A point-in-time read-only copy of a dataset. A snapshot includes associated metadata represented by the SnapshotDefinition Schema Snapshot Parameter \u00b6 User provided parameters for the snapshot operation. Currently the only properties the parameter has is resync. Source Config \u00b6 A collection of information that the Delphix Engine needs to interact with a dataset (whether linked or virtual on an environment . Source Environment \u00b6 An environment containing data that is ingested by the Delphix Engine. Staged Linking \u00b6 A strategy where a staging environment is used to coordinate the ingestion of data into a dsource . Staging Environment \u00b6 An environment used by the Delphix Engine to coordinate ingestion from a source environment . Syncing \u00b6 The process by which the Delphix Engine ingests data from a dataset on a source environment into a dsource . Syncing always happens immediately after linking , and typically is done periodically thereafter. Target Environment \u00b6 An environment on which Delphix-provided virtualized datasets can be used. Upgrade Operation \u00b6 A special plugin operation that takes data produced by an older version of a plugin, and transforms it into the format expected by the new version of the plugin. VDB \u00b6 See Virtual Dataset Version \u00b6 A string identifier that is unique for every public release of a plugin. Virtual Dataset \u00b6 A dataset that has been cloned from a snapshot, and whose data is stored on the Delphix Engine. A virtual dataset is made available for use by mounting it to a target environment . A virtual dataset is often called a \"VDB\". Virtual Source \u00b6 An object on the Delphix Engine that holds information related to a virtual dataset . YAML \u00b6 YAML is a simple language often used for configuration files. Plugins define their plugin config using YAML.","title":"Glossary"},{"location":"References/Glossary/#glossary","text":"","title":"Glossary"},{"location":"References/Glossary/#artifact","text":"A single file that is the result of a build . It is this artifact which is distributed to users, and which is installed onto engines.","title":"Artifact"},{"location":"References/Glossary/#automatic-discovery","text":"Discovery which is done by the Delphix Engine (with help from a plugin) itself, with no need for the end user to provide any information.","title":"Automatic Discovery"},{"location":"References/Glossary/#building","text":"The process of creating an artifact from the collection of files that make up the plugin's source code.","title":"Building"},{"location":"References/Glossary/#data-migration","text":"A python function which is called as part of the upgrade process. It handles transforming data from an older format to a newer format. More details here .","title":"Data Migration"},{"location":"References/Glossary/#data-migration-id","text":"Each data migration is tagged with a unique ID. This allows the Delphix Engine to know which data migrations need to be run, in which order, when upgrading to a new plugin version. More details here .","title":"Data Migration ID"},{"location":"References/Glossary/#decorator","text":"A Python construct which is used by plugins to \"tag\" certain functions, so that the Delphix Engine knows which function corresponds to which plugin operation.","title":"Decorator"},{"location":"References/Glossary/#direct-linking","text":"A strategy that involves data being ingested directly from the source environment onto the Delphix Engine, without the assistance of a staging environment.","title":"Direct Linking"},{"location":"References/Glossary/#discovery","text":"The process by which the Delphix Engine learns about how a particular environment can be used for ingesting or virtualizing datasets.","title":"Discovery"},{"location":"References/Glossary/#dsource","text":"See Linked Dataset","title":"dSource"},{"location":"References/Glossary/#environment","text":"A remote system that the Delphix Engine can interact with. An environment can be used as a source , staging or target environment (or any combination of those). For example, a Linux machine that the Delphix Engine can connect to is an environment.","title":"Environment"},{"location":"References/Glossary/#environment-user","text":"A set of user credentials that the Delphix Engine can use to interact with an Environmnet . For example, a username and password to login to a Linux machine.","title":"Environment User"},{"location":"References/Glossary/#linked-dataset","text":"A dataset on the Delphix Engine which holds an ingested copy of a pre-existing external dataset from a source environment. A linked dataset is often called a dSource .","title":"Linked Dataset"},{"location":"References/Glossary/#linked-source","text":"An object on the Delphix Engine that holds information related to a linked dataset .","title":"Linked Source"},{"location":"References/Glossary/#linking","text":"The process by which the Delphix Engine connects a new dSource to a pre-existing dataset on a source environment.","title":"Linking"},{"location":"References/Glossary/#logging","text":"Logging is when a plugin writes out some human-readable information to a log file. The log file can then be examined, typically in order to debug a problem with the plugin.","title":"Logging"},{"location":"References/Glossary/#plugin-config","text":"A YAML file containing a list of plugin properties: What is the plugin's name? What version of the plugin is this? Etc. More details here .","title":"Plugin Config"},{"location":"References/Glossary/#manual-discovery","text":"Discovery which the end user does by manually entering the necessary information into the Delphix Engine.","title":"Manual Discovery"},{"location":"References/Glossary/#mount-specification","text":"A collection of information, provided by the plugin, which give all the details about how and where virtual datasets should be mounted onto target environments . This term is often shortened to \"Mount Spec\".","title":"Mount Specification"},{"location":"References/Glossary/#password-properties","text":"In schemas , any string property can be tagged with \"format\": \"password\" . This will let the Delphix Engine know that the property contains sensitive information. Any such values will only be stored in encrypted format, and the UI will not display the values on screen.","title":"Password Properties"},{"location":"References/Glossary/#platform-libraries","text":"A set of Python functions that are provided by the Virtualization Platform. Plugins use these library functions to request that the Virtualization Platform do some task on behalf of the plugin. For example, running a Bash command on an environment, or making an log entry.","title":"Platform Libraries"},{"location":"References/Glossary/#plugin","text":"A tool that customizes the Delphix Engine so it knows how to interact with a particular kind of dataset.","title":"Plugin"},{"location":"References/Glossary/#plugin-operation","text":"A piece of functionality that provided by a plugin in order to customize Delphix Engine behavior to work with a particular kind of dataset. A plugin operation is implemented as a Python function. For example, a MySQL plugin might provide an operation called \"stop\" which knows how to stop a MySQL database.","title":"Plugin Operation"},{"location":"References/Glossary/#provisioning","text":"The process of making a virtual copy of a dataset and making it available for use on a target environment.","title":"Provisioning"},{"location":"References/Glossary/#replication","text":"Delphix allows end users to replicate data objects between Delphix Engines by creating a replication profile. Data objects that belong to a plugin can also be part of the replication profile. Refer to the Delphix Engine Documentation for more details.","title":"Replication"},{"location":"References/Glossary/#repository","text":"Information that represents a set of dependencies that a dataset requires in order to be functional. For example, a particular Postgres database might require an installed Postgres 9.6 DBMS, and so its associated repository would contain all the information required to interact with that DBMS.","title":"Repository"},{"location":"References/Glossary/#schema","text":"A formal description of a data type. Plugins use JSON format for their schemas .","title":"Schema"},{"location":"References/Glossary/#snapshot","text":"A point-in-time read-only copy of a dataset. A snapshot includes associated metadata represented by the SnapshotDefinition Schema","title":"Snapshot"},{"location":"References/Glossary/#snapshot-parameter","text":"User provided parameters for the snapshot operation. Currently the only properties the parameter has is resync.","title":"Snapshot Parameter"},{"location":"References/Glossary/#source-config","text":"A collection of information that the Delphix Engine needs to interact with a dataset (whether linked or virtual on an environment .","title":"Source Config"},{"location":"References/Glossary/#source-environment","text":"An environment containing data that is ingested by the Delphix Engine.","title":"Source Environment"},{"location":"References/Glossary/#staged-linking","text":"A strategy where a staging environment is used to coordinate the ingestion of data into a dsource .","title":"Staged Linking"},{"location":"References/Glossary/#staging-environment","text":"An environment used by the Delphix Engine to coordinate ingestion from a source environment .","title":"Staging Environment"},{"location":"References/Glossary/#syncing","text":"The process by which the Delphix Engine ingests data from a dataset on a source environment into a dsource . Syncing always happens immediately after linking , and typically is done periodically thereafter.","title":"Syncing"},{"location":"References/Glossary/#target-environment","text":"An environment on which Delphix-provided virtualized datasets can be used.","title":"Target Environment"},{"location":"References/Glossary/#upgrade-operation","text":"A special plugin operation that takes data produced by an older version of a plugin, and transforms it into the format expected by the new version of the plugin.","title":"Upgrade Operation"},{"location":"References/Glossary/#vdb","text":"See Virtual Dataset","title":"VDB"},{"location":"References/Glossary/#version","text":"A string identifier that is unique for every public release of a plugin.","title":"Version"},{"location":"References/Glossary/#virtual-dataset","text":"A dataset that has been cloned from a snapshot, and whose data is stored on the Delphix Engine. A virtual dataset is made available for use by mounting it to a target environment . A virtual dataset is often called a \"VDB\".","title":"Virtual Dataset"},{"location":"References/Glossary/#virtual-source","text":"An object on the Delphix Engine that holds information related to a virtual dataset .","title":"Virtual Source"},{"location":"References/Glossary/#yaml","text":"YAML is a simple language often used for configuration files. Plugins define their plugin config using YAML.","title":"YAML"},{"location":"References/Logging/","text":"Logging \u00b6 What is logging? \u00b6 The Virtualization Platform keeps plugin-specific log files. A plugin can, at any point in any of its plugin operations , write out some text to its log file(s). These log files can be examined later, typically to try to debug a problem with the plugin. Overview \u00b6 The Virtualization Platform integrates with Python's built-in logging framework . A special Handler is exposed by the platform at dlpx.virtualization.libs.PlatformHandler . This handler needs to be added to the Python logger your plugin creates. Logging statements made through Python's logging framework will then be routed to the platform. Basic Setup \u00b6 Below is the absolute minimum needed to setup logging for the platform. Please refer to Python's logging documentation and the example below to better understand how it can be customized. import logging from dlpx.virtualization.libs import PlatformHandler # Get the root logger. logger = logging . getLogger () logger . addHandler ( PlatformHandler ()) # The root logger's default level is logging.WARNING. # Without the line below, logging statements of levels # lower than logging.WARNING will be suppressed. logger . setLevel ( logging . DEBUG ) Logging Setup Python's logging framework is global. Setup only needs to happen once, but where it happens is important. Any logging statements that occur before the PlatformHandler is added will not be logged by the platform. It is highly recommended that the logging setup is done in the plugin's entry point module before any operations are ran. Add the PlatformHandler to the root logger Loggers in Python have a hierarchy and all loggers are children of a special logger called the \"root logger\". Logging hierarchy is not always intuitive and depends on how modules are structured. To avoid this complexity, add the PlatformHandler to the root logger. The root logger can be retrieved with logging.getLogger() . Usage \u00b6 Once the PlatformHandler has been added to the logger, logging is done with Python's Logger object. Below is a simple example including the basic setup code used above: import logging from dlpx.virtualization.libs import PlatformHandler logger = logging . getLogger () logger . addHandler ( PlatformHandler ()) # The root logger's default level is logging.WARNING. # Without the line below, logging statements of levels # lower than logging.WARNING will be suppressed. logger . setLevel ( logging . DEBUG ) logger . debug ( 'debug' ) logger . info ( 'info' ) logger . error ( 'error' ) Example \u00b6 Imagine you notice that your plugin is taking a very long time to do discovery. Everything works, it just takes much longer than expected. You'd like to figure out why. Info Refer to Managing Scripts for Remote Execution for how remote scripts can be stored and retrieved. Suppose your plugin has a source config discovery operation that looks like this (code is abbreviated to be easier to follow): import pkgutil from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . discovery . repository () def repository_discovery ( source_connection ): return [ RepositoryDefinition ( 'Logging Example' )] @plugin . discovery . source_config () def source_config_discovery ( source_connection , repository ): version_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_db_version.sh' )) users_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_db_users.sh' )) db_results = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_databases.sh' )) status_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_database_statuses.sh' )) # Return an empty list for simplicity. In reality # something would be done with the results above. return [] Now, imagine that you notice that it's taking a long time to do discovery, and you'd like to try to figure out why. One thing that might help is to add logging, like this: import logging import pkgutil from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin from generated.definitions import RepositoryDefinition # This should probably be defined in its own module outside # of the plugin's entry point file. It is here for simplicity. def _setup_logger (): # This will log the time, level, filename, line number, and log message. log_message_format = '[ %(asctime)s ] [ %(levelname)s ] [ %(filename)s : %(lineno)d ] %(message)s ' log_message_date_format = '%Y-%m- %d %H:%M:%S' # Create a custom formatter. This will help with diagnosability. formatter = logging . Formatter ( log_message_format , datefmt = log_message_date_format ) platform_handler = libs . PlatformHandler () platform_handler . setFormatter ( formatter ) logger = logging . getLogger () logger . addHandler ( platform_handler ) # By default the root logger's level is logging.WARNING. logger . setLevel ( logging . DEBUG ) # Setup the logger. _setup_logger () # logging.getLogger(__name__) is the convention way to get a logger in Python. # It returns a new logger per module and will be a child of the root logger. # Since we setup the root logger, nothing else needs to be done to set this # one up. logger = logging . getLogger ( __name__ ) plugin = Plugin () @plugin . discovery . repository () def repository_discovery ( source_connection ): return [ RepositoryDefinition ( 'Logging Example' )] @plugin . discovery . source_config () def source_config_discovery ( source_connection , repository ): logger . debug ( 'About to get DB version' ) version_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_db_version.sh' )) logger . debug ( 'About to get DB users' ) users_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_db_users.sh' )) logger . debug ( 'About to get databases' ) db_results = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_databases.sh' )) logger . debug ( 'About to get DB statuses' ) status_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_database_statuses.sh' )) logger . debug ( 'Done collecting data' ) # Return an empty list for simplicity. In reality # something would be done with the results above. return [] When you look at the log file, perhaps you'll see something like this: [Worker-360|JOB-315|ENVIRONMENT_DISCOVER(UNIX_HOST_ENVIRONMENT-5)] [2019-04-30 12:10:42] [DEBUG] [python_runner.py:44] About to get DB version [Worker-360|JOB-316|DB_SYNC(APPDATA_CONTAINER-21)] [2019-04-30 12:19:35] [DEBUG] [python_runner.py:49] About to get DB users [Worker-325|JOB-280|ENVIRONMENT_REFRESH(UNIX_HOST_ENVIRONMENT-5)] [DEBUG] [plugin_runner.py:51] About to get databases [Worker-326|JOB-281|SOURCES_DISABLE(UNIX_HOST_ENVIRONMENT-5)] [DEBUG] [plugin_runner.py:53] About to get DB statuses You can see that it only takes a few seconds for us do each of our data collection steps, with the exception of getting the users, which takes over 13 minutes! We now know that our slowdown is something to do with how our bash script is collecting all the users. Logging has gotten us a lot closer to figuring out the problem. How to retrieve logs \u00b6 Download a support bundle by going to Help > Support Logs and select Download . The logs will be in a the support bundle under log/mgmt_log/plugin_log/<plugin name> . Logging Levels \u00b6 Python has a number of preset logging levels and allows for custom ones as well. Since logging on the Virtualization Platform uses the logging framework, log statements of all levels are supported. However, the Virtualization Platform will map all logging levels into three files: debug.log , info.log , and error.log in the following way: Python Logging Level Logging File DEBUG debug.log INFO info.log WARN error.log WARNING error.log ERROR error.log CRITICAL error.log As is the case with the logging framework, logging statements are hierarchical: logging statements made at the logging.DEBUG level will be written only to debug.log while logging statements made at the logging.ERROR level will be written to debug.log , info.log , and error.log . Sensitive data \u00b6 Remember that logging data means writing that data out in cleartext. Make sure you never log any data that could be secret or sensitive (passwords, etc.). For more details please see our section on sensitive data","title":"Logging"},{"location":"References/Logging/#logging","text":"","title":"Logging"},{"location":"References/Logging/#what-is-logging","text":"The Virtualization Platform keeps plugin-specific log files. A plugin can, at any point in any of its plugin operations , write out some text to its log file(s). These log files can be examined later, typically to try to debug a problem with the plugin.","title":"What is logging?"},{"location":"References/Logging/#overview","text":"The Virtualization Platform integrates with Python's built-in logging framework . A special Handler is exposed by the platform at dlpx.virtualization.libs.PlatformHandler . This handler needs to be added to the Python logger your plugin creates. Logging statements made through Python's logging framework will then be routed to the platform.","title":"Overview"},{"location":"References/Logging/#basic-setup","text":"Below is the absolute minimum needed to setup logging for the platform. Please refer to Python's logging documentation and the example below to better understand how it can be customized. import logging from dlpx.virtualization.libs import PlatformHandler # Get the root logger. logger = logging . getLogger () logger . addHandler ( PlatformHandler ()) # The root logger's default level is logging.WARNING. # Without the line below, logging statements of levels # lower than logging.WARNING will be suppressed. logger . setLevel ( logging . DEBUG ) Logging Setup Python's logging framework is global. Setup only needs to happen once, but where it happens is important. Any logging statements that occur before the PlatformHandler is added will not be logged by the platform. It is highly recommended that the logging setup is done in the plugin's entry point module before any operations are ran. Add the PlatformHandler to the root logger Loggers in Python have a hierarchy and all loggers are children of a special logger called the \"root logger\". Logging hierarchy is not always intuitive and depends on how modules are structured. To avoid this complexity, add the PlatformHandler to the root logger. The root logger can be retrieved with logging.getLogger() .","title":"Basic Setup"},{"location":"References/Logging/#usage","text":"Once the PlatformHandler has been added to the logger, logging is done with Python's Logger object. Below is a simple example including the basic setup code used above: import logging from dlpx.virtualization.libs import PlatformHandler logger = logging . getLogger () logger . addHandler ( PlatformHandler ()) # The root logger's default level is logging.WARNING. # Without the line below, logging statements of levels # lower than logging.WARNING will be suppressed. logger . setLevel ( logging . DEBUG ) logger . debug ( 'debug' ) logger . info ( 'info' ) logger . error ( 'error' )","title":"Usage"},{"location":"References/Logging/#example","text":"Imagine you notice that your plugin is taking a very long time to do discovery. Everything works, it just takes much longer than expected. You'd like to figure out why. Info Refer to Managing Scripts for Remote Execution for how remote scripts can be stored and retrieved. Suppose your plugin has a source config discovery operation that looks like this (code is abbreviated to be easier to follow): import pkgutil from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . discovery . repository () def repository_discovery ( source_connection ): return [ RepositoryDefinition ( 'Logging Example' )] @plugin . discovery . source_config () def source_config_discovery ( source_connection , repository ): version_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_db_version.sh' )) users_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_db_users.sh' )) db_results = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_databases.sh' )) status_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_database_statuses.sh' )) # Return an empty list for simplicity. In reality # something would be done with the results above. return [] Now, imagine that you notice that it's taking a long time to do discovery, and you'd like to try to figure out why. One thing that might help is to add logging, like this: import logging import pkgutil from dlpx.virtualization import libs from dlpx.virtualization.platform import Plugin from generated.definitions import RepositoryDefinition # This should probably be defined in its own module outside # of the plugin's entry point file. It is here for simplicity. def _setup_logger (): # This will log the time, level, filename, line number, and log message. log_message_format = '[ %(asctime)s ] [ %(levelname)s ] [ %(filename)s : %(lineno)d ] %(message)s ' log_message_date_format = '%Y-%m- %d %H:%M:%S' # Create a custom formatter. This will help with diagnosability. formatter = logging . Formatter ( log_message_format , datefmt = log_message_date_format ) platform_handler = libs . PlatformHandler () platform_handler . setFormatter ( formatter ) logger = logging . getLogger () logger . addHandler ( platform_handler ) # By default the root logger's level is logging.WARNING. logger . setLevel ( logging . DEBUG ) # Setup the logger. _setup_logger () # logging.getLogger(__name__) is the convention way to get a logger in Python. # It returns a new logger per module and will be a child of the root logger. # Since we setup the root logger, nothing else needs to be done to set this # one up. logger = logging . getLogger ( __name__ ) plugin = Plugin () @plugin . discovery . repository () def repository_discovery ( source_connection ): return [ RepositoryDefinition ( 'Logging Example' )] @plugin . discovery . source_config () def source_config_discovery ( source_connection , repository ): logger . debug ( 'About to get DB version' ) version_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_db_version.sh' )) logger . debug ( 'About to get DB users' ) users_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_db_users.sh' )) logger . debug ( 'About to get databases' ) db_results = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_databases.sh' )) logger . debug ( 'About to get DB statuses' ) status_result = libs . run_bash ( source_connection , pkgutil . get_data ( 'resources' , 'get_database_statuses.sh' )) logger . debug ( 'Done collecting data' ) # Return an empty list for simplicity. In reality # something would be done with the results above. return [] When you look at the log file, perhaps you'll see something like this: [Worker-360|JOB-315|ENVIRONMENT_DISCOVER(UNIX_HOST_ENVIRONMENT-5)] [2019-04-30 12:10:42] [DEBUG] [python_runner.py:44] About to get DB version [Worker-360|JOB-316|DB_SYNC(APPDATA_CONTAINER-21)] [2019-04-30 12:19:35] [DEBUG] [python_runner.py:49] About to get DB users [Worker-325|JOB-280|ENVIRONMENT_REFRESH(UNIX_HOST_ENVIRONMENT-5)] [DEBUG] [plugin_runner.py:51] About to get databases [Worker-326|JOB-281|SOURCES_DISABLE(UNIX_HOST_ENVIRONMENT-5)] [DEBUG] [plugin_runner.py:53] About to get DB statuses You can see that it only takes a few seconds for us do each of our data collection steps, with the exception of getting the users, which takes over 13 minutes! We now know that our slowdown is something to do with how our bash script is collecting all the users. Logging has gotten us a lot closer to figuring out the problem.","title":"Example"},{"location":"References/Logging/#how-to-retrieve-logs","text":"Download a support bundle by going to Help > Support Logs and select Download . The logs will be in a the support bundle under log/mgmt_log/plugin_log/<plugin name> .","title":"How to retrieve logs"},{"location":"References/Logging/#logging-levels","text":"Python has a number of preset logging levels and allows for custom ones as well. Since logging on the Virtualization Platform uses the logging framework, log statements of all levels are supported. However, the Virtualization Platform will map all logging levels into three files: debug.log , info.log , and error.log in the following way: Python Logging Level Logging File DEBUG debug.log INFO info.log WARN error.log WARNING error.log ERROR error.log CRITICAL error.log As is the case with the logging framework, logging statements are hierarchical: logging statements made at the logging.DEBUG level will be written only to debug.log while logging statements made at the logging.ERROR level will be written to debug.log , info.log , and error.log .","title":"Logging Levels"},{"location":"References/Logging/#sensitive-data","text":"Remember that logging data means writing that data out in cleartext. Make sure you never log any data that could be secret or sensitive (passwords, etc.). For more details please see our section on sensitive data","title":"Sensitive data"},{"location":"References/Platform_Libraries/","text":"Platform Libraries \u00b6 Set of functions that plugins can use these for executing remote commands, etc. run_bash \u00b6 Executes a bash command on a remote Unix host. Signature \u00b6 def run_bash(remote_connection, command, variables=None, use_login_shell=False, check=False) Arguments \u00b6 Argument Type Description remote_connection RemoteConnection Connection associated with the remote host to run the command on. command String Command to run on the host. variables dict[String, String] Optional . Environement variables to set when running the command. use_login_shell boolean Optional . Whether to use a login shell. check boolean Optional . Whether or not to raise an exception if the exit_code in the RunBashResponse is non-zero. Returns \u00b6 An object of RunBashResponse Field Type Description exit_code Integer Exit code from the command. stdout String Stdout from the command. stderr String Stderr from the command. Examples \u00b6 Calling bash with an inline command. from dlpx.virtualization import libs command = \"echo 'Hi' >> /tmp/debug.log\" variables = { \"var\" : \"val\" } response = libs . run_bash ( connection , command , variables ) print response . exit_code print response . stdout print response . stderr Using parameters to construct a bash command. from dlpx.virtualization import libs name = virtual_source . parameters . username port = virtual_source . parameters . port command = \"mysqldump -u {} -p {} \" . format ( name , port ) response = libs . run_bash ( connection , command ) Running a bash script that is saved in a directory. import pkgutil from dlpx.virtualization import libs script_content = pkgutil . get_data ( 'resources' , 'get_date.sh' ) # Execute script on remote host response = libs . run_bash ( direct_source . connection , script_content ) For more information please go to Managing Scripts for Remote Execution section. run_expect \u00b6 Executes a tcl command or script on a remote Unix host. Signature \u00b6 def run_expect(remote_connection, command, variables=None) Arguments \u00b6 Argument Type Description remote_connection RemoteConnection Connection associated with the remote host to run the command on. command String Expect(Tcl) command to run. variables dict[String, String] Optional . Environement variables to set when running the command. Returns \u00b6 An object of RunExpectResponse Field Type Description exit_code Integer Exit code from the command. stdout String Stdout from the command. stderr String Stderr from the command. Example \u00b6 Calling expect with an inline command. from dlpx.virtualization import libs command = \"puts 'Hi'\" variables = { \"var\" : \"val\" } repsonse = libs . run_expect ( connection , command , variables ) print response . exit_code print response . stdout print response . stderr run_powershell \u00b6 Executes a powershell command on a remote Windows host. Signature \u00b6 def run_powershell(remote_connection, command, variables=None, check=False) Arguments \u00b6 Argument Type Description remote_connection RemoteConnection Connection associated with the remote host to run the command on. command String Command to run to the remote host. variables dict[String, String] Optional . Environement variables to set when running the command. check boolean Optional . Whether or not to raise an exception if the exit_code in the RunPowershellResponse is non-zero. Returns \u00b6 An object of RunPowershellResponse Field Type Description exit_code Integer Exit code from the command. stdout String Stdout from the command. stderr String Stderr from the command. Example \u00b6 Calling powershell with an inline command. from dlpx.virtualization import libs command = \"Write-Output 'Hi'\" variables = { \"var\" : \"val\" } response = libs . run_powershell ( connection , command , variables ) print response . exit_code print response . stdout print response . stderr run_sync \u00b6 Copies files from the remote source host directly into the dSource, without involving a staging host. Signature \u00b6 def run_sync(remote_connection, source_directory, rsync_user=None, exclude_paths=None, sym_links_to_follow=None) Arguments \u00b6 Argument Type Description remote_connection RemoteConnection Connection associated with the remote host to run the command on. source_directory String Directory of files to be synced. rsync_user String Optional User who has access to the directory to be synced. exclude_paths list[String] Optional Paths to be excluded. sym_links_to_follow list[String] Optional Symbollic links to follow if any. Returns \u00b6 None Example \u00b6 from dlpx.virtualization import libs source_directory = \"sourceDirectory\" rsync_user = \"rsyncUser\" exclude_paths = [ \"/path1\" , \"/path2\" ] sym_links_to_follow = [ \"/path3\" , \"/path4\" ] libs . run_sync ( connection , source_directory , rsync_user , exclude_paths , sym_links_to_follow )","title":"Platform Libraries"},{"location":"References/Platform_Libraries/#platform-libraries","text":"Set of functions that plugins can use these for executing remote commands, etc.","title":"Platform Libraries"},{"location":"References/Platform_Libraries/#run_bash","text":"Executes a bash command on a remote Unix host.","title":"run_bash"},{"location":"References/Platform_Libraries/#signature","text":"def run_bash(remote_connection, command, variables=None, use_login_shell=False, check=False)","title":"Signature"},{"location":"References/Platform_Libraries/#arguments","text":"Argument Type Description remote_connection RemoteConnection Connection associated with the remote host to run the command on. command String Command to run on the host. variables dict[String, String] Optional . Environement variables to set when running the command. use_login_shell boolean Optional . Whether to use a login shell. check boolean Optional . Whether or not to raise an exception if the exit_code in the RunBashResponse is non-zero.","title":"Arguments"},{"location":"References/Platform_Libraries/#returns","text":"An object of RunBashResponse Field Type Description exit_code Integer Exit code from the command. stdout String Stdout from the command. stderr String Stderr from the command.","title":"Returns"},{"location":"References/Platform_Libraries/#examples","text":"Calling bash with an inline command. from dlpx.virtualization import libs command = \"echo 'Hi' >> /tmp/debug.log\" variables = { \"var\" : \"val\" } response = libs . run_bash ( connection , command , variables ) print response . exit_code print response . stdout print response . stderr Using parameters to construct a bash command. from dlpx.virtualization import libs name = virtual_source . parameters . username port = virtual_source . parameters . port command = \"mysqldump -u {} -p {} \" . format ( name , port ) response = libs . run_bash ( connection , command ) Running a bash script that is saved in a directory. import pkgutil from dlpx.virtualization import libs script_content = pkgutil . get_data ( 'resources' , 'get_date.sh' ) # Execute script on remote host response = libs . run_bash ( direct_source . connection , script_content ) For more information please go to Managing Scripts for Remote Execution section.","title":"Examples"},{"location":"References/Platform_Libraries/#run_expect","text":"Executes a tcl command or script on a remote Unix host.","title":"run_expect"},{"location":"References/Platform_Libraries/#signature_1","text":"def run_expect(remote_connection, command, variables=None)","title":"Signature"},{"location":"References/Platform_Libraries/#arguments_1","text":"Argument Type Description remote_connection RemoteConnection Connection associated with the remote host to run the command on. command String Expect(Tcl) command to run. variables dict[String, String] Optional . Environement variables to set when running the command.","title":"Arguments"},{"location":"References/Platform_Libraries/#returns_1","text":"An object of RunExpectResponse Field Type Description exit_code Integer Exit code from the command. stdout String Stdout from the command. stderr String Stderr from the command.","title":"Returns"},{"location":"References/Platform_Libraries/#example","text":"Calling expect with an inline command. from dlpx.virtualization import libs command = \"puts 'Hi'\" variables = { \"var\" : \"val\" } repsonse = libs . run_expect ( connection , command , variables ) print response . exit_code print response . stdout print response . stderr","title":"Example"},{"location":"References/Platform_Libraries/#run_powershell","text":"Executes a powershell command on a remote Windows host.","title":"run_powershell"},{"location":"References/Platform_Libraries/#signature_2","text":"def run_powershell(remote_connection, command, variables=None, check=False)","title":"Signature"},{"location":"References/Platform_Libraries/#arguments_2","text":"Argument Type Description remote_connection RemoteConnection Connection associated with the remote host to run the command on. command String Command to run to the remote host. variables dict[String, String] Optional . Environement variables to set when running the command. check boolean Optional . Whether or not to raise an exception if the exit_code in the RunPowershellResponse is non-zero.","title":"Arguments"},{"location":"References/Platform_Libraries/#returns_2","text":"An object of RunPowershellResponse Field Type Description exit_code Integer Exit code from the command. stdout String Stdout from the command. stderr String Stderr from the command.","title":"Returns"},{"location":"References/Platform_Libraries/#example_1","text":"Calling powershell with an inline command. from dlpx.virtualization import libs command = \"Write-Output 'Hi'\" variables = { \"var\" : \"val\" } response = libs . run_powershell ( connection , command , variables ) print response . exit_code print response . stdout print response . stderr","title":"Example"},{"location":"References/Platform_Libraries/#run_sync","text":"Copies files from the remote source host directly into the dSource, without involving a staging host.","title":"run_sync"},{"location":"References/Platform_Libraries/#signature_3","text":"def run_sync(remote_connection, source_directory, rsync_user=None, exclude_paths=None, sym_links_to_follow=None)","title":"Signature"},{"location":"References/Platform_Libraries/#arguments_3","text":"Argument Type Description remote_connection RemoteConnection Connection associated with the remote host to run the command on. source_directory String Directory of files to be synced. rsync_user String Optional User who has access to the directory to be synced. exclude_paths list[String] Optional Paths to be excluded. sym_links_to_follow list[String] Optional Symbollic links to follow if any.","title":"Arguments"},{"location":"References/Platform_Libraries/#returns_3","text":"None","title":"Returns"},{"location":"References/Platform_Libraries/#example_2","text":"from dlpx.virtualization import libs source_directory = \"sourceDirectory\" rsync_user = \"rsyncUser\" exclude_paths = [ \"/path1\" , \"/path2\" ] sym_links_to_follow = [ \"/path3\" , \"/path4\" ] libs . run_sync ( connection , source_directory , rsync_user , exclude_paths , sym_links_to_follow )","title":"Example"},{"location":"References/Plugin_Config/","text":"Plugin Config \u00b6 The plugin config is a YAML file that marks the root of a plugin and defines metadata about the plugin and its structure. The config file is read at build time to generate the upload artifact. The name of the file can be specified during the build. By default, the build looks for plugin_config.yml in the current working directory. Fields \u00b6 Field Name Required Type Description id Y string The unique id of the plugin in a valid UUID format. name N string The display name of the plugin. This will be used in the UI. If it is not specified name will be equal to id. externalVersion N string The plugin's external version . This is a freeform string. If it is not supplied, the build number is used as an external version. buildNumber Y string The plugin's build number . This string must conform to the format described here . hostTypes Y list The host type that the plugin supports. Either UNIX or WINDOWS . schemaFile Y string The path to the JSON file that contains the plugin's schema definitions . This path can be absolute or relative to the directory containing the plugin config file. srcDir Y string The path to the directory that contains the source code for the plugin. During execution of a plugin operation, this directory will be the current working directory of the Python interpreter. Any modules or resources defined outside of this directory will be inaccessible at runtime. This path can be absolute or relative to the directory containing the plugin config file. entryPoint Y string A fully qualified Python symbol that points to the dlpx.virtualization.platform.Plugin object that defines the plugin. It must be in the form importable.module:object_name where importable.module is in srcDir . manualDiscovery N boolean True if the plugin supports manual discovery of source config objects. The default value is true . pluginType Y enum The ingestion strategy of the plugin. Can be either STAGED or DIRECT . language Y enum Must be PYTHON27 . defaultLocale N enum The locale to be used by the plugin if the Delphix user does not specify one. Plugin messages will be displayed in this locale by default. The default value is en-us . rootSquashEnabled N boolean This dictates whether \"root squash\" is enabled on NFS mounts for the plugin (i.e. whether the root user on remote hosts has access to the NFS mounts). Setting this to false allows processes usually run as root , like Docker daemons, access to the NFS mounts. The default value is true . This field only applies to Unix hosts. Example \u00b6 Assume the following basic plugin structure: \u251c\u2500\u2500 plugin_config.yml \u251c\u2500\u2500 schema.json \u2514\u2500\u2500 src \u2514\u2500\u2500 mongo_runner.py mongo_runner.py contains: from dlpx.virtualization.platform import Plugin mongodb = Plugin () This is a valid plugin config for the plugin: id : 7cf830f2-82f3-4d5d-a63c-7bbe50c22b32 name : MongoDB version : 2.0.0 hostTypes : - UNIX entryPoint : mongo_runner:mongodb srcDir : src/ schemaFile : schema.json pluginType : DIRECT language : PYTHON27 buildNumber : 0.1.0 This is a valid plugin config for the plugin with manualDiscovery set to false and an externalVersion set: id : 7cf830f2-82f3-4d5d-a63c-7bbe50c22b32 name : MongoDB hostTypes : - UNIX entryPoint : mongo_runner:mongodb srcDir : src/ schemaFile : schema.json manualDiscovery : false pluginType : DIRECT language : PYTHON27 externalVersion : \"MongoDB 1.0\" buildNumber : \"1\"","title":"Plugin Config"},{"location":"References/Plugin_Config/#plugin-config","text":"The plugin config is a YAML file that marks the root of a plugin and defines metadata about the plugin and its structure. The config file is read at build time to generate the upload artifact. The name of the file can be specified during the build. By default, the build looks for plugin_config.yml in the current working directory.","title":"Plugin Config"},{"location":"References/Plugin_Config/#fields","text":"Field Name Required Type Description id Y string The unique id of the plugin in a valid UUID format. name N string The display name of the plugin. This will be used in the UI. If it is not specified name will be equal to id. externalVersion N string The plugin's external version . This is a freeform string. If it is not supplied, the build number is used as an external version. buildNumber Y string The plugin's build number . This string must conform to the format described here . hostTypes Y list The host type that the plugin supports. Either UNIX or WINDOWS . schemaFile Y string The path to the JSON file that contains the plugin's schema definitions . This path can be absolute or relative to the directory containing the plugin config file. srcDir Y string The path to the directory that contains the source code for the plugin. During execution of a plugin operation, this directory will be the current working directory of the Python interpreter. Any modules or resources defined outside of this directory will be inaccessible at runtime. This path can be absolute or relative to the directory containing the plugin config file. entryPoint Y string A fully qualified Python symbol that points to the dlpx.virtualization.platform.Plugin object that defines the plugin. It must be in the form importable.module:object_name where importable.module is in srcDir . manualDiscovery N boolean True if the plugin supports manual discovery of source config objects. The default value is true . pluginType Y enum The ingestion strategy of the plugin. Can be either STAGED or DIRECT . language Y enum Must be PYTHON27 . defaultLocale N enum The locale to be used by the plugin if the Delphix user does not specify one. Plugin messages will be displayed in this locale by default. The default value is en-us . rootSquashEnabled N boolean This dictates whether \"root squash\" is enabled on NFS mounts for the plugin (i.e. whether the root user on remote hosts has access to the NFS mounts). Setting this to false allows processes usually run as root , like Docker daemons, access to the NFS mounts. The default value is true . This field only applies to Unix hosts.","title":"Fields"},{"location":"References/Plugin_Config/#example","text":"Assume the following basic plugin structure: \u251c\u2500\u2500 plugin_config.yml \u251c\u2500\u2500 schema.json \u2514\u2500\u2500 src \u2514\u2500\u2500 mongo_runner.py mongo_runner.py contains: from dlpx.virtualization.platform import Plugin mongodb = Plugin () This is a valid plugin config for the plugin: id : 7cf830f2-82f3-4d5d-a63c-7bbe50c22b32 name : MongoDB version : 2.0.0 hostTypes : - UNIX entryPoint : mongo_runner:mongodb srcDir : src/ schemaFile : schema.json pluginType : DIRECT language : PYTHON27 buildNumber : 0.1.0 This is a valid plugin config for the plugin with manualDiscovery set to false and an externalVersion set: id : 7cf830f2-82f3-4d5d-a63c-7bbe50c22b32 name : MongoDB hostTypes : - UNIX entryPoint : mongo_runner:mongodb srcDir : src/ schemaFile : schema.json manualDiscovery : false pluginType : DIRECT language : PYTHON27 externalVersion : \"MongoDB 1.0\" buildNumber : \"1\"","title":"Example"},{"location":"References/Plugin_Operations/","text":"Plugin Operations \u00b6 Warning If a Plugin Operation is Required and is not present, the corresponding Delphix Engine Operation will fail when invoked. The plugin can still be built and uploaded to the Delphix Engine. Warning For each operation, the argument names must match exactly. For example, the Repository Discovery operation must have a single argument named source_connection . Plugin Operation Required Decorator Delphix Engine Operations Repository Discovery Yes discovery.repository() Environment Discovery Environment Refresh Source Config Discovery Yes discovery.source_config() Environment Discovery Environment Refresh Direct Linked Source Pre-Snapshot No linked.pre_snapshot() Linked Source Sync Direct Linked Source Post-Snapshot Yes linked.post_snapshot() Linked Source Sync Staged Linked Source Pre-Snapshot No linked.pre_snapshot() Linked Source Sync Staged Linked Source Post-Snapshot Yes linked.post_snapshot() Linked Source Sync Staged Linked Source Start-Staging No linked.start_staging() Linked Source Enable Staged Linked Source Stop-Staging No linked.stop_staging() Linked Source Disable Linked Source Delete Staged Linked Source Status No linked.status() N/A Staged Linked Source Worker No linked.worker() N/A Staged Linked Source Mount Specification Yes linked.mount_specification() Linked Source Sync Linked Source Enable Virtual Source Configure Yes virtual.configure() Virtual Source Provision Virtual Source Refresh Virtual Source Unconfigure No virtual.unconfigure() Virtual Source Refresh Virtual Source Delete Virtual Source Reconfigure Yes virtual.reconfigure() Virtual Source Rollback Virtual Source Enable Virtual Source Start No virtual.start() Virtual Source Start Virtual Source Stop No virtual.stop() Virtual Source Stop Virtual Source Pre-Snapshot No virtual.pre_snapshot() Virtual Source Snapshot Virtual Source Post-Snapshot Yes virtual.post_snapshot() Virtual Source Snapshot Virtual Source Mount Specification Yes virtual.mount_specification() Virtual Source Enable Virtual Source Provision Virtual Source Refresh Virtual Source Rollback Virtual Source Start Virtual Source Status No virtual.status() Virtual Source Enable Repository Data Migration No upgrade.repository(migration_id) Upgrade Source Config Data Migration No upgrade.source_config(migration_id) Upgrade Linked Source Data Migration No upgrade.linked_source(migration_id) Upgrade Virtual Source Data Migration No upgrade.virtual_source(migration_id) Upgrade Snapshot Data Migration No upgrade.snapshot(migration_id) Upgrade Repository Discovery \u00b6 Discovers the set of repositories for a plugin on an environment . For a DBMS, this can correspond to the set of binaries installed on a Unix host. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Environment Refresh Environment Discovery Signature \u00b6 def repository_discovery(source_connection) Decorator \u00b6 discovery.repository() Arguments \u00b6 Argument Type Description source_connection RemoteConnection The connection associated with the remote environment to run repository discovery Returns \u00b6 A list of RepositoryDefinition objects. Example \u00b6 from dlpx.virtualization.platform import Plugin from generated.defintions import RepositoryDefinition plugin = Plugin () @plugin . discovery . repository () def repository_discovery ( source_connection ): # Initialize the object, filling in all required fields repository = RepositoryDefinition ( installPath = \"/usr/bin/install\" ) # Set any additional non-required properties repository . version = \"1.2.3\" # Return one single repository return [ repository ] The above command assumes a Repository Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"installPath\" ], \"properties\" : { \"installPath\" : { \"type\" : \"string\" }, \"version\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"installPath\" ], \"nameField\" : [ \"installPath\" ] } Source Config Discovery \u00b6 Discovers the set of source configs for a plugin for a repository . For a DBMS, this can correspond to the set of unique databases running using a particular installation on a Unix host. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Environment Refresh Environment Discovery Signature \u00b6 def source_config_discovery(source_connection, repository) Decorator \u00b6 discovery.source_config() Arguments \u00b6 Argument Type Description source_connection RemoteConnection The connection to the remote environment the corresponds to the repository. repository RepositoryDefinition The repository to discover source configs for. Returns \u00b6 A list of SourceConfigDefinition objects. Example \u00b6 from dlpx.virtualization.platform import Plugin from generated.definitions import SourceConfigDefinition plugin = Plugin () @plugin . discovery . source_config () def source_config_discovery ( source_connection , repository ): source_config = SourceConfigDefinition ( name = \"my_name\" , port = 1000 ) return [ source_config ] The above command assumes a Source Config Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"name\" ], \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"number\" } }, \"identityFields\" : [ \"name\" ], \"nameField\" : [ \"name\" ] } Direct Linked Source Pre-Snapshot \u00b6 Sets up a dSource to ingest data. Only applies when using a Direct Linking strategy. Required / Optional \u00b6 Optional Delphix Engine Operations \u00b6 Linked Source Sync Signature \u00b6 def linked_pre_snapshot(direct_source, repository, source_config) Decorator \u00b6 linked.pre_snapshot() Arguments \u00b6 Argument Type Description direct_source DirectSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 None Example \u00b6 from dlpx.virtualization.platform import Plugin from generated.definitions import SourceConfigDefinition plugin = Plugin () @plugin . linked . pre_snapshot () def linked_pre_snapshot ( direct_source , repository , source_config ): pass Direct Linked Source Post-Snapshot \u00b6 Captures metadata from a dSource once data has been ingested. Only applies when using a Direct Linking strategy. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Linked Source Sync Signature \u00b6 def linked_post_snapshot(direct_source, repository, source_config) Decorator \u00b6 linked.post_snapshot() Arguments \u00b6 Argument Type Description direct_source DirectSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 SnapshotDefinition Example \u00b6 from dlpx.virtualization.platform import Plugin from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin . linked . post_snapshot () def linked_post_snapshot ( direct_source , repository , source_config ): snapshot = SnapshotDefinition () snapshot . transaction_id = 1000 return snapshot The above command assumes a Snapshot Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"transactionId\" : { \"type\" : \"integer\" } } } Staged Linked Source Pre-Snapshot \u00b6 Sets up a dSource to ingest data. Only applies when using a Staged Linking strategy. Required / Optional \u00b6 Optional. Delphix Engine Operations \u00b6 Linked Source Sync Signature \u00b6 def linked_pre_snapshot(staged_source, repository, source_config, snapshot_parameters) Decorator \u00b6 linked.pre_snapshot() Arguments \u00b6 Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. snapshot_parameters SnapshotParametersDefinition The snapshot parameters. Returns \u00b6 None Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . linked . pre_snapshot () def linked_pre_snapshot ( staged_source , repository , source_config , snapshot_parameters ): pass Staged Linked Source Post-Snapshot \u00b6 Captures metadata from a dSource once data has been ingested. Only applies when using a Staged Linking strategy. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Linked Source Sync Signature \u00b6 def linked_post_snapshot(staged_source, repository, source_config, snapshot_parameters) Decorator \u00b6 linked.post_snapshot() Arguments \u00b6 Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. snapshot_parameters SnapshotParametersDefinition The snapshot parameters. Returns \u00b6 SnapshotDefinition Example \u00b6 from dlpx.virtualization.platform import Plugin from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin . linked . post_snapshot () def linked_post_snapshot ( staged_source , repository , source_config , snapshot_parameters ): snapshot = SnapshotDefinition () if snapshot_parameters . resync : snapshot . transaction_id = 1000 else : snapshot . transaction_id = 10 return snapshot The above command assumes a Snapshot Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"transactionId\" : { \"type\" : \"integer\" } } } Staged Linked Source Start-Staging \u00b6 Sets up a Staging Source to ingest data. Only applies when using a Staged Linking strategy. Required to implement for Delphix Engine operations: Required / Optional \u00b6 Optional. Delphix Engine Operations \u00b6 Linked Source Enable Signature \u00b6 def start_staging(staged_source, repository, source_config) Decorator \u00b6 linked.start_staging() Arguments \u00b6 Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 None Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . linked . start_staging () def start_staging ( staged_source , repository , source_config ): pass Staged Linked Source Stop-Staging \u00b6 Quiesces a Staging Source to pause ingestion. Only applies when using a Staged Linking strategy. Required / Optional \u00b6 Optional. Delphix Engine Operations \u00b6 Linked Source Disable Linked Source Delete Signature \u00b6 def stop_staging(staged_source, repository, source_config) Decorator \u00b6 linked.stop_staging() Arguments \u00b6 Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 None Examples \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . linked . stop_staging () def stop_staging ( staged_source , repository , source_config ): pass Staged Linked Source Status \u00b6 Determines the status of a Staging Source to show end users whether it is healthy or not. Only applies when using a Staged Linking strategy. Required / Optional \u00b6 Optional. If not implemented, the platform assumes that the status is Status.ACTIVE Delphix Engine Operations \u00b6 N/A Signature \u00b6 def linked_status(staged_source, repository, source_config) Decorator \u00b6 linked.status() Arguments \u00b6 Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 Status Status.ACTIVE if the plugin operation is not implemented. Example \u00b6 from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform import Status plugin = Plugin () @plugin . linked . status () def linked_status ( staged_source , repository , source_config ): return Status . ACTIVE Staged Linked Source Worker \u00b6 Monitors the status of a Staging Source on a reqular interval. It can be used to fix up any errors on staging if it is not functioning as expected. Only applies when using a Staged Linking strategy. Required / Optional \u00b6 Optional. Delphix Engine Operations \u00b6 N/A Signature \u00b6 def worker(staged_source, repository, source_config) Decorator \u00b6 linked.worker() Arguments \u00b6 Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 None Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . linked . worker () def worker ( staged_source , repository , source_config ): pass Staged Linked Source Mount Specification \u00b6 Returns configurations for the mounts associated for data in staged source. The ownership_specification is optional. If not specified, the platform will default the ownership settings to the environment user used for the Delphix Operation. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Linked Source Sync Linked Source Enable Signature \u00b6 def linked_mount_specification(staged_source, repository) Decorator \u00b6 linked.mount_specification() Arguments \u00b6 Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. Returns \u00b6 MountSpecification Example \u00b6 Info ownership_specification only applies to Unix hosts. from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform import Mount from dlpx.virtualization.platform import MountSpecification from dlpx.virtualization.platform import OwenershipSpecification from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin . linked . mount_specification () def linked_mount_specification ( staged_source , repository ): mount = Mount ( staged_source . staged_connection . environment , \"/some/path\" ) ownership_spec = OwenershipSpecification ( repository . uid , repository . gid ) return MountSpecification ([ mount ], ownership_spec ) Virtual Source Configure \u00b6 Configures the data in a particular snapshot to be usable on a target environment. For database data files, this may mean recovering from a crash consistent format or backup. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Virtual Source Provision Virtual Source Refresh Signature \u00b6 def configure(virtual_source, snapshot, repository) Decorator \u00b6 virtual.configure() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. snapshot SnapshotDefinition The snapshot of the data set to configure. repository RepositoryDefinition The repository associated with this source. Returns \u00b6 SourceConfigDefinition Example \u00b6 from dlpx.virtualization.platform import Plugin from generated.defintions import SourceConfigDefinition plugin = Plugin () @plugin . virtual . configure () def configure ( virtual_source , repository , snapshot ): source_config = SourceConfigDefinition ( name = \"config_name\" ) return source_config The above command assumes a SourceConfig Schema defined as: { \"type\" : \"object\" , \"required\" : [ \"name\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"name\" ], \"nameField\" : [ \"name\" ] } Virtual Source Unconfigure \u00b6 Quiesces the virtual source on a target environment. For database data files, shutting down and unregistering a database on a host. Required / Optional \u00b6 Optional. Delphix Engine Operations \u00b6 Virtual Source Refresh Virtual Source Delete Signature \u00b6 def unconfigure(virtual_source, repository, source_config) Decorator \u00b6 virtual.unconfigure() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 None Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . virtual . unconfigure () def unconfigure ( virtual_source , repository , source_config ): pass Virtual Source Reconfigure \u00b6 Re-configures the data for a virtual source to point to the data in a prior snapshot for the virtual source. For database data files, this may mean recovering from a crash consistent format or backup of a new snapshot. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Virtual Source Rollback Virtual Source Enable Signature \u00b6 def reconfigure(virtual_source, repository, source_config, snapshot) Decorator \u00b6 virtual.reconfigure() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. snapshot SnapshotDefinition The snapshot of the data set to configure. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 SourceConfigDefinition Example \u00b6 from dlpx.virtualization.platform import Plugin from generated.definitions import SourceConfigDefinition plugin = Plugin () @plugin . virtual . reconfigure () def reconfigure ( virtual_source , repository , source_config , snapshot ): return SourceConfigDefinition ( name = \"updated_config_name\" ) The above command assumes a SourceConfig Schema defined as: { \"type\" : \"object\" , \"required\" : [ \"name\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"name\" ], \"nameField\" : [ \"name\" ] } Virtual Source Start \u00b6 Executed whenever the data should be placed in a \"running\" state. Required / Optional \u00b6 Optional. Delphix Engine Operations \u00b6 Virtual Source Start Signature \u00b6 def start(virtual_source, repository, source_config) Decorator \u00b6 virtual.start() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 None Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . virtual . start () def start ( virtual_source , repository , source_config ): pass Virtual Source Stop \u00b6 Executed whenever the data needs to be shut down. Required to implement for Delphix Engine operations: Required / Optional \u00b6 Optional. Delphix Engine Operations \u00b6 Virtual Source Stop Signature \u00b6 def stop(virtual_source, repository, source_config) Decorator \u00b6 virtual.stop() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 None Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . virtual . stop () def stop ( virtual_source , repository , source_config ): pass Virtual Source Pre-Snapshot \u00b6 Prepares the virtual source for taking a snapshot of the data. Required / Optional \u00b6 Optional. Delphix Engine Operations \u00b6 Virtual Source Snapshot Signature \u00b6 def virtual_pre_snapshot(virtual_source, repository, source_config) Decorator \u00b6 virtual.pre_snapshot() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 None Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . virtual . pre_snapshot () def virtual_pre_snapshot ( virtual_source , repository , source_config ): pass Virtual Source Post-Snapshot \u00b6 Captures metadata after a snapshot. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Virtual Source Snapshot Signature \u00b6 def virtual_post_snapshot(virtual_source, repository, source_config) Decorator \u00b6 virtual.post_snapshot() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 SnapshotDefinition Example \u00b6 from dlpx.virtualization.platform import Plugin from generated.defintions import SnapshotDefinition plugin = Plugin () @plugin . virtual . post_snapshot () def virtual_post_snapshot ( virtual_source , repository , source_config ): snapshot = SnapshotDefinition () snapshot . transaction_id = 1000 return snapshot The above command assumes a Snapshot Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"transactionId\" : { \"type\" : \"string\" } } } Virtual Source Mount Specification \u00b6 Returns configurations for the mounts associated for data in virtual source. The ownership_specification is optional. If not specified, the platform will default the ownership settings to the environment user used for the Delphix Operation. Required / Optional \u00b6 Required. Delphix Engine Operations \u00b6 Virtual Source Enable Virtual Source Provision Virtual Source Refresh Virtual Source Rollback Virtual Source Start Signature \u00b6 def virtual_mount_specification(virtual_source, repository) Decorator \u00b6 virtual.mount_specification() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. Returns \u00b6 MountSpecification Example \u00b6 Info ownership_specification only applies to Unix hosts. from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform import Mount from dlpx.virtualization.platform import MountSpecification from dlpx.virtualization.platform import OwenershipSpecification from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin . virtual . mount_specification () def virtual_mount_specification ( virtual_source , repository ): mount = Mount ( virtual_source . connection . environment , \"/some/path\" ) ownership_spec = OwenershipSpecification ( repository . uid , repository . gid ) return MountSpecification ([ mount ], ownership_spec ) Virtual Source Status \u00b6 Determines the status of a Virtual Source to show end users whether it is healthy or not. Required / Optional \u00b6 Optional. If not implemented, the platform assumes that the status is Status.ACTIVE . Delphix Engine Operations \u00b6 Virtual Source Enable Signature \u00b6 def virtual_status(virtual_source, repository, source_config) Decorator \u00b6 virtual.status() Arguments \u00b6 Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. Returns \u00b6 Status Status.ACTIVE if the plugin operation is not implemented. Example \u00b6 from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform import Status plugin = Plugin () @plugin . virtual . status () def virtual_status ( virtual_source , repository , source_config ): return Status . ACTIVE Repository Data Migration \u00b6 A Repository Data Migration migrates repository data from an older schema format to an updated schema format. Required / Optional \u00b6 Optional. Warning You must ensure that all repository data will match your updated repository schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more repository data migrations. Delphix Engine Operations \u00b6 Upgrade Signature \u00b6 def migrate_repository(old_repository) Decorator \u00b6 upgrade.repository(migration_id) Decorator Arguments \u00b6 Argument Type Description migration_id String The ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details here . Function Arguments \u00b6 Argument Type Description old_repository Dictionary The plugin-specific data associated with a repository, that conforms to the previous schema. Warning The function argument old_repository is a Python dictionary, where each property name appears exactly as described in the previous repository schema. This differs from non-upgrade-related operations, where the function arguments are autogenerated classes based on the schema. Returns \u00b6 Dictionary A migrated version of the old_repository input that must conform to the updated repository schema. Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . upgrade . repository ( \"2019.12.15\" ) def add_new_flag_to_repo ( old_repository ): new_repository = dict ( old_repository ) new_repository [ \"useNewFeature\" ] = False return new_repository Source Config Data Migration \u00b6 A Source Config Data Migration migrates source config data from an older schema format to an updated schema format. Required / Optional \u00b6 Optional. Warning You must ensure that all source config data will match your source config schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more source config data migrations. Delphix Engine Operations \u00b6 Upgrade Signature \u00b6 def migrate_source_config(old_source_config) Decorator \u00b6 upgrade.source_config(migration_id) Decorator Arguments \u00b6 Argument Type Description migration_id String The ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details here . Function Arguments \u00b6 Argument Type Description old_source_config Dictionary The plugin-specific data associated with a source config, that conforms to the previous schema. Warning The function argument old_source_config is a Python dictionary, where each property name appears exactly as described in the previous source config schema. This differs from non-upgrade-related operations, where the function arguments are autogenerated classes based on the schema. Returns \u00b6 Dictionary A migrated version of the old_source_config input that must conform to the updated source config schema. Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . upgrade . source_config ( \"2019.12.15\" ) def add_new_flag_to_source_config ( old_source_config ): new_source_config = dict ( old_source_config ) new_source_config [ \"useNewFeature\" ] = False return new_source_config Linked Source Data Migration \u00b6 A Linked Source Data Migration migrates linked source data from an older schema format to an updated schema format. Required / Optional \u00b6 Optional. Warning You must ensure that all linked source data will match your linked source schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more linked source data migrations. Delphix Engine Operations \u00b6 Upgrade Signature \u00b6 def migrate_linked_source(old_linked_source) Decorator \u00b6 upgrade.linked_source(migration_id) Decorator Arguments \u00b6 Argument Type Description migration_id String The ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details here . Function Arguments \u00b6 Argument Type Description old_linked_source Dictionary The plugin-specific data associated with a linked source, that conforms to the previous schema. Warning The function argument old_linked_source is a Python dictionary, where each property name appears exactly as described in the previous linked source schema. This differs from non-upgrade-related operations, where the function arguments are autogenerated classes based on the schema. Returns \u00b6 Dictionary A migrated version of the old_linked_source input that must conform to the updated linked source schema. Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . upgrade . linked_source ( \"2019.12.15\" ) def add_new_flag_to_dsource ( old_linked_source ): new_linked_source = dict ( old_linked_source ) new_linked_source [ \"useNewFeature\" ] = False return new_linked_source Virtual Source Data Migration \u00b6 A Virtual Source Data Migration migrates virtual source data from an older schema format to an updated schema format. Required / Optional \u00b6 Optional. Warning You must ensure that all virtual source data will match your virtual source schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more virtual source data migrations. Delphix Engine Operations \u00b6 Upgrade Signature \u00b6 def migrate_virtual_source(old_virtual_source) Decorator \u00b6 upgrade.virtual_source(migration_id) Decorator Arguments \u00b6 Argument Type Description migration_id String The ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details here . Function Arguments \u00b6 Argument Type Description old_virtual_source Dictionary The plugin-specific data associated with a virtual source, that conforms to the previous schema. Warning The function argument old_virtual_source is a Python dictionary, where each property name appears exactly as described in the previous virtual source schema. This differs from non-upgrade-related operations, where the function arguments are autogenerated classes based on the schema. Returns \u00b6 Dictionary A migrated version of the old_virtual_source input that must conform to the updated virtual source schema. Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . upgrade . virtual_source ( \"2019.12.15\" ) def add_new_flag_to_vdb ( old_virtual_source ): new_virtual_source = dict ( old_virtual_source ) new_virtual_source [ \"useNewFeature\" ] = False return new_virtual_source Snapshot Data Migration \u00b6 A Snapshot Data Migration migrates snapshot data from an older schema format to an updated schema format. Required / Optional \u00b6 Optional. Warning You must ensure that all snapshot data will match your snapshot schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more snapshot migrations. Delphix Engine Operations \u00b6 Upgrade Signature \u00b6 def migrate_snapshot(old_snapshot) Decorator \u00b6 upgrade.snapshot(migration_id) Decorator Arguments \u00b6 Argument Type Description migration_id String The ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details here . Function Arguments \u00b6 Argument Type Description old_snapshot Dictionary The plugin-specific data associated with a snapshot, that conforms to the previous schema. Warning The function argument old_snapshot is a Python dictionary, where each property name appears exactly as described in the previous snapshot schema. This differs from non-upgrade-related operations, where the function arguments are autogenerated classes based on the schema. Returns \u00b6 Dictionary A migrated version of the old_snapshot input that must conform to the updated snapshot schema. Example \u00b6 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . upgrade . snapshot ( \"2019.12.15\" ) def add_new_flag_to_snapshot ( old_snapshot ): new_snapshot = dict ( old_snapshot ) new_snapshot [ \"useNewFeature\" ] = False return new_snapshot","title":"Plugin Operations"},{"location":"References/Plugin_Operations/#plugin-operations","text":"Warning If a Plugin Operation is Required and is not present, the corresponding Delphix Engine Operation will fail when invoked. The plugin can still be built and uploaded to the Delphix Engine. Warning For each operation, the argument names must match exactly. For example, the Repository Discovery operation must have a single argument named source_connection . Plugin Operation Required Decorator Delphix Engine Operations Repository Discovery Yes discovery.repository() Environment Discovery Environment Refresh Source Config Discovery Yes discovery.source_config() Environment Discovery Environment Refresh Direct Linked Source Pre-Snapshot No linked.pre_snapshot() Linked Source Sync Direct Linked Source Post-Snapshot Yes linked.post_snapshot() Linked Source Sync Staged Linked Source Pre-Snapshot No linked.pre_snapshot() Linked Source Sync Staged Linked Source Post-Snapshot Yes linked.post_snapshot() Linked Source Sync Staged Linked Source Start-Staging No linked.start_staging() Linked Source Enable Staged Linked Source Stop-Staging No linked.stop_staging() Linked Source Disable Linked Source Delete Staged Linked Source Status No linked.status() N/A Staged Linked Source Worker No linked.worker() N/A Staged Linked Source Mount Specification Yes linked.mount_specification() Linked Source Sync Linked Source Enable Virtual Source Configure Yes virtual.configure() Virtual Source Provision Virtual Source Refresh Virtual Source Unconfigure No virtual.unconfigure() Virtual Source Refresh Virtual Source Delete Virtual Source Reconfigure Yes virtual.reconfigure() Virtual Source Rollback Virtual Source Enable Virtual Source Start No virtual.start() Virtual Source Start Virtual Source Stop No virtual.stop() Virtual Source Stop Virtual Source Pre-Snapshot No virtual.pre_snapshot() Virtual Source Snapshot Virtual Source Post-Snapshot Yes virtual.post_snapshot() Virtual Source Snapshot Virtual Source Mount Specification Yes virtual.mount_specification() Virtual Source Enable Virtual Source Provision Virtual Source Refresh Virtual Source Rollback Virtual Source Start Virtual Source Status No virtual.status() Virtual Source Enable Repository Data Migration No upgrade.repository(migration_id) Upgrade Source Config Data Migration No upgrade.source_config(migration_id) Upgrade Linked Source Data Migration No upgrade.linked_source(migration_id) Upgrade Virtual Source Data Migration No upgrade.virtual_source(migration_id) Upgrade Snapshot Data Migration No upgrade.snapshot(migration_id) Upgrade","title":"Plugin Operations"},{"location":"References/Plugin_Operations/#repository-discovery","text":"Discovers the set of repositories for a plugin on an environment . For a DBMS, this can correspond to the set of binaries installed on a Unix host.","title":"Repository Discovery"},{"location":"References/Plugin_Operations/#required-optional","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations","text":"Environment Refresh Environment Discovery","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature","text":"def repository_discovery(source_connection)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator","text":"discovery.repository()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments","text":"Argument Type Description source_connection RemoteConnection The connection associated with the remote environment to run repository discovery","title":"Arguments"},{"location":"References/Plugin_Operations/#returns","text":"A list of RepositoryDefinition objects.","title":"Returns"},{"location":"References/Plugin_Operations/#example","text":"from dlpx.virtualization.platform import Plugin from generated.defintions import RepositoryDefinition plugin = Plugin () @plugin . discovery . repository () def repository_discovery ( source_connection ): # Initialize the object, filling in all required fields repository = RepositoryDefinition ( installPath = \"/usr/bin/install\" ) # Set any additional non-required properties repository . version = \"1.2.3\" # Return one single repository return [ repository ] The above command assumes a Repository Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"installPath\" ], \"properties\" : { \"installPath\" : { \"type\" : \"string\" }, \"version\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"installPath\" ], \"nameField\" : [ \"installPath\" ] }","title":"Example"},{"location":"References/Plugin_Operations/#source-config-discovery","text":"Discovers the set of source configs for a plugin for a repository . For a DBMS, this can correspond to the set of unique databases running using a particular installation on a Unix host.","title":"Source Config Discovery"},{"location":"References/Plugin_Operations/#required-optional_1","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_1","text":"Environment Refresh Environment Discovery","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_1","text":"def source_config_discovery(source_connection, repository)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_1","text":"discovery.source_config()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_1","text":"Argument Type Description source_connection RemoteConnection The connection to the remote environment the corresponds to the repository. repository RepositoryDefinition The repository to discover source configs for.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_1","text":"A list of SourceConfigDefinition objects.","title":"Returns"},{"location":"References/Plugin_Operations/#example_1","text":"from dlpx.virtualization.platform import Plugin from generated.definitions import SourceConfigDefinition plugin = Plugin () @plugin . discovery . source_config () def source_config_discovery ( source_connection , repository ): source_config = SourceConfigDefinition ( name = \"my_name\" , port = 1000 ) return [ source_config ] The above command assumes a Source Config Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"name\" ], \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"number\" } }, \"identityFields\" : [ \"name\" ], \"nameField\" : [ \"name\" ] }","title":"Example"},{"location":"References/Plugin_Operations/#direct-linked-source-pre-snapshot","text":"Sets up a dSource to ingest data. Only applies when using a Direct Linking strategy.","title":"Direct Linked Source Pre-Snapshot"},{"location":"References/Plugin_Operations/#required-optional_2","text":"Optional","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_2","text":"Linked Source Sync","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_2","text":"def linked_pre_snapshot(direct_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_2","text":"linked.pre_snapshot()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_2","text":"Argument Type Description direct_source DirectSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_2","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#example_2","text":"from dlpx.virtualization.platform import Plugin from generated.definitions import SourceConfigDefinition plugin = Plugin () @plugin . linked . pre_snapshot () def linked_pre_snapshot ( direct_source , repository , source_config ): pass","title":"Example"},{"location":"References/Plugin_Operations/#direct-linked-source-post-snapshot","text":"Captures metadata from a dSource once data has been ingested. Only applies when using a Direct Linking strategy.","title":"Direct Linked Source Post-Snapshot"},{"location":"References/Plugin_Operations/#required-optional_3","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_3","text":"Linked Source Sync","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_3","text":"def linked_post_snapshot(direct_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_3","text":"linked.post_snapshot()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_3","text":"Argument Type Description direct_source DirectSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_3","text":"SnapshotDefinition","title":"Returns"},{"location":"References/Plugin_Operations/#example_3","text":"from dlpx.virtualization.platform import Plugin from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin . linked . post_snapshot () def linked_post_snapshot ( direct_source , repository , source_config ): snapshot = SnapshotDefinition () snapshot . transaction_id = 1000 return snapshot The above command assumes a Snapshot Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"transactionId\" : { \"type\" : \"integer\" } } }","title":"Example"},{"location":"References/Plugin_Operations/#staged-linked-source-pre-snapshot","text":"Sets up a dSource to ingest data. Only applies when using a Staged Linking strategy.","title":"Staged Linked Source Pre-Snapshot"},{"location":"References/Plugin_Operations/#required-optional_4","text":"Optional.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_4","text":"Linked Source Sync","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_4","text":"def linked_pre_snapshot(staged_source, repository, source_config, snapshot_parameters)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_4","text":"linked.pre_snapshot()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_4","text":"Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. snapshot_parameters SnapshotParametersDefinition The snapshot parameters.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_4","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#example_4","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . linked . pre_snapshot () def linked_pre_snapshot ( staged_source , repository , source_config , snapshot_parameters ): pass","title":"Example"},{"location":"References/Plugin_Operations/#staged-linked-source-post-snapshot","text":"Captures metadata from a dSource once data has been ingested. Only applies when using a Staged Linking strategy.","title":"Staged Linked Source Post-Snapshot"},{"location":"References/Plugin_Operations/#required-optional_5","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_5","text":"Linked Source Sync","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_5","text":"def linked_post_snapshot(staged_source, repository, source_config, snapshot_parameters)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_5","text":"linked.post_snapshot()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_5","text":"Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source. snapshot_parameters SnapshotParametersDefinition The snapshot parameters.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_5","text":"SnapshotDefinition","title":"Returns"},{"location":"References/Plugin_Operations/#example_5","text":"from dlpx.virtualization.platform import Plugin from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin . linked . post_snapshot () def linked_post_snapshot ( staged_source , repository , source_config , snapshot_parameters ): snapshot = SnapshotDefinition () if snapshot_parameters . resync : snapshot . transaction_id = 1000 else : snapshot . transaction_id = 10 return snapshot The above command assumes a Snapshot Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"transactionId\" : { \"type\" : \"integer\" } } }","title":"Example"},{"location":"References/Plugin_Operations/#staged-linked-source-start-staging","text":"Sets up a Staging Source to ingest data. Only applies when using a Staged Linking strategy. Required to implement for Delphix Engine operations:","title":"Staged Linked Source Start-Staging"},{"location":"References/Plugin_Operations/#required-optional_6","text":"Optional.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_6","text":"Linked Source Enable","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_6","text":"def start_staging(staged_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_6","text":"linked.start_staging()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_6","text":"Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_6","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#example_6","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . linked . start_staging () def start_staging ( staged_source , repository , source_config ): pass","title":"Example"},{"location":"References/Plugin_Operations/#staged-linked-source-stop-staging","text":"Quiesces a Staging Source to pause ingestion. Only applies when using a Staged Linking strategy.","title":"Staged Linked Source Stop-Staging"},{"location":"References/Plugin_Operations/#required-optional_7","text":"Optional.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_7","text":"Linked Source Disable Linked Source Delete","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_7","text":"def stop_staging(staged_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_7","text":"linked.stop_staging()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_7","text":"Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_7","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#examples","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . linked . stop_staging () def stop_staging ( staged_source , repository , source_config ): pass","title":"Examples"},{"location":"References/Plugin_Operations/#staged-linked-source-status","text":"Determines the status of a Staging Source to show end users whether it is healthy or not. Only applies when using a Staged Linking strategy.","title":"Staged Linked Source Status"},{"location":"References/Plugin_Operations/#required-optional_8","text":"Optional. If not implemented, the platform assumes that the status is Status.ACTIVE","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_8","text":"N/A","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_8","text":"def linked_status(staged_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_8","text":"linked.status()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_8","text":"Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_8","text":"Status Status.ACTIVE if the plugin operation is not implemented.","title":"Returns"},{"location":"References/Plugin_Operations/#example_7","text":"from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform import Status plugin = Plugin () @plugin . linked . status () def linked_status ( staged_source , repository , source_config ): return Status . ACTIVE","title":"Example"},{"location":"References/Plugin_Operations/#staged-linked-source-worker","text":"Monitors the status of a Staging Source on a reqular interval. It can be used to fix up any errors on staging if it is not functioning as expected. Only applies when using a Staged Linking strategy.","title":"Staged Linked Source Worker"},{"location":"References/Plugin_Operations/#required-optional_9","text":"Optional.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_9","text":"N/A","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_9","text":"def worker(staged_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_9","text":"linked.worker()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_9","text":"Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_9","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#example_8","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . linked . worker () def worker ( staged_source , repository , source_config ): pass","title":"Example"},{"location":"References/Plugin_Operations/#staged-linked-source-mount-specification","text":"Returns configurations for the mounts associated for data in staged source. The ownership_specification is optional. If not specified, the platform will default the ownership settings to the environment user used for the Delphix Operation.","title":"Staged Linked Source Mount Specification"},{"location":"References/Plugin_Operations/#required-optional_10","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_10","text":"Linked Source Sync Linked Source Enable","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_10","text":"def linked_mount_specification(staged_source, repository)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_10","text":"linked.mount_specification()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_10","text":"Argument Type Description staged_source StagedSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_10","text":"MountSpecification","title":"Returns"},{"location":"References/Plugin_Operations/#example_9","text":"Info ownership_specification only applies to Unix hosts. from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform import Mount from dlpx.virtualization.platform import MountSpecification from dlpx.virtualization.platform import OwenershipSpecification from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin . linked . mount_specification () def linked_mount_specification ( staged_source , repository ): mount = Mount ( staged_source . staged_connection . environment , \"/some/path\" ) ownership_spec = OwenershipSpecification ( repository . uid , repository . gid ) return MountSpecification ([ mount ], ownership_spec )","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-configure","text":"Configures the data in a particular snapshot to be usable on a target environment. For database data files, this may mean recovering from a crash consistent format or backup. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks.","title":"Virtual Source Configure"},{"location":"References/Plugin_Operations/#required-optional_11","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_11","text":"Virtual Source Provision Virtual Source Refresh","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_11","text":"def configure(virtual_source, snapshot, repository)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_11","text":"virtual.configure()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_11","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. snapshot SnapshotDefinition The snapshot of the data set to configure. repository RepositoryDefinition The repository associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_11","text":"SourceConfigDefinition","title":"Returns"},{"location":"References/Plugin_Operations/#example_10","text":"from dlpx.virtualization.platform import Plugin from generated.defintions import SourceConfigDefinition plugin = Plugin () @plugin . virtual . configure () def configure ( virtual_source , repository , snapshot ): source_config = SourceConfigDefinition ( name = \"config_name\" ) return source_config The above command assumes a SourceConfig Schema defined as: { \"type\" : \"object\" , \"required\" : [ \"name\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"name\" ], \"nameField\" : [ \"name\" ] }","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-unconfigure","text":"Quiesces the virtual source on a target environment. For database data files, shutting down and unregistering a database on a host.","title":"Virtual Source Unconfigure"},{"location":"References/Plugin_Operations/#required-optional_12","text":"Optional.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_12","text":"Virtual Source Refresh Virtual Source Delete","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_12","text":"def unconfigure(virtual_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_12","text":"virtual.unconfigure()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_12","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_12","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#example_11","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . virtual . unconfigure () def unconfigure ( virtual_source , repository , source_config ): pass","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-reconfigure","text":"Re-configures the data for a virtual source to point to the data in a prior snapshot for the virtual source. For database data files, this may mean recovering from a crash consistent format or backup of a new snapshot. For application files, this may mean reconfiguring XML files or rewriting hostnames and symlinks.","title":"Virtual Source Reconfigure"},{"location":"References/Plugin_Operations/#required-optional_13","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_13","text":"Virtual Source Rollback Virtual Source Enable","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_13","text":"def reconfigure(virtual_source, repository, source_config, snapshot)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_13","text":"virtual.reconfigure()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_13","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. snapshot SnapshotDefinition The snapshot of the data set to configure. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_13","text":"SourceConfigDefinition","title":"Returns"},{"location":"References/Plugin_Operations/#example_12","text":"from dlpx.virtualization.platform import Plugin from generated.definitions import SourceConfigDefinition plugin = Plugin () @plugin . virtual . reconfigure () def reconfigure ( virtual_source , repository , source_config , snapshot ): return SourceConfigDefinition ( name = \"updated_config_name\" ) The above command assumes a SourceConfig Schema defined as: { \"type\" : \"object\" , \"required\" : [ \"name\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"name\" ], \"nameField\" : [ \"name\" ] }","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-start","text":"Executed whenever the data should be placed in a \"running\" state.","title":"Virtual Source Start"},{"location":"References/Plugin_Operations/#required-optional_14","text":"Optional.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_14","text":"Virtual Source Start","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_14","text":"def start(virtual_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_14","text":"virtual.start()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_14","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_14","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#example_13","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . virtual . start () def start ( virtual_source , repository , source_config ): pass","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-stop","text":"Executed whenever the data needs to be shut down. Required to implement for Delphix Engine operations:","title":"Virtual Source Stop"},{"location":"References/Plugin_Operations/#required-optional_15","text":"Optional.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_15","text":"Virtual Source Stop","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_15","text":"def stop(virtual_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_15","text":"virtual.stop()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_15","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_15","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#example_14","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . virtual . stop () def stop ( virtual_source , repository , source_config ): pass","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-pre-snapshot","text":"Prepares the virtual source for taking a snapshot of the data.","title":"Virtual Source Pre-Snapshot"},{"location":"References/Plugin_Operations/#required-optional_16","text":"Optional.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_16","text":"Virtual Source Snapshot","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_16","text":"def virtual_pre_snapshot(virtual_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_16","text":"virtual.pre_snapshot()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_16","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_16","text":"None","title":"Returns"},{"location":"References/Plugin_Operations/#example_15","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . virtual . pre_snapshot () def virtual_pre_snapshot ( virtual_source , repository , source_config ): pass","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-post-snapshot","text":"Captures metadata after a snapshot.","title":"Virtual Source Post-Snapshot"},{"location":"References/Plugin_Operations/#required-optional_17","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_17","text":"Virtual Source Snapshot","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_17","text":"def virtual_post_snapshot(virtual_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_17","text":"virtual.post_snapshot()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_17","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_17","text":"SnapshotDefinition","title":"Returns"},{"location":"References/Plugin_Operations/#example_16","text":"from dlpx.virtualization.platform import Plugin from generated.defintions import SnapshotDefinition plugin = Plugin () @plugin . virtual . post_snapshot () def virtual_post_snapshot ( virtual_source , repository , source_config ): snapshot = SnapshotDefinition () snapshot . transaction_id = 1000 return snapshot The above command assumes a Snapshot Schema defined as: { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"transactionId\" : { \"type\" : \"string\" } } }","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-mount-specification","text":"Returns configurations for the mounts associated for data in virtual source. The ownership_specification is optional. If not specified, the platform will default the ownership settings to the environment user used for the Delphix Operation.","title":"Virtual Source Mount Specification"},{"location":"References/Plugin_Operations/#required-optional_18","text":"Required.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_18","text":"Virtual Source Enable Virtual Source Provision Virtual Source Refresh Virtual Source Rollback Virtual Source Start","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_18","text":"def virtual_mount_specification(virtual_source, repository)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_18","text":"virtual.mount_specification()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_18","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_18","text":"MountSpecification","title":"Returns"},{"location":"References/Plugin_Operations/#example_17","text":"Info ownership_specification only applies to Unix hosts. from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform import Mount from dlpx.virtualization.platform import MountSpecification from dlpx.virtualization.platform import OwenershipSpecification from generated.definitions import SnapshotDefinition plugin = Plugin () @plugin . virtual . mount_specification () def virtual_mount_specification ( virtual_source , repository ): mount = Mount ( virtual_source . connection . environment , \"/some/path\" ) ownership_spec = OwenershipSpecification ( repository . uid , repository . gid ) return MountSpecification ([ mount ], ownership_spec )","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-status","text":"Determines the status of a Virtual Source to show end users whether it is healthy or not.","title":"Virtual Source Status"},{"location":"References/Plugin_Operations/#required-optional_19","text":"Optional. If not implemented, the platform assumes that the status is Status.ACTIVE .","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_19","text":"Virtual Source Enable","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_19","text":"def virtual_status(virtual_source, repository, source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_19","text":"virtual.status()","title":"Decorator"},{"location":"References/Plugin_Operations/#arguments_19","text":"Argument Type Description virtual_source VirtualSource The source associated with this operation. repository RepositoryDefinition The repository associated with this source. source_config SourceConfigDefinition The source config associated with this source.","title":"Arguments"},{"location":"References/Plugin_Operations/#returns_19","text":"Status Status.ACTIVE if the plugin operation is not implemented.","title":"Returns"},{"location":"References/Plugin_Operations/#example_18","text":"from dlpx.virtualization.platform import Plugin from dlpx.virtualization.platform import Status plugin = Plugin () @plugin . virtual . status () def virtual_status ( virtual_source , repository , source_config ): return Status . ACTIVE","title":"Example"},{"location":"References/Plugin_Operations/#repository-data-migration","text":"A Repository Data Migration migrates repository data from an older schema format to an updated schema format.","title":"Repository Data Migration"},{"location":"References/Plugin_Operations/#required-optional_20","text":"Optional. Warning You must ensure that all repository data will match your updated repository schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more repository data migrations.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_20","text":"Upgrade","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_20","text":"def migrate_repository(old_repository)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_20","text":"upgrade.repository(migration_id)","title":"Decorator"},{"location":"References/Plugin_Operations/#decorator-arguments","text":"Argument Type Description migration_id String The ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details here .","title":"Decorator Arguments"},{"location":"References/Plugin_Operations/#function-arguments","text":"Argument Type Description old_repository Dictionary The plugin-specific data associated with a repository, that conforms to the previous schema. Warning The function argument old_repository is a Python dictionary, where each property name appears exactly as described in the previous repository schema. This differs from non-upgrade-related operations, where the function arguments are autogenerated classes based on the schema.","title":"Function Arguments"},{"location":"References/Plugin_Operations/#returns_20","text":"Dictionary A migrated version of the old_repository input that must conform to the updated repository schema.","title":"Returns"},{"location":"References/Plugin_Operations/#example_19","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . upgrade . repository ( \"2019.12.15\" ) def add_new_flag_to_repo ( old_repository ): new_repository = dict ( old_repository ) new_repository [ \"useNewFeature\" ] = False return new_repository","title":"Example"},{"location":"References/Plugin_Operations/#source-config-data-migration","text":"A Source Config Data Migration migrates source config data from an older schema format to an updated schema format.","title":"Source Config Data Migration"},{"location":"References/Plugin_Operations/#required-optional_21","text":"Optional. Warning You must ensure that all source config data will match your source config schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more source config data migrations.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_21","text":"Upgrade","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_21","text":"def migrate_source_config(old_source_config)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_21","text":"upgrade.source_config(migration_id)","title":"Decorator"},{"location":"References/Plugin_Operations/#decorator-arguments_1","text":"Argument Type Description migration_id String The ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details here .","title":"Decorator Arguments"},{"location":"References/Plugin_Operations/#function-arguments_1","text":"Argument Type Description old_source_config Dictionary The plugin-specific data associated with a source config, that conforms to the previous schema. Warning The function argument old_source_config is a Python dictionary, where each property name appears exactly as described in the previous source config schema. This differs from non-upgrade-related operations, where the function arguments are autogenerated classes based on the schema.","title":"Function Arguments"},{"location":"References/Plugin_Operations/#returns_21","text":"Dictionary A migrated version of the old_source_config input that must conform to the updated source config schema.","title":"Returns"},{"location":"References/Plugin_Operations/#example_20","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . upgrade . source_config ( \"2019.12.15\" ) def add_new_flag_to_source_config ( old_source_config ): new_source_config = dict ( old_source_config ) new_source_config [ \"useNewFeature\" ] = False return new_source_config","title":"Example"},{"location":"References/Plugin_Operations/#linked-source-data-migration","text":"A Linked Source Data Migration migrates linked source data from an older schema format to an updated schema format.","title":"Linked Source Data Migration"},{"location":"References/Plugin_Operations/#required-optional_22","text":"Optional. Warning You must ensure that all linked source data will match your linked source schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more linked source data migrations.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_22","text":"Upgrade","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_22","text":"def migrate_linked_source(old_linked_source)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_22","text":"upgrade.linked_source(migration_id)","title":"Decorator"},{"location":"References/Plugin_Operations/#decorator-arguments_2","text":"Argument Type Description migration_id String The ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details here .","title":"Decorator Arguments"},{"location":"References/Plugin_Operations/#function-arguments_2","text":"Argument Type Description old_linked_source Dictionary The plugin-specific data associated with a linked source, that conforms to the previous schema. Warning The function argument old_linked_source is a Python dictionary, where each property name appears exactly as described in the previous linked source schema. This differs from non-upgrade-related operations, where the function arguments are autogenerated classes based on the schema.","title":"Function Arguments"},{"location":"References/Plugin_Operations/#returns_22","text":"Dictionary A migrated version of the old_linked_source input that must conform to the updated linked source schema.","title":"Returns"},{"location":"References/Plugin_Operations/#example_21","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . upgrade . linked_source ( \"2019.12.15\" ) def add_new_flag_to_dsource ( old_linked_source ): new_linked_source = dict ( old_linked_source ) new_linked_source [ \"useNewFeature\" ] = False return new_linked_source","title":"Example"},{"location":"References/Plugin_Operations/#virtual-source-data-migration","text":"A Virtual Source Data Migration migrates virtual source data from an older schema format to an updated schema format.","title":"Virtual Source Data Migration"},{"location":"References/Plugin_Operations/#required-optional_23","text":"Optional. Warning You must ensure that all virtual source data will match your virtual source schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more virtual source data migrations.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_23","text":"Upgrade","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_23","text":"def migrate_virtual_source(old_virtual_source)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_23","text":"upgrade.virtual_source(migration_id)","title":"Decorator"},{"location":"References/Plugin_Operations/#decorator-arguments_3","text":"Argument Type Description migration_id String The ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details here .","title":"Decorator Arguments"},{"location":"References/Plugin_Operations/#function-arguments_3","text":"Argument Type Description old_virtual_source Dictionary The plugin-specific data associated with a virtual source, that conforms to the previous schema. Warning The function argument old_virtual_source is a Python dictionary, where each property name appears exactly as described in the previous virtual source schema. This differs from non-upgrade-related operations, where the function arguments are autogenerated classes based on the schema.","title":"Function Arguments"},{"location":"References/Plugin_Operations/#returns_23","text":"Dictionary A migrated version of the old_virtual_source input that must conform to the updated virtual source schema.","title":"Returns"},{"location":"References/Plugin_Operations/#example_22","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . upgrade . virtual_source ( \"2019.12.15\" ) def add_new_flag_to_vdb ( old_virtual_source ): new_virtual_source = dict ( old_virtual_source ) new_virtual_source [ \"useNewFeature\" ] = False return new_virtual_source","title":"Example"},{"location":"References/Plugin_Operations/#snapshot-data-migration","text":"A Snapshot Data Migration migrates snapshot data from an older schema format to an updated schema format.","title":"Snapshot Data Migration"},{"location":"References/Plugin_Operations/#required-optional_24","text":"Optional. Warning You must ensure that all snapshot data will match your snapshot schema after an upgrade operation. Depending on how your schema has changed, this might imply that you need to write one or more snapshot migrations.","title":"Required / Optional"},{"location":"References/Plugin_Operations/#delphix-engine-operations_24","text":"Upgrade","title":"Delphix Engine Operations"},{"location":"References/Plugin_Operations/#signature_24","text":"def migrate_snapshot(old_snapshot)","title":"Signature"},{"location":"References/Plugin_Operations/#decorator_24","text":"upgrade.snapshot(migration_id)","title":"Decorator"},{"location":"References/Plugin_Operations/#decorator-arguments_4","text":"Argument Type Description migration_id String The ID of this migration. An ID is a string containing one or more positive integers separated by periods. Each ID must be unique. More details here .","title":"Decorator Arguments"},{"location":"References/Plugin_Operations/#function-arguments_4","text":"Argument Type Description old_snapshot Dictionary The plugin-specific data associated with a snapshot, that conforms to the previous schema. Warning The function argument old_snapshot is a Python dictionary, where each property name appears exactly as described in the previous snapshot schema. This differs from non-upgrade-related operations, where the function arguments are autogenerated classes based on the schema.","title":"Function Arguments"},{"location":"References/Plugin_Operations/#returns_24","text":"Dictionary A migrated version of the old_snapshot input that must conform to the updated snapshot schema.","title":"Returns"},{"location":"References/Plugin_Operations/#example_23","text":"from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . upgrade . snapshot ( \"2019.12.15\" ) def add_new_flag_to_snapshot ( old_snapshot ): new_snapshot = dict ( old_snapshot ) new_snapshot [ \"useNewFeature\" ] = False return new_snapshot","title":"Example"},{"location":"References/Schemas/","text":"Schemas \u00b6 About Schemas \u00b6 Any time a plugin needs to store its own data, or needs to ask the user for data, the Delphix Engine needs to be told about the format of that data: What is the set of data needed and what should they be called? What is the type of each piece of data: Strings? Integers? Booleans? Plugins use schemas to describe the format of such data. Once a schema is defined, it is used in three ways It tells the Delphix Engine how to store the data for later use. It is used to autogenerate a custom user interface, and to validate user inputs. It is used to autogenerate Python classes that can be used by plugin code to access and manipulate user input and stored data. There are five plugin-customizable data formats: Delphix Object Schema Autogenerated Class Repository RepositoryDefinition RepositoryDefinition Source Config SourceConfigDefinition SourceConfigDefinition Linked Source LinkedSourceDefinition LinkedSourceDefinition Virtual Source VirtualSourceDefinition VirtualSourceDefinition Snapshot SnapshotDefinition SnapshotDefinition JSON Schemas \u00b6 Plugins use JSON schemas for their custom datatypes. There are three main things to understand about them, which are explained just below: What is JSON? What is a JSON schema? How has Delphix augmented JSON schemas? JSON \u00b6 JSON stands for \"Javascript Object Notation\". JSON is a data-interchange format that is intended to be precise and also somewhat human-readable. Here are some simple examples of data in JSON format: JSON Description \"hello\" A string. Note the double quotes. 17 An integer true A boolean {\"name\": \"Julie\", \"age\": 37} A JSON object with two fields, name (a string), and age (an integer). Objects are denoted with curly braces. [ true, false, true] A JSON array with three booleans. Arrays are denoted with square brackets. For more details on JSON, please see https://www.json.org/ . JSON Schemas \u00b6 The \"JSON schema\" format is built on top of JSON. This adds some special rules and keywords that are intended to facilitate the description of the format of data (whereas \"raw\" JSON is intended for storing data). Here is an example of a JSON schema that defines a (simplified) US address: { \"type\" : \"object\" , \"required\" : [ \"name\" , \"streetNumber\" , \"street\" , \"city\" , \"state\" , \"zip5\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"streetNumber\" : { \"type\" : \"string\" }, \"street\" : { \"type\" : \"string\" }, \"unit\" : { \"type\" : \"string\" }, \"city\" : { \"type\" : \"string\" , \"pattern\" : \"^[A-Z][A-Za-z ]*$\" }, \"state\" : { \"type\" : \"string\" , \"pattern\" : \"^[A-Z]{2}$\" }, \"zip5\" : { \"type\" : \"string\" , \"pattern\" : \"^[0-9]{5}\" }, \"zipPlus4\" : { \"type\" : \"string\" , \"pattern\" : \"^[0-9]{4}\" } } } Note that this is perfectly valid JSON data. It's a JSON object with four fields: type (a JSON string), required (A JSON array), additionalProperties (a JSON boolean), and properties . properties , in turn is a JSON object with with 8 fields, each of which is a JSON object, with its own properties, etc. But, this isn't just a JSON object. This is a JSON schema. It uses special keywords like type required , and additionalProperties . These have specially-defined meanings in the context of JSON schemas. Here is a list of the special keywords used by the above schema. Note that this is only a small subset of JSON schema keywords. keyword description additionalProperties Determines whether the schema allows properties that are not explicitly listed in the properties specification. Must be a true or false . pattern Used with string types to specify a regular expression that the property must conform to. required A list of required properties. Properties not listed in this list are optional. string Used with type to declare that a property must be a string. type Specifies a datatype. Common values are object , array , number , integer , boolean , and string . Some points to note about the address schema above: Because of the required list, all valid addresses must have fields called name , streetNumber and so on. unit and zipPlus4 do not appear in the required list, and therefore are optional. Because of additionalProperties being false , valid addresses cannot make up their own fields like nickname or doorbellLocation . Because of the pattern , any state field in a valid address must consist of exactly two capital letters. Similarly, city must only contain letters and spaces, and zip and zipPlus4 must only contain digits. Each property has its own valid subschema that describes its own type definition. Here is a JSON object that conforms to the above schema: { \"name\" : \"Delphix\" , \"streetNumber\" : \"220\" , \"street\" : \"Congress St.\" , \"unit\" : \"200\" , \"city\" : \"Boston\" , \"state\" : \"MA\" , \"zip\" : \"02210\" } Info A common point of confusion is the distinction between a JSON schema and a JSON object that conforms to a schema. Remember, a schema describes the form of data. In our example, the schema describes what an address looks like. The address itself is not a schema. For much more detail on JSON schemas, including which keywords are available, what they mean, and where you can use them, see https://json-schema.org/understanding-json-schema/ . Delphix-specific Extensions to JSON Schema \u00b6 The JSON schema vocabulary is designed to be extensible for special uses, and Delphix has taken advantage of this to add some new Delphix-specific keywords. The list below outlines each of these keywords, and provides minimal examples of how they might be used. description \u00b6 Summary Required or Optional? Optional Where? In any property subschema, at the same level as type . The description keyword can optionally appear on any property. If it does appear, it is used by the UI as explanatory text for the UI widget associated with the property. If it does not appear, then no explanatory text is shown. In this example, the UI would show \"User-readable name for the provisioned database\" in small text under the widget. { \"properties\" : { \"name\" : { \"type\" : \"string\" , \"description\" : \"User-readable name for the provisioned database\" } } } identityFields \u00b6 Summary Required or Optional? Required (for repository and source config schemas only) Where? At the top level of a repository or source config schema, at the same level as type and properties . The identityFields is a list of property names that, together, serve as a unique identifier for a repository or source config. When a plugin's automatic discovery code is called, it will return a list of repositories (or source configs). The Delphix Engine needs to be able to compare this new list with whatever repositories it already knows about. For example, suppose the engine already knows about a single repository with data {\"dbname\": \"my_databsae\", \"path\": \"/var/db/db01\"} (note the misspelling!). And, then suppose that automatic discovery is re-run and it returns repository data { \"dbname\": \"my_database\", \"path\": \"/var/db/db01\"} . What should the Delphix Engine do? Should it conclude that \"my_databsae\" has been deleted, and there is a completely new repository named \"my_database\"? Or, should it conclude that we still have the same old repository, but with an updated name? identityFields is used to handle this. When the engine compares \"new\" data with \"old\" data, it concludes that they belong to the same repository if all of the identity fields match. If any of the identity fields do not match, then the \"new\" repository data is judged to represent a different repository than the old data. identityFields is required for RepositoryDefinition and SourceConfigDefinition schemas, and may not be used in any other schemas. In this example, we'll tell the Delphix Engine that path is the sole unique identifier. { \"properties\" : { \"dbname\" : { \"type\" : \"string\" }, \"path\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"path\" ] } nameField \u00b6 Summary Required or Optional? Required (for repository and source config schemas only) Where? At the top level of a repository or source config schema, at the same level as type and properties . The nameField keyword specifies a single property that is to be used to name the object in the Delphix Engine. The property must be a string field. This keyword is used at the same level as properties . It is required for RepositoryDefinition and SourceConfigDefinition schemas, and may not be used in any other schemas. In this example, we will use the path property as the user-visible name. { \"properties\" : { \"path\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"integer\" } }, \"nameField\" : \"path\" } So, if we have an repository object that looks like { \"path\" : \"/usr/bin\" , \"port\" : 8800 } then the user will be able to refer to this object as /usr/bin . ordering \u00b6 Summary Required or Optional? Optional Where? At the top level, same level as type and properties . The ordering keyword can be used to order the fields when the UI is autogenerated. { \"properties\" : { \"path\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"integer\" } }, \"ordering\" : [ \"port\" , \"path\" ] } In the example above, the port will be the first field in the autogenerated UI wizard followed by path . password \u00b6 Summary Required or Optional? Optional Where? As the value for the format keyword in any string property's subschema. The password keyword can be used to specify the format of a string . (Note that format is a standard keyword and is not Delphix-specific). If a property is tagged as a password, then the UI will never show the value on screen, and the value will be encrypted before being stored as described here . In this example, the dbPass field on any object will be treated as a password. { \"properties\" : { \"dbPass\" : { \"type\" : \"string\" , \"format\" : \"password\" } } } prettyName \u00b6 Summary Required or Optional? Optional Where? In any property subschema, at the same level as type . The prettyName keyword can optionally appear on any property. If it does appear, it is used by the UI as a title for the UI widget associated with the property. If it does not appear, then the name of the property is used. In this example, the user would see \"Name of Database\" on the UI, instead of just \"name\". { \"properties\" : { \"name\" : { \"type\" : \"string\" , \"prettyName\" : \"Name of Database\" } } } unixpath \u00b6 Summary Required or Optional? Optional Where? As the value for the format keyword in any string property's subschema. The unixpath keyword is used to specify the format of a string. This will allow the Delphix Engine to verify and enforce that a particular field can be parsed as a valid Unix path. { \"properties\" : { \"datapath\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" } } } reference \u00b6 Summary Required or Optional? Optional Where? As the value for the format keyword in any string property's subschema. The reference keyword is used to specify the format of a string. This will allow the plugin author to ask the user to select environments and environment users on the Delphix Engine. \"properties\" : { \"env\" : { \"type\" : \"string\" , \"format\" : \"reference\" , \"referenceType\" : \"UNIX_HOST_ENVIRONMENT\" }, \"envUser\" : { \"type\" : \"string\" , \"format\" : \"reference\" , \"referenceType\" : \"HOST_USER\" , \"matches\u201d: \" env\" } } referenceType \u00b6 Summary Required or Optional? Optional Where? In any property subschema of type string and format reference , at the same level as type. The referenceType keyword is used to specify the reference type. Possible values: Environment : UNIX_HOST_ENVIRONMENT Environment User : HOST_USER \"properties\" : { \"env\" : { \"type\" : \"string\" , \"format\" : \"reference\" , \"referenceType\" : \"UNIX_HOST_ENVIRONMENT\" }, \"envUser\" : { \"type\" : \"string\" , \"format\" : \"reference\" , \"referenceType\" : \"HOST_USER\" , \"matches\u201d: \" env\" } } matches \u00b6 Summary Required or Optional? Optional Where? In any property subschema of type string and format reference , at the same level as type. The matches keyword is used to map an environment user to an environment by specifying the environment's property name. \"properties\" : { \"env\" : { \"type\" : \"string\" , \"format\" : \"reference\" , \"referenceType\" : \"UNIX_HOST_ENVIRONMENT\" }, \"envUser\" : { \"type\" : \"string\" , \"format\" : \"reference\" , \"referenceType\" : \"HOST_USER\" , \"matches\u201d: \" env\" } } In the example above, environment user envUser maps to environment env . JSON Schema Limitations \u00b6 To be able to autogenerate Python classes there are some restrictions to the JSON Schemas that are supported. Generation Error \u00b6 There are some valid JSON schemas that will cause the property to not be generated in the autogenerated Python classes. Unfortunately the build command will silently fail so be sure to look at the generated classes and verify all the properties exist. Multiple types \u00b6 For the type keyword, only a single type may be specified. Arrays of types are not supported. { \"repositoryDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : \"false\" , \"properties\" : { \"data\" : { \"type\" : [ \"integer\" , \"string\" ] } }, \"nameField\" : \"data\" , \"identityFields\" : [ \"data\" ] } } The data property will not even exist: from generated.defintions import RepositoryDefinition repository = RepositoryDefinition () repository . data = 3 print ( repository ) This would print: {} Combining schemas \u00b6 For the following keywords, if they are specified the property will not exist in the class. * anyOf * allOf * oneOf * not { \"repositoryDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : \"false\" , \"properties\" : { \"any\" : { \"anyOf\" : [ { \"type\" : \"integer\" , \"minimum\" : 2 }, { \"type\" : \"string\" , \"minLength\" : 4 } ] }, \"one\" : { \"oneOf\" : [ { \"type\" : \"integer\" , \"minimum\" : 3 }, { \"type\" : \"integer\" , \"maximum\" : 5 } ] } }, \"nameField\" : \"data\" , \"identityFields\" : [ \"data\" ] } } The any and one properties would not exist: from generated.defintions import RepositoryDefinition repository = RepositoryDefinition () repository . any = \"string\" repository . one = 6 print ( repository ) This would print: {} Object Additional Properties \u00b6 The additionalProperties keyword inside the object property can either be a boolean or a JSON schema. If it is a schema it needs to have the keyword type . If the additionalProperties is set to a JSON schema then the properties keyword will be ignored. If the keyword is set to a boolean the behaviour will be the same regardless of if it was set to true or false . { \"repositoryDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : \"false\" , \"properties\" : { \"dataOne\" : { \"type\" : \"object\" , \"addtionalProperties\" : { \"type\" : \"string\" } }, \"dataTwo\" : { \"type\" : \"object\" , \"addtionalProperties\" : { \"type\" : \"string\" }, \"properties\" : { \"data\" : { \"type\" : \"string\" } } }, \"dataThree\" : { \"type\" : \"object\" , \"addtionalProperties\" : \"false\" , \"properties\" : { \"data\" : { \"type\" : \"string\" } } }, \"dataFour\" : { \"type\" : \"object\" , \"addtionalProperties\" : \"true\" , \"properties\" : { \"data\" : { \"type\" : \"string\" } } }, \"dataFive\" : { \"type\" : \"object\" , \"addtionalProperties\" : \"false\" , }, \"dataSix\" : { \"type\" : \"object\" , \"addtionalProperties\" : \"true\" , } }, \"nameField\" : \"dataOne\" , \"identityFields\" : [ \"dataOne\" ] } } From the schema above, the properties dataOne and dataTwo , dataThree and dataFour , and dataFive and dataSix will have an identical validations. The first two will validate that the object passed in is a dict with key and value both string type. The next two will create a new inner Python class called either OtherDefinitionDataThree or OtherDefinitionDataFour , they optomize for creating only one as they are identical. Inside that object will be one property data . The last two properties will validate that the object passed in is a dict with the key as a string type, and the value can be anything. Validation Keywords \u00b6 In general all property types are supported however some validation keywords will be ignored during the execution of the Python code. This means that if these keywords are used, no error would be raised within Python if the object violates the schema. Listed below are the keywords ignored for each type that wouldn't validate. Some have examples to be more clear. Number / Integer \u00b6 multipleOf { \"repositoryDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : \"false\" , \"properties\" : { \"data\" : { \"type\" : \"integer\" , \"multipleOf\" : 2 } }, \"nameField\" : \"data\" , \"identityFields\" : [ \"data\" ] } } This would work even though it would fail the schema check: from generated.defintions import RepositoryDefinition repository = RepositoryDefinition () repository . data = 3 Arrays / Tuples \u00b6 additionalItems minItems maxItems uniqueItems contains items Must be a single type, not an array (tuples are not supported): { \"repositoryDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : \"false\" , \"properties\" : { \"data\" : { \"type\" : \"array\" , \"items\" : [ { \"type\" : \"number\" }, { \"type\" : \"string\" }, { \"type\" : \"boolean\" } ] } }, \"nameField\" : \"data\" , \"identityFields\" : [ \"data\" ] } } This would work even though it would fail the schema check: from generated.defintions import RepositoryDefinition repository = RepositoryDefinition () repository . data = [ \"string\" , False , 3 ] Objects \u00b6 minProperties maxProperties patternProperties dependencies propertyNames Enumerated values \u00b6 If the enum keyword is used within a subobject, type has to be string . { \"repositoryDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : \"false\" , \"properties\" : { \"stringData\" : { \"enum\" : [ \"A\" , \"B\" , \"C\" ] }, \"arrayData\" : { \"type\" : \"array\" , \"items\" : { \"enum\" : [ \"DO\" , \"RE\" , \"MI\" ] } }, \"objectData\" : { \"type\" : \"object\" , \"additionalProperties\" : { \"enum\" : [ \"ONE\" , \"TWO\" , \"THREE\" ] } }, \"definedObjectData\" : { \"type\" : \"object\" , \"properties\" : { \"objectStringData\" : { \"enum\" : [ \"o.A\" , \"o.B\" , \"o.C\" ] }, }, \"additionalProperties\" : \"false\" } }, \"nameField\" : \"stringData\" , \"identityFields\" : [ \"stringData\" ] } } In the above example there are four properties: stringData , arrayData , objectData , and definedObjectData . Validation works for stringData but are skipped for the other three. In fact the definedObjectData which with properties would usually create a separte Python class does not at all. This means the following code would work even though it would fail the schema check: from generated.defintions import RepositoryDefinition repository = RepositoryDefinition () repository . array_data = [ 10 , 11 , 12 ] repository . object_data = { \"key\" : 1 } repository . defined_object_data = { \"key\" : 2 } And this code would actually fail with a GeneratedClassesError during the Python execution saying Invalid enum value D for 'string_data', must be one of [A, B, C] if defined. : from generated.defintions import RepositoryDefinition repository = RepositoryDefinition () repository . string_data = \"D\"","title":"Schemas"},{"location":"References/Schemas/#schemas","text":"","title":"Schemas"},{"location":"References/Schemas/#about-schemas","text":"Any time a plugin needs to store its own data, or needs to ask the user for data, the Delphix Engine needs to be told about the format of that data: What is the set of data needed and what should they be called? What is the type of each piece of data: Strings? Integers? Booleans? Plugins use schemas to describe the format of such data. Once a schema is defined, it is used in three ways It tells the Delphix Engine how to store the data for later use. It is used to autogenerate a custom user interface, and to validate user inputs. It is used to autogenerate Python classes that can be used by plugin code to access and manipulate user input and stored data. There are five plugin-customizable data formats: Delphix Object Schema Autogenerated Class Repository RepositoryDefinition RepositoryDefinition Source Config SourceConfigDefinition SourceConfigDefinition Linked Source LinkedSourceDefinition LinkedSourceDefinition Virtual Source VirtualSourceDefinition VirtualSourceDefinition Snapshot SnapshotDefinition SnapshotDefinition","title":"About Schemas"},{"location":"References/Schemas/#json-schemas","text":"Plugins use JSON schemas for their custom datatypes. There are three main things to understand about them, which are explained just below: What is JSON? What is a JSON schema? How has Delphix augmented JSON schemas?","title":"JSON Schemas"},{"location":"References/Schemas/#json","text":"JSON stands for \"Javascript Object Notation\". JSON is a data-interchange format that is intended to be precise and also somewhat human-readable. Here are some simple examples of data in JSON format: JSON Description \"hello\" A string. Note the double quotes. 17 An integer true A boolean {\"name\": \"Julie\", \"age\": 37} A JSON object with two fields, name (a string), and age (an integer). Objects are denoted with curly braces. [ true, false, true] A JSON array with three booleans. Arrays are denoted with square brackets. For more details on JSON, please see https://www.json.org/ .","title":"JSON"},{"location":"References/Schemas/#json-schemas_1","text":"The \"JSON schema\" format is built on top of JSON. This adds some special rules and keywords that are intended to facilitate the description of the format of data (whereas \"raw\" JSON is intended for storing data). Here is an example of a JSON schema that defines a (simplified) US address: { \"type\" : \"object\" , \"required\" : [ \"name\" , \"streetNumber\" , \"street\" , \"city\" , \"state\" , \"zip5\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"streetNumber\" : { \"type\" : \"string\" }, \"street\" : { \"type\" : \"string\" }, \"unit\" : { \"type\" : \"string\" }, \"city\" : { \"type\" : \"string\" , \"pattern\" : \"^[A-Z][A-Za-z ]*$\" }, \"state\" : { \"type\" : \"string\" , \"pattern\" : \"^[A-Z]{2}$\" }, \"zip5\" : { \"type\" : \"string\" , \"pattern\" : \"^[0-9]{5}\" }, \"zipPlus4\" : { \"type\" : \"string\" , \"pattern\" : \"^[0-9]{4}\" } } } Note that this is perfectly valid JSON data. It's a JSON object with four fields: type (a JSON string), required (A JSON array), additionalProperties (a JSON boolean), and properties . properties , in turn is a JSON object with with 8 fields, each of which is a JSON object, with its own properties, etc. But, this isn't just a JSON object. This is a JSON schema. It uses special keywords like type required , and additionalProperties . These have specially-defined meanings in the context of JSON schemas. Here is a list of the special keywords used by the above schema. Note that this is only a small subset of JSON schema keywords. keyword description additionalProperties Determines whether the schema allows properties that are not explicitly listed in the properties specification. Must be a true or false . pattern Used with string types to specify a regular expression that the property must conform to. required A list of required properties. Properties not listed in this list are optional. string Used with type to declare that a property must be a string. type Specifies a datatype. Common values are object , array , number , integer , boolean , and string . Some points to note about the address schema above: Because of the required list, all valid addresses must have fields called name , streetNumber and so on. unit and zipPlus4 do not appear in the required list, and therefore are optional. Because of additionalProperties being false , valid addresses cannot make up their own fields like nickname or doorbellLocation . Because of the pattern , any state field in a valid address must consist of exactly two capital letters. Similarly, city must only contain letters and spaces, and zip and zipPlus4 must only contain digits. Each property has its own valid subschema that describes its own type definition. Here is a JSON object that conforms to the above schema: { \"name\" : \"Delphix\" , \"streetNumber\" : \"220\" , \"street\" : \"Congress St.\" , \"unit\" : \"200\" , \"city\" : \"Boston\" , \"state\" : \"MA\" , \"zip\" : \"02210\" } Info A common point of confusion is the distinction between a JSON schema and a JSON object that conforms to a schema. Remember, a schema describes the form of data. In our example, the schema describes what an address looks like. The address itself is not a schema. For much more detail on JSON schemas, including which keywords are available, what they mean, and where you can use them, see https://json-schema.org/understanding-json-schema/ .","title":"JSON Schemas"},{"location":"References/Schemas/#delphix-specific-extensions-to-json-schema","text":"The JSON schema vocabulary is designed to be extensible for special uses, and Delphix has taken advantage of this to add some new Delphix-specific keywords. The list below outlines each of these keywords, and provides minimal examples of how they might be used.","title":"Delphix-specific Extensions to JSON Schema"},{"location":"References/Schemas/#description","text":"Summary Required or Optional? Optional Where? In any property subschema, at the same level as type . The description keyword can optionally appear on any property. If it does appear, it is used by the UI as explanatory text for the UI widget associated with the property. If it does not appear, then no explanatory text is shown. In this example, the UI would show \"User-readable name for the provisioned database\" in small text under the widget. { \"properties\" : { \"name\" : { \"type\" : \"string\" , \"description\" : \"User-readable name for the provisioned database\" } } }","title":"description"},{"location":"References/Schemas/#identityfields","text":"Summary Required or Optional? Required (for repository and source config schemas only) Where? At the top level of a repository or source config schema, at the same level as type and properties . The identityFields is a list of property names that, together, serve as a unique identifier for a repository or source config. When a plugin's automatic discovery code is called, it will return a list of repositories (or source configs). The Delphix Engine needs to be able to compare this new list with whatever repositories it already knows about. For example, suppose the engine already knows about a single repository with data {\"dbname\": \"my_databsae\", \"path\": \"/var/db/db01\"} (note the misspelling!). And, then suppose that automatic discovery is re-run and it returns repository data { \"dbname\": \"my_database\", \"path\": \"/var/db/db01\"} . What should the Delphix Engine do? Should it conclude that \"my_databsae\" has been deleted, and there is a completely new repository named \"my_database\"? Or, should it conclude that we still have the same old repository, but with an updated name? identityFields is used to handle this. When the engine compares \"new\" data with \"old\" data, it concludes that they belong to the same repository if all of the identity fields match. If any of the identity fields do not match, then the \"new\" repository data is judged to represent a different repository than the old data. identityFields is required for RepositoryDefinition and SourceConfigDefinition schemas, and may not be used in any other schemas. In this example, we'll tell the Delphix Engine that path is the sole unique identifier. { \"properties\" : { \"dbname\" : { \"type\" : \"string\" }, \"path\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"path\" ] }","title":"identityFields"},{"location":"References/Schemas/#namefield","text":"Summary Required or Optional? Required (for repository and source config schemas only) Where? At the top level of a repository or source config schema, at the same level as type and properties . The nameField keyword specifies a single property that is to be used to name the object in the Delphix Engine. The property must be a string field. This keyword is used at the same level as properties . It is required for RepositoryDefinition and SourceConfigDefinition schemas, and may not be used in any other schemas. In this example, we will use the path property as the user-visible name. { \"properties\" : { \"path\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"integer\" } }, \"nameField\" : \"path\" } So, if we have an repository object that looks like { \"path\" : \"/usr/bin\" , \"port\" : 8800 } then the user will be able to refer to this object as /usr/bin .","title":"nameField"},{"location":"References/Schemas/#ordering","text":"Summary Required or Optional? Optional Where? At the top level, same level as type and properties . The ordering keyword can be used to order the fields when the UI is autogenerated. { \"properties\" : { \"path\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"integer\" } }, \"ordering\" : [ \"port\" , \"path\" ] } In the example above, the port will be the first field in the autogenerated UI wizard followed by path .","title":"ordering"},{"location":"References/Schemas/#password","text":"Summary Required or Optional? Optional Where? As the value for the format keyword in any string property's subschema. The password keyword can be used to specify the format of a string . (Note that format is a standard keyword and is not Delphix-specific). If a property is tagged as a password, then the UI will never show the value on screen, and the value will be encrypted before being stored as described here . In this example, the dbPass field on any object will be treated as a password. { \"properties\" : { \"dbPass\" : { \"type\" : \"string\" , \"format\" : \"password\" } } }","title":"password"},{"location":"References/Schemas/#prettyname","text":"Summary Required or Optional? Optional Where? In any property subschema, at the same level as type . The prettyName keyword can optionally appear on any property. If it does appear, it is used by the UI as a title for the UI widget associated with the property. If it does not appear, then the name of the property is used. In this example, the user would see \"Name of Database\" on the UI, instead of just \"name\". { \"properties\" : { \"name\" : { \"type\" : \"string\" , \"prettyName\" : \"Name of Database\" } } }","title":"prettyName"},{"location":"References/Schemas/#unixpath","text":"Summary Required or Optional? Optional Where? As the value for the format keyword in any string property's subschema. The unixpath keyword is used to specify the format of a string. This will allow the Delphix Engine to verify and enforce that a particular field can be parsed as a valid Unix path. { \"properties\" : { \"datapath\" : { \"type\" : \"string\" , \"format\" : \"unixpath\" } } }","title":"unixpath"},{"location":"References/Schemas/#reference","text":"Summary Required or Optional? Optional Where? As the value for the format keyword in any string property's subschema. The reference keyword is used to specify the format of a string. This will allow the plugin author to ask the user to select environments and environment users on the Delphix Engine. \"properties\" : { \"env\" : { \"type\" : \"string\" , \"format\" : \"reference\" , \"referenceType\" : \"UNIX_HOST_ENVIRONMENT\" }, \"envUser\" : { \"type\" : \"string\" , \"format\" : \"reference\" , \"referenceType\" : \"HOST_USER\" , \"matches\u201d: \" env\" } }","title":"reference"},{"location":"References/Schemas/#referencetype","text":"Summary Required or Optional? Optional Where? In any property subschema of type string and format reference , at the same level as type. The referenceType keyword is used to specify the reference type. Possible values: Environment : UNIX_HOST_ENVIRONMENT Environment User : HOST_USER \"properties\" : { \"env\" : { \"type\" : \"string\" , \"format\" : \"reference\" , \"referenceType\" : \"UNIX_HOST_ENVIRONMENT\" }, \"envUser\" : { \"type\" : \"string\" , \"format\" : \"reference\" , \"referenceType\" : \"HOST_USER\" , \"matches\u201d: \" env\" } }","title":"referenceType"},{"location":"References/Schemas/#matches","text":"Summary Required or Optional? Optional Where? In any property subschema of type string and format reference , at the same level as type. The matches keyword is used to map an environment user to an environment by specifying the environment's property name. \"properties\" : { \"env\" : { \"type\" : \"string\" , \"format\" : \"reference\" , \"referenceType\" : \"UNIX_HOST_ENVIRONMENT\" }, \"envUser\" : { \"type\" : \"string\" , \"format\" : \"reference\" , \"referenceType\" : \"HOST_USER\" , \"matches\u201d: \" env\" } } In the example above, environment user envUser maps to environment env .","title":"matches"},{"location":"References/Schemas/#json-schema-limitations","text":"To be able to autogenerate Python classes there are some restrictions to the JSON Schemas that are supported.","title":"JSON Schema Limitations"},{"location":"References/Schemas/#generation-error","text":"There are some valid JSON schemas that will cause the property to not be generated in the autogenerated Python classes. Unfortunately the build command will silently fail so be sure to look at the generated classes and verify all the properties exist.","title":"Generation Error"},{"location":"References/Schemas/#multiple-types","text":"For the type keyword, only a single type may be specified. Arrays of types are not supported. { \"repositoryDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : \"false\" , \"properties\" : { \"data\" : { \"type\" : [ \"integer\" , \"string\" ] } }, \"nameField\" : \"data\" , \"identityFields\" : [ \"data\" ] } } The data property will not even exist: from generated.defintions import RepositoryDefinition repository = RepositoryDefinition () repository . data = 3 print ( repository ) This would print: {}","title":"Multiple types"},{"location":"References/Schemas/#combining-schemas","text":"For the following keywords, if they are specified the property will not exist in the class. * anyOf * allOf * oneOf * not { \"repositoryDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : \"false\" , \"properties\" : { \"any\" : { \"anyOf\" : [ { \"type\" : \"integer\" , \"minimum\" : 2 }, { \"type\" : \"string\" , \"minLength\" : 4 } ] }, \"one\" : { \"oneOf\" : [ { \"type\" : \"integer\" , \"minimum\" : 3 }, { \"type\" : \"integer\" , \"maximum\" : 5 } ] } }, \"nameField\" : \"data\" , \"identityFields\" : [ \"data\" ] } } The any and one properties would not exist: from generated.defintions import RepositoryDefinition repository = RepositoryDefinition () repository . any = \"string\" repository . one = 6 print ( repository ) This would print: {}","title":"Combining schemas"},{"location":"References/Schemas/#object-additional-properties","text":"The additionalProperties keyword inside the object property can either be a boolean or a JSON schema. If it is a schema it needs to have the keyword type . If the additionalProperties is set to a JSON schema then the properties keyword will be ignored. If the keyword is set to a boolean the behaviour will be the same regardless of if it was set to true or false . { \"repositoryDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : \"false\" , \"properties\" : { \"dataOne\" : { \"type\" : \"object\" , \"addtionalProperties\" : { \"type\" : \"string\" } }, \"dataTwo\" : { \"type\" : \"object\" , \"addtionalProperties\" : { \"type\" : \"string\" }, \"properties\" : { \"data\" : { \"type\" : \"string\" } } }, \"dataThree\" : { \"type\" : \"object\" , \"addtionalProperties\" : \"false\" , \"properties\" : { \"data\" : { \"type\" : \"string\" } } }, \"dataFour\" : { \"type\" : \"object\" , \"addtionalProperties\" : \"true\" , \"properties\" : { \"data\" : { \"type\" : \"string\" } } }, \"dataFive\" : { \"type\" : \"object\" , \"addtionalProperties\" : \"false\" , }, \"dataSix\" : { \"type\" : \"object\" , \"addtionalProperties\" : \"true\" , } }, \"nameField\" : \"dataOne\" , \"identityFields\" : [ \"dataOne\" ] } } From the schema above, the properties dataOne and dataTwo , dataThree and dataFour , and dataFive and dataSix will have an identical validations. The first two will validate that the object passed in is a dict with key and value both string type. The next two will create a new inner Python class called either OtherDefinitionDataThree or OtherDefinitionDataFour , they optomize for creating only one as they are identical. Inside that object will be one property data . The last two properties will validate that the object passed in is a dict with the key as a string type, and the value can be anything.","title":"Object Additional Properties"},{"location":"References/Schemas/#validation-keywords","text":"In general all property types are supported however some validation keywords will be ignored during the execution of the Python code. This means that if these keywords are used, no error would be raised within Python if the object violates the schema. Listed below are the keywords ignored for each type that wouldn't validate. Some have examples to be more clear.","title":"Validation Keywords"},{"location":"References/Schemas/#number-integer","text":"multipleOf { \"repositoryDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : \"false\" , \"properties\" : { \"data\" : { \"type\" : \"integer\" , \"multipleOf\" : 2 } }, \"nameField\" : \"data\" , \"identityFields\" : [ \"data\" ] } } This would work even though it would fail the schema check: from generated.defintions import RepositoryDefinition repository = RepositoryDefinition () repository . data = 3","title":"Number / Integer"},{"location":"References/Schemas/#arrays-tuples","text":"additionalItems minItems maxItems uniqueItems contains items Must be a single type, not an array (tuples are not supported): { \"repositoryDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : \"false\" , \"properties\" : { \"data\" : { \"type\" : \"array\" , \"items\" : [ { \"type\" : \"number\" }, { \"type\" : \"string\" }, { \"type\" : \"boolean\" } ] } }, \"nameField\" : \"data\" , \"identityFields\" : [ \"data\" ] } } This would work even though it would fail the schema check: from generated.defintions import RepositoryDefinition repository = RepositoryDefinition () repository . data = [ \"string\" , False , 3 ]","title":"Arrays / Tuples"},{"location":"References/Schemas/#objects","text":"minProperties maxProperties patternProperties dependencies propertyNames","title":"Objects"},{"location":"References/Schemas/#enumerated-values","text":"If the enum keyword is used within a subobject, type has to be string . { \"repositoryDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : \"false\" , \"properties\" : { \"stringData\" : { \"enum\" : [ \"A\" , \"B\" , \"C\" ] }, \"arrayData\" : { \"type\" : \"array\" , \"items\" : { \"enum\" : [ \"DO\" , \"RE\" , \"MI\" ] } }, \"objectData\" : { \"type\" : \"object\" , \"additionalProperties\" : { \"enum\" : [ \"ONE\" , \"TWO\" , \"THREE\" ] } }, \"definedObjectData\" : { \"type\" : \"object\" , \"properties\" : { \"objectStringData\" : { \"enum\" : [ \"o.A\" , \"o.B\" , \"o.C\" ] }, }, \"additionalProperties\" : \"false\" } }, \"nameField\" : \"stringData\" , \"identityFields\" : [ \"stringData\" ] } } In the above example there are four properties: stringData , arrayData , objectData , and definedObjectData . Validation works for stringData but are skipped for the other three. In fact the definedObjectData which with properties would usually create a separte Python class does not at all. This means the following code would work even though it would fail the schema check: from generated.defintions import RepositoryDefinition repository = RepositoryDefinition () repository . array_data = [ 10 , 11 , 12 ] repository . object_data = { \"key\" : 1 } repository . defined_object_data = { \"key\" : 2 } And this code would actually fail with a GeneratedClassesError during the Python execution saying Invalid enum value D for 'string_data', must be one of [A, B, C] if defined. : from generated.defintions import RepositoryDefinition repository = RepositoryDefinition () repository . string_data = \"D\"","title":"Enumerated values"},{"location":"References/Schemas_and_Autogenerated_Classes/","text":"Schemas and Autogenerated Classes \u00b6 Plugin operations will sometimes need to work with data in these custom formats. For example, the configure operation will accept snapshot data as an input, and must produce source config data as an output. To enable this, Python classes are generated from the snapshot schema. The aforementioned inputs and outputs are instances of these autogenerated classes. Info Autogenerated Python code will use lower_case_with_underscores as attribute names as per Python variable naming conventions. That is, if we were to use mountLocation as the schema property name, it would be called mount_location in the generated Python code. Info Note that, wherever they can, these generated Python classes will enforce the constraints made by the schema. For example, if a property is listed as required in the schema, then every Python object will be required to always have this property. This implies that all required fields must be given values when the object is constructed. For various examples of this, see the examples below. RepositoryDefinition \u00b6 Defines properties used to identify a Repository . RepositoryDefinition Schema \u00b6 The plugin must also decide on a name field and a set of identityFields to display and uniquely identify the repository . { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"name\" , \"path\" ], \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"path\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"name\" , \"path\" ], \"nameField\" : \"name\" } RepositoryDefinition Class \u00b6 Autogenerated based on the RepositoryDefinition Schema . class RepositoryDefinition : def __init__ ( self , name , path ): self . _inner_dict = { \"name\" : name , \"path\" : path } To use the class: from generated.defintions import RepositoryDefinition # Since both properties are required, they must be specified when constructing the object repository = RepositoryDefinition ( name = \"name\" , path = \"/some/path\" ) SourceConfigDefinition \u00b6 Defines properties used to identify a Source Config . SourceConfigDefinition Schema \u00b6 The plugin must also decide on a name field and a set of identityFields to display and uniquely identify the source config . { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"name\" ], \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"path\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"name\" ], \"nameField\" : \"name\" } SourceConfigDefinition Class \u00b6 Autogenerated based on the SourceConfigDefinition Schema . class SourceConfigDefinition : def __init__ ( self , name , path ): self . _inner_dict = { \"name\" : name , \"path\" : path } To use the class: from generated.defintions import SourceConfigDefinition # A source config that only defines the required \"name\" property. source_config1 = SourceConfigDefinition ( name = \"sc1\" ) # A Source config that defines both \"name\" and \"path\". source_config2 = SourceConfigDefinition ( name = \"sc2\" , path = \"/some/path\" ) # Setting the optional \"path\" property after construction source_config3 = SourceConfigDefinition ( name = \"sc3\" ) install_path = find_install_path () source_config3 . path = install_path LinkedSourceDefinition \u00b6 Defines properties used to identify linked sources . LinkedSourceDefinition Schema \u00b6 { \"type\" : \"object\" , \"required\" : [ \"name\" , \"port\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"integer\" } } } LinkedSourceDefinition Class \u00b6 Autogenerated based on the LinkedSourceDefinition Schema . class LinkedSourceDefinition : def __init__ ( self , name , port ): self . _inner_dict = { \"name\" : name , \"port\" : port } To use the class: from generated.defintions import LinkedSourceDefinition source = LinkedSourceDefinition ( name = \"name\" , port = 1000 ) # Retrieve the properties from the object and log them name = source . name port = source . port logger . debug ( \"Creating source \\\" {} \\\" with port {} \" . format ( name , port )) VirtualSourceDefinition \u00b6 Defines properties used to identify virtual sources . VirtualSourceDefinition Schema \u00b6 { \"type\" : \"object\" , \"required\" : [ \"name\" , \"port\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"integer\" } } } VirtualSourceDefinition Class \u00b6 Autogenerated based on the VirtualSourceDefinition Schema . class VirtualSourceDefinition : def __init__ ( self , name , port ): self . _inner_dict = { \"name\" : name , \"port\" : port } To use the class: from generated.defintions import VirtualSourceDefinition source = VirtualSourceDefinition ( name = \"name\" , port = 1000 ) SnapshotDefinition \u00b6 Defines properties used to snapshots . SnapshotDefinition Schema \u00b6 { \"type\" : \"object\" , \"properties\" : { \"version\" : { \"type\" : \"string\" }, \"transation_id\" : { \"type\" : \"integer\" } } } SnapshotDefinition Class \u00b6 Autogenerated based on the VirtualSourceDefinition Schema . class VirtualSourceDefinition : def __init__ ( self , version , transaction_id ): self . _inner_dict = { \"version\" : version , \"transaction_id\" : transaction_id } To use the class: from generated.defintions import SnapshotDefinition # A snapshot with both properties defined at construction time snapshot1 = SnapshotDefinition ( version = \"1.2.3\" , transaction_id = 1000 ) # A snapshot with properties defined after construction snapshot2 = SnapshotDefinition () snapshot2 . version = \"2.0.0\" snapshot2 . transaction_id = 1500 # A snapshot that omits the optional \"transaction_id\" property snapshot3 = SnapshotDefinition ( version = \"1.0.0\" )","title":"Schemas and Autogenerated Classes"},{"location":"References/Schemas_and_Autogenerated_Classes/#schemas-and-autogenerated-classes","text":"Plugin operations will sometimes need to work with data in these custom formats. For example, the configure operation will accept snapshot data as an input, and must produce source config data as an output. To enable this, Python classes are generated from the snapshot schema. The aforementioned inputs and outputs are instances of these autogenerated classes. Info Autogenerated Python code will use lower_case_with_underscores as attribute names as per Python variable naming conventions. That is, if we were to use mountLocation as the schema property name, it would be called mount_location in the generated Python code. Info Note that, wherever they can, these generated Python classes will enforce the constraints made by the schema. For example, if a property is listed as required in the schema, then every Python object will be required to always have this property. This implies that all required fields must be given values when the object is constructed. For various examples of this, see the examples below.","title":"Schemas and Autogenerated Classes"},{"location":"References/Schemas_and_Autogenerated_Classes/#repositorydefinition","text":"Defines properties used to identify a Repository .","title":"RepositoryDefinition"},{"location":"References/Schemas_and_Autogenerated_Classes/#repositorydefinition-schema","text":"The plugin must also decide on a name field and a set of identityFields to display and uniquely identify the repository . { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"name\" , \"path\" ], \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"path\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"name\" , \"path\" ], \"nameField\" : \"name\" }","title":"RepositoryDefinition Schema"},{"location":"References/Schemas_and_Autogenerated_Classes/#repositorydefinition-class","text":"Autogenerated based on the RepositoryDefinition Schema . class RepositoryDefinition : def __init__ ( self , name , path ): self . _inner_dict = { \"name\" : name , \"path\" : path } To use the class: from generated.defintions import RepositoryDefinition # Since both properties are required, they must be specified when constructing the object repository = RepositoryDefinition ( name = \"name\" , path = \"/some/path\" )","title":"RepositoryDefinition Class"},{"location":"References/Schemas_and_Autogenerated_Classes/#sourceconfigdefinition","text":"Defines properties used to identify a Source Config .","title":"SourceConfigDefinition"},{"location":"References/Schemas_and_Autogenerated_Classes/#sourceconfigdefinition-schema","text":"The plugin must also decide on a name field and a set of identityFields to display and uniquely identify the source config . { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"name\" ], \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"path\" : { \"type\" : \"string\" } }, \"identityFields\" : [ \"name\" ], \"nameField\" : \"name\" }","title":"SourceConfigDefinition Schema"},{"location":"References/Schemas_and_Autogenerated_Classes/#sourceconfigdefinition-class","text":"Autogenerated based on the SourceConfigDefinition Schema . class SourceConfigDefinition : def __init__ ( self , name , path ): self . _inner_dict = { \"name\" : name , \"path\" : path } To use the class: from generated.defintions import SourceConfigDefinition # A source config that only defines the required \"name\" property. source_config1 = SourceConfigDefinition ( name = \"sc1\" ) # A Source config that defines both \"name\" and \"path\". source_config2 = SourceConfigDefinition ( name = \"sc2\" , path = \"/some/path\" ) # Setting the optional \"path\" property after construction source_config3 = SourceConfigDefinition ( name = \"sc3\" ) install_path = find_install_path () source_config3 . path = install_path","title":"SourceConfigDefinition Class"},{"location":"References/Schemas_and_Autogenerated_Classes/#linkedsourcedefinition","text":"Defines properties used to identify linked sources .","title":"LinkedSourceDefinition"},{"location":"References/Schemas_and_Autogenerated_Classes/#linkedsourcedefinition-schema","text":"{ \"type\" : \"object\" , \"required\" : [ \"name\" , \"port\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"integer\" } } }","title":"LinkedSourceDefinition Schema"},{"location":"References/Schemas_and_Autogenerated_Classes/#linkedsourcedefinition-class","text":"Autogenerated based on the LinkedSourceDefinition Schema . class LinkedSourceDefinition : def __init__ ( self , name , port ): self . _inner_dict = { \"name\" : name , \"port\" : port } To use the class: from generated.defintions import LinkedSourceDefinition source = LinkedSourceDefinition ( name = \"name\" , port = 1000 ) # Retrieve the properties from the object and log them name = source . name port = source . port logger . debug ( \"Creating source \\\" {} \\\" with port {} \" . format ( name , port ))","title":"LinkedSourceDefinition Class"},{"location":"References/Schemas_and_Autogenerated_Classes/#virtualsourcedefinition","text":"Defines properties used to identify virtual sources .","title":"VirtualSourceDefinition"},{"location":"References/Schemas_and_Autogenerated_Classes/#virtualsourcedefinition-schema","text":"{ \"type\" : \"object\" , \"required\" : [ \"name\" , \"port\" ], \"additionalProperties\" : false , \"properties\" : { \"name\" : { \"type\" : \"string\" }, \"port\" : { \"type\" : \"integer\" } } }","title":"VirtualSourceDefinition Schema"},{"location":"References/Schemas_and_Autogenerated_Classes/#virtualsourcedefinition-class","text":"Autogenerated based on the VirtualSourceDefinition Schema . class VirtualSourceDefinition : def __init__ ( self , name , port ): self . _inner_dict = { \"name\" : name , \"port\" : port } To use the class: from generated.defintions import VirtualSourceDefinition source = VirtualSourceDefinition ( name = \"name\" , port = 1000 )","title":"VirtualSourceDefinition Class"},{"location":"References/Schemas_and_Autogenerated_Classes/#snapshotdefinition","text":"Defines properties used to snapshots .","title":"SnapshotDefinition"},{"location":"References/Schemas_and_Autogenerated_Classes/#snapshotdefinition-schema","text":"{ \"type\" : \"object\" , \"properties\" : { \"version\" : { \"type\" : \"string\" }, \"transation_id\" : { \"type\" : \"integer\" } } }","title":"SnapshotDefinition Schema"},{"location":"References/Schemas_and_Autogenerated_Classes/#snapshotdefinition-class","text":"Autogenerated based on the VirtualSourceDefinition Schema . class VirtualSourceDefinition : def __init__ ( self , version , transaction_id ): self . _inner_dict = { \"version\" : version , \"transaction_id\" : transaction_id } To use the class: from generated.defintions import SnapshotDefinition # A snapshot with both properties defined at construction time snapshot1 = SnapshotDefinition ( version = \"1.2.3\" , transaction_id = 1000 ) # A snapshot with properties defined after construction snapshot2 = SnapshotDefinition () snapshot2 . version = \"2.0.0\" snapshot2 . transaction_id = 1500 # A snapshot that omits the optional \"transaction_id\" property snapshot3 = SnapshotDefinition ( version = \"1.0.0\" )","title":"SnapshotDefinition Class"},{"location":"References/Workflows/","text":"Workflows \u00b6 Legend \u00b6 Environment Discovery / Refresh \u00b6 Linked Source Sync \u00b6 Linked Source Enable \u00b6 Linked Source Disable \u00b6 Linked Source Delete \u00b6 Virtual Source Provision \u00b6 Virtual Source Snapshot \u00b6 Virtual Source Refresh \u00b6 Virtual Source Rollback \u00b6 Virtual Source Delete \u00b6 Virtual Source Start \u00b6 Virtual Source Stop \u00b6 Virtual Source Enable \u00b6 Virtual Source Disable \u00b6 Upgrade \u00b6","title":"Workflows"},{"location":"References/Workflows/#workflows","text":"","title":"Workflows"},{"location":"References/Workflows/#legend","text":"","title":"Legend"},{"location":"References/Workflows/#environment-discovery-refresh","text":"","title":"Environment Discovery / Refresh"},{"location":"References/Workflows/#linked-source-sync","text":"","title":"Linked Source Sync"},{"location":"References/Workflows/#linked-source-enable","text":"","title":"Linked Source Enable"},{"location":"References/Workflows/#linked-source-disable","text":"","title":"Linked Source Disable"},{"location":"References/Workflows/#linked-source-delete","text":"","title":"Linked Source Delete"},{"location":"References/Workflows/#virtual-source-provision","text":"","title":"Virtual Source Provision"},{"location":"References/Workflows/#virtual-source-snapshot","text":"","title":"Virtual Source Snapshot"},{"location":"References/Workflows/#virtual-source-refresh","text":"","title":"Virtual Source Refresh"},{"location":"References/Workflows/#virtual-source-rollback","text":"","title":"Virtual Source Rollback"},{"location":"References/Workflows/#virtual-source-delete","text":"","title":"Virtual Source Delete"},{"location":"References/Workflows/#virtual-source-start","text":"","title":"Virtual Source Start"},{"location":"References/Workflows/#virtual-source-stop","text":"","title":"Virtual Source Stop"},{"location":"References/Workflows/#virtual-source-enable","text":"","title":"Virtual Source Enable"},{"location":"References/Workflows/#virtual-source-disable","text":"","title":"Virtual Source Disable"},{"location":"References/Workflows/#upgrade","text":"","title":"Upgrade"},{"location":"Release_Notes/0.4.0/0.4.0/","text":"Release - Early Preview 2 (v0.4.0) \u00b6 To install or upgrade the SDK, refer to instructions here . New & Improved \u00b6 Added a new CLI command download-logs to enable downloading plugin generated logs from the Delphix Engine. Added an optional argument named check to the following platform library functions: run_bash run_powershell With check=true , the platform library function checks the exit_code and raises an exception if it is non-zero. Modified init to auto-generate default implementations for all required plugin operations. Improved build validation for: Required plugin operations . Incorrect plugin operation argument names. Plugin Config entryPoint : The entryPoint is now imported during the build as part of the validation. Schemas : Validated to conform to the JSON Schema Draft-07 Specification . Improved runtime validation and error messages for: Objects returned from plugin operations . Platform Classes during instantiation. Platform Library function arguments. Added support for Docker based plugins by specifying rootSquashEnabled: false in the plugin config . Added Job and thread information to plugin generated log messages to increase diagnosability and observability. Breaking Changes \u00b6 A new argument snapshot_parameters was added to the following staged plugin operations: Staged Linked Source Pre-Snapshot Staged Linked Source Post-Snapshot This argument will allow the end user to indicate to the plugin whether or not to initiate a full ingestion for a dSource. More details about the new argument are here . Detailed steps to detect and make changes. Properties of the StagedSource class were modified: connection was renamed to source_connection . staged_connection was added to allow connecting to the staging environment. This will enable plugins to connect to both the source and staging environments. More details about these properties are here . Detailed steps to detect and make changes. Fixed \u00b6 Allow access to nested package resources via pkgutil.get_data . Fixed Out of Memory exceptions. Fixed missing or incorrectly populated properties for the following classes: Class Properties VirtualSource mounts RemoteUser name RemoteEnvironment name RemoteHost name binary_path Updated Job warnings during discovery to display the underlying Python exceptions if one is raised by the plugin operations. Recreate the plugin's log directory if a plugin is deleted and re-uploaded to the Delphix Engine. Mark incorrectly provisioned VDBs as unusable and prevent subsequent Delphix Engine operations on such VDBs. Better error messages when incorrect environment types are used for Platform Libraries. Better error messages when a plugin's schema is updated and the plugin is re-uploaded to the Delphix Engine, with clear instructions on how to proceed. Fixed build failures on Windows.","title":"Release - Early Preview 2 (v0.4.0)"},{"location":"Release_Notes/0.4.0/0.4.0/#release-early-preview-2-v040","text":"To install or upgrade the SDK, refer to instructions here .","title":"Release - Early Preview 2 (v0.4.0)"},{"location":"Release_Notes/0.4.0/0.4.0/#new-improved","text":"Added a new CLI command download-logs to enable downloading plugin generated logs from the Delphix Engine. Added an optional argument named check to the following platform library functions: run_bash run_powershell With check=true , the platform library function checks the exit_code and raises an exception if it is non-zero. Modified init to auto-generate default implementations for all required plugin operations. Improved build validation for: Required plugin operations . Incorrect plugin operation argument names. Plugin Config entryPoint : The entryPoint is now imported during the build as part of the validation. Schemas : Validated to conform to the JSON Schema Draft-07 Specification . Improved runtime validation and error messages for: Objects returned from plugin operations . Platform Classes during instantiation. Platform Library function arguments. Added support for Docker based plugins by specifying rootSquashEnabled: false in the plugin config . Added Job and thread information to plugin generated log messages to increase diagnosability and observability.","title":"New &amp; Improved"},{"location":"Release_Notes/0.4.0/0.4.0/#breaking-changes","text":"A new argument snapshot_parameters was added to the following staged plugin operations: Staged Linked Source Pre-Snapshot Staged Linked Source Post-Snapshot This argument will allow the end user to indicate to the plugin whether or not to initiate a full ingestion for a dSource. More details about the new argument are here . Detailed steps to detect and make changes. Properties of the StagedSource class were modified: connection was renamed to source_connection . staged_connection was added to allow connecting to the staging environment. This will enable plugins to connect to both the source and staging environments. More details about these properties are here . Detailed steps to detect and make changes.","title":"Breaking Changes"},{"location":"Release_Notes/0.4.0/0.4.0/#fixed","text":"Allow access to nested package resources via pkgutil.get_data . Fixed Out of Memory exceptions. Fixed missing or incorrectly populated properties for the following classes: Class Properties VirtualSource mounts RemoteUser name RemoteEnvironment name RemoteHost name binary_path Updated Job warnings during discovery to display the underlying Python exceptions if one is raised by the plugin operations. Recreate the plugin's log directory if a plugin is deleted and re-uploaded to the Delphix Engine. Mark incorrectly provisioned VDBs as unusable and prevent subsequent Delphix Engine operations on such VDBs. Better error messages when incorrect environment types are used for Platform Libraries. Better error messages when a plugin's schema is updated and the plugin is re-uploaded to the Delphix Engine, with clear instructions on how to proceed. Fixed build failures on Windows.","title":"Fixed"},{"location":"Release_Notes/0.4.0/0.4.0_Breaking_Changes/","text":"Breaking Changes - Early Preview 2 (v.0.4.0) \u00b6 New Argument snapshot_parameters \u00b6 A new argument snapshot_parameters was added to the following staged plugin operations: Staged Linked Source Pre-Snapshot Staged Linked Source Post-Snapshot This argument will allow the end user to indicate to the plugin whether or not to initiate a full ingestion for a dSource. More details about the new argument are here . What is affected \u00b6 This argument applies only to staged plugins. The plugin's source code will have to be updated for the following staged plugin operations: Staged Linked Source Pre-Snapshot : This plugin operation is optional and will need to be updated if the plugin implements it. Staged Linked Source Post-Snapshot : This plugin operation is required and will need to be updated. How does it fail \u00b6 build will fail with the following error message if the new argument is not added to the affected staged plugin operations: $ dvp build Error: Number of arguments do not match in method staged_post_snapshot. Expected: [ 'staged_source' , 'repository' , 'source_config' , 'snapshot_parameters' ] , Found: [ 'repository' , 'source_config' , 'staged_source' ] . Error: Number of arguments do not match in method staged_pre_snapshot. Expected: [ 'staged_source' , 'repository' , 'source_config' , 'snapshot_parameters' ] , Found: [ 'repository' , 'source_config' , 'staged_source' ] . 0 Warning ( s ) . 2 Error ( s ) . BUILD FAILED. How to fix it \u00b6 Update the affected staged plugin operations to include the new argument snapshot_parameters . Previous releases from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . linked . pre_snapshot () def linked_pre_snapshot_prior ( staged_source , repository , source_config ): # This was the function signature prior to 0.4.0 pass @plugin . linked . post_snapshot () def linked_post_snapshot_prior ( staged_source , repository , source_config ): # This was the function signature prior to 0.4.0 return SnapshotDefinition () 0.4.0 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . linked . pre_snapshot () def linked_pre_snapshot_040 ( staged_source , repository , source_config , snapshot_parameters ): # Updated function signature in 0.4.0 pass @plugin . linked . post_snapshot () def linked_post_snapshot_040 ( staged_source , repository , source_config , snapshot_parameters ): # Updated function signature in 0.4.0 return SnapshotDefinition () StagedSource Properties Modified \u00b6 Properties of the StagedSource class were modified: connection was renamed to source_connection . staged_connection was added to allow connecting to the staging environment. This will enable plugins to connect to both the source and staging environments. More details about these properties are here . What is affected \u00b6 This change applies only to staged plugins. Required Changes \u00b6 The plugin's source code will have to be updated for any staged plugin operations that accesses the connection propery of a StagedSource object. Optional Changes \u00b6 The plugin can choose to use the new staged_connection property to connect to the staging environment of a dSource. How does it fail \u00b6 Any Delphix Engine operation that calls a plugin operation that has not been fixed would fail with the following stack trace as part of the output of the user exception: AttributeError : 'StagedSource' object has no attribute 'connection' How to fix it \u00b6 Update any staged plugin operations that access the renamed property. Previous releases from dlpx.virtualization.platform import Plugin from dlpx.virtualization import libs plugin = Plugin () @plugin . linked . pre_snapshot () def linked_pre_snapshot_prior ( staged_source , repository , source_config ): # Property name was 'connection' was the name of the property for staged_source prior to 0.4.0 libs . run_bash ( staged_source . connection , 'date' ) 0.4.0 from dlpx.virtualization.platform import Plugin from dlpx.virtualization import libs plugin = Plugin () @plugin . linked . pre_snapshot () def linked_pre_snapshot_prior ( staged_source , repository , source_config ): # Property name was updated to 'source_connection' in 0.4.0 libs . run_bash ( staged_source . source_connection , 'date' )","title":"Breaking Changes - Early Preview 2 (v.0.4.0)"},{"location":"Release_Notes/0.4.0/0.4.0_Breaking_Changes/#breaking-changes-early-preview-2-v040","text":"","title":"Breaking Changes - Early Preview 2 (v.0.4.0)"},{"location":"Release_Notes/0.4.0/0.4.0_Breaking_Changes/#new-argument-snapshot_parameters","text":"A new argument snapshot_parameters was added to the following staged plugin operations: Staged Linked Source Pre-Snapshot Staged Linked Source Post-Snapshot This argument will allow the end user to indicate to the plugin whether or not to initiate a full ingestion for a dSource. More details about the new argument are here .","title":"New Argument snapshot_parameters"},{"location":"Release_Notes/0.4.0/0.4.0_Breaking_Changes/#what-is-affected","text":"This argument applies only to staged plugins. The plugin's source code will have to be updated for the following staged plugin operations: Staged Linked Source Pre-Snapshot : This plugin operation is optional and will need to be updated if the plugin implements it. Staged Linked Source Post-Snapshot : This plugin operation is required and will need to be updated.","title":"What is affected"},{"location":"Release_Notes/0.4.0/0.4.0_Breaking_Changes/#how-does-it-fail","text":"build will fail with the following error message if the new argument is not added to the affected staged plugin operations: $ dvp build Error: Number of arguments do not match in method staged_post_snapshot. Expected: [ 'staged_source' , 'repository' , 'source_config' , 'snapshot_parameters' ] , Found: [ 'repository' , 'source_config' , 'staged_source' ] . Error: Number of arguments do not match in method staged_pre_snapshot. Expected: [ 'staged_source' , 'repository' , 'source_config' , 'snapshot_parameters' ] , Found: [ 'repository' , 'source_config' , 'staged_source' ] . 0 Warning ( s ) . 2 Error ( s ) . BUILD FAILED.","title":"How does it fail"},{"location":"Release_Notes/0.4.0/0.4.0_Breaking_Changes/#how-to-fix-it","text":"Update the affected staged plugin operations to include the new argument snapshot_parameters . Previous releases from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . linked . pre_snapshot () def linked_pre_snapshot_prior ( staged_source , repository , source_config ): # This was the function signature prior to 0.4.0 pass @plugin . linked . post_snapshot () def linked_post_snapshot_prior ( staged_source , repository , source_config ): # This was the function signature prior to 0.4.0 return SnapshotDefinition () 0.4.0 from dlpx.virtualization.platform import Plugin plugin = Plugin () @plugin . linked . pre_snapshot () def linked_pre_snapshot_040 ( staged_source , repository , source_config , snapshot_parameters ): # Updated function signature in 0.4.0 pass @plugin . linked . post_snapshot () def linked_post_snapshot_040 ( staged_source , repository , source_config , snapshot_parameters ): # Updated function signature in 0.4.0 return SnapshotDefinition ()","title":"How to fix it"},{"location":"Release_Notes/0.4.0/0.4.0_Breaking_Changes/#stagedsource-properties-modified","text":"Properties of the StagedSource class were modified: connection was renamed to source_connection . staged_connection was added to allow connecting to the staging environment. This will enable plugins to connect to both the source and staging environments. More details about these properties are here .","title":"StagedSource Properties Modified"},{"location":"Release_Notes/0.4.0/0.4.0_Breaking_Changes/#what-is-affected_1","text":"This change applies only to staged plugins.","title":"What is affected"},{"location":"Release_Notes/0.4.0/0.4.0_Breaking_Changes/#required-changes","text":"The plugin's source code will have to be updated for any staged plugin operations that accesses the connection propery of a StagedSource object.","title":"Required Changes"},{"location":"Release_Notes/0.4.0/0.4.0_Breaking_Changes/#optional-changes","text":"The plugin can choose to use the new staged_connection property to connect to the staging environment of a dSource.","title":"Optional Changes"},{"location":"Release_Notes/0.4.0/0.4.0_Breaking_Changes/#how-does-it-fail_1","text":"Any Delphix Engine operation that calls a plugin operation that has not been fixed would fail with the following stack trace as part of the output of the user exception: AttributeError : 'StagedSource' object has no attribute 'connection'","title":"How does it fail"},{"location":"Release_Notes/0.4.0/0.4.0_Breaking_Changes/#how-to-fix-it_1","text":"Update any staged plugin operations that access the renamed property. Previous releases from dlpx.virtualization.platform import Plugin from dlpx.virtualization import libs plugin = Plugin () @plugin . linked . pre_snapshot () def linked_pre_snapshot_prior ( staged_source , repository , source_config ): # Property name was 'connection' was the name of the property for staged_source prior to 0.4.0 libs . run_bash ( staged_source . connection , 'date' ) 0.4.0 from dlpx.virtualization.platform import Plugin from dlpx.virtualization import libs plugin = Plugin () @plugin . linked . pre_snapshot () def linked_pre_snapshot_prior ( staged_source , repository , source_config ): # Property name was updated to 'source_connection' in 0.4.0 libs . run_bash ( staged_source . source_connection , 'date' )","title":"How to fix it"},{"location":"Release_Notes/1.0.0/1.0.0/","text":"Release - GA (v1.0.0) \u00b6 To install or upgrade the SDK, refer to instructions here . New & Improved \u00b6 Added support for a CLI configuration file to specify default options for dvp commands. More details here . Improved speed and scalability of plugin operations: Reduced startup time for plugin operations from seconds to milliseconds. Improved memory utilization on the Delphix Engine to enable a large number of plugin operations to execute in parallel. Added the ability for plugins to raise user visible messages with a custom message, action and output related to a failure during a plugin operation. Refer to the User Visible Errors section for more details. Improved validation for type and range checks for autogenerated classes. Improved security for the plugin's runtime when executed on the Delphix Engine. Removed the Delphix Engine feature flag PYTHON_TOOLKITS as the Delphix Engine supports plugins built on the SDK by default. The Getting Started section has been updated has well. Breaking Changes \u00b6 The following fields in the Plugin Config were renamed: Previous Updated name plugin_id prettyName name Additionally, the plugin_id is now required to be a UUID with a format: [0-9a-fA-F]{8}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{12} . Detailed steps to detect and make changes. Fixed \u00b6 Updated remote host operations to execute as the RemoteUser specfied instead of the primary environment user. Fixed an incorrect user exception when the required plugin operation linked.post_snapshot was missing. Updated run_expect to return an exit_code , stdout , stderr like other platform library functions. Fixed run_powershell to not automatically redirect stderr to stdout . Ensured that all exceptions raised by the Staged Linked Source Worker plugin operation are converted to faults for the user. Enabled the MountSpecification to be constructed with mounts that refer to different environments. Sanitized the Python stack traces from exceptions during plugin execution and removed paths that reference where the plugin was built. Removed a spurious build warning for DIRECT plugins that incorrectly suggested implementing the Staged Linked Source Mount Specification plugin operation. Removed a spurious message global name 'exit' is not defined which was displayed when a plugin library function failed. Updated manualDiscovery to be optional in the Plugin Config . The default value will be True .","title":"Release - GA (v1.0.0)"},{"location":"Release_Notes/1.0.0/1.0.0/#release-ga-v100","text":"To install or upgrade the SDK, refer to instructions here .","title":"Release - GA (v1.0.0)"},{"location":"Release_Notes/1.0.0/1.0.0/#new-improved","text":"Added support for a CLI configuration file to specify default options for dvp commands. More details here . Improved speed and scalability of plugin operations: Reduced startup time for plugin operations from seconds to milliseconds. Improved memory utilization on the Delphix Engine to enable a large number of plugin operations to execute in parallel. Added the ability for plugins to raise user visible messages with a custom message, action and output related to a failure during a plugin operation. Refer to the User Visible Errors section for more details. Improved validation for type and range checks for autogenerated classes. Improved security for the plugin's runtime when executed on the Delphix Engine. Removed the Delphix Engine feature flag PYTHON_TOOLKITS as the Delphix Engine supports plugins built on the SDK by default. The Getting Started section has been updated has well.","title":"New &amp; Improved"},{"location":"Release_Notes/1.0.0/1.0.0/#breaking-changes","text":"The following fields in the Plugin Config were renamed: Previous Updated name plugin_id prettyName name Additionally, the plugin_id is now required to be a UUID with a format: [0-9a-fA-F]{8}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{12} . Detailed steps to detect and make changes.","title":"Breaking Changes"},{"location":"Release_Notes/1.0.0/1.0.0/#fixed","text":"Updated remote host operations to execute as the RemoteUser specfied instead of the primary environment user. Fixed an incorrect user exception when the required plugin operation linked.post_snapshot was missing. Updated run_expect to return an exit_code , stdout , stderr like other platform library functions. Fixed run_powershell to not automatically redirect stderr to stdout . Ensured that all exceptions raised by the Staged Linked Source Worker plugin operation are converted to faults for the user. Enabled the MountSpecification to be constructed with mounts that refer to different environments. Sanitized the Python stack traces from exceptions during plugin execution and removed paths that reference where the plugin was built. Removed a spurious build warning for DIRECT plugins that incorrectly suggested implementing the Staged Linked Source Mount Specification plugin operation. Removed a spurious message global name 'exit' is not defined which was displayed when a plugin library function failed. Updated manualDiscovery to be optional in the Plugin Config . The default value will be True .","title":"Fixed"},{"location":"Release_Notes/1.0.0/1.0.0_Breaking_Changes/","text":"Breaking Changes - GA (v.1.0.0) \u00b6 Plugin Config Fields Renamed \u00b6 The following fields in the Plugin Config were renamed: Previous Updated name plugin_id prettyName name Additionally, the plugin_id is now required to be a UUID with format: [0-9a-fA-F]{8}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{12} . This will allow the plugins to be uniquely identified across plugin developers. What is affected \u00b6 All plugins built with v0.3.0 or v0.4.0 will be affected. The Plugin Config fields will have to be updated. How does it fail \u00b6 dvp build will fail with the following error message if the Plugin Config fields are not updated: $ dvp build Error: Additional properties are not allowed ( 'prettyName' was unexpected ) on [] { \"pluginType\" : \"DIRECT\" , \"name\" : \"My Plugin\" , \"language\" : \"PYTHON27\" , \"manualDiscovery\" : true, \"hostTypes\" : [ \"UNIX\" ] , \"version\" : \"0.1.0\" , \"entryPoint\" : \"plugin_runner:plugin\" , \"srcDir\" : \"src\" , \"prettyName\" : \"My Plugin\" , \"schemaFile\" : \"schema.json\" } Error: 'id' is a required property on [] { \"pluginType\" : \"DIRECT\" , \"name\" : \"My Plugin\" , \"language\" : \"PYTHON27\" , \"manualDiscovery\" : true, \"hostTypes\" : [ \"UNIX\" ] , \"version\" : \"0.1.0\" , \"entryPoint\" : \"plugin_runner:plugin\" , \"srcDir\" : \"src\" , \"prettyName\" : \"My Plugin\" , \"schemaFile\" : \"schema.json\" } Validation failed on plugin_config.yml. 0 Warning ( s ) . 2 Error ( s ) BUILD FAILED. How to fix it \u00b6 Rename the Plugin Config fields. Make sure that the id is a UUID of the format [0-9a-fA-F]{8}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{12} . A UUID can be generated manually using an online generator or via Python: $ python >>> import uuid >>> uuid.uuid4 () UUID ( '4174f1b8-45df-43cc-8e4c-21d309c17861' ) Previous releases name: my_plugin prettyName: My Plugin version: 0.1.0 language: PYTHON27 hostTypes: - UNIX pluginType: DIRECT manualDiscovery: true entryPoint: plugin_runner:plugin srcDir: src schemaFile: schema.json 1.0.0 id: 4174f1b8-45df-43cc-8e4c-21d309c17861 name: My Plugin version: 0.1.0 language: PYTHON27 hostTypes: - UNIX pluginType: DIRECT manualDiscovery: true entryPoint: plugin_runner:plugin srcDir: src schemaFile: schema.json","title":"Breaking Changes - GA (v.1.0.0)"},{"location":"Release_Notes/1.0.0/1.0.0_Breaking_Changes/#breaking-changes-ga-v100","text":"","title":"Breaking Changes - GA (v.1.0.0)"},{"location":"Release_Notes/1.0.0/1.0.0_Breaking_Changes/#plugin-config-fields-renamed","text":"The following fields in the Plugin Config were renamed: Previous Updated name plugin_id prettyName name Additionally, the plugin_id is now required to be a UUID with format: [0-9a-fA-F]{8}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{12} . This will allow the plugins to be uniquely identified across plugin developers.","title":"Plugin Config Fields Renamed"},{"location":"Release_Notes/1.0.0/1.0.0_Breaking_Changes/#what-is-affected","text":"All plugins built with v0.3.0 or v0.4.0 will be affected. The Plugin Config fields will have to be updated.","title":"What is affected"},{"location":"Release_Notes/1.0.0/1.0.0_Breaking_Changes/#how-does-it-fail","text":"dvp build will fail with the following error message if the Plugin Config fields are not updated: $ dvp build Error: Additional properties are not allowed ( 'prettyName' was unexpected ) on [] { \"pluginType\" : \"DIRECT\" , \"name\" : \"My Plugin\" , \"language\" : \"PYTHON27\" , \"manualDiscovery\" : true, \"hostTypes\" : [ \"UNIX\" ] , \"version\" : \"0.1.0\" , \"entryPoint\" : \"plugin_runner:plugin\" , \"srcDir\" : \"src\" , \"prettyName\" : \"My Plugin\" , \"schemaFile\" : \"schema.json\" } Error: 'id' is a required property on [] { \"pluginType\" : \"DIRECT\" , \"name\" : \"My Plugin\" , \"language\" : \"PYTHON27\" , \"manualDiscovery\" : true, \"hostTypes\" : [ \"UNIX\" ] , \"version\" : \"0.1.0\" , \"entryPoint\" : \"plugin_runner:plugin\" , \"srcDir\" : \"src\" , \"prettyName\" : \"My Plugin\" , \"schemaFile\" : \"schema.json\" } Validation failed on plugin_config.yml. 0 Warning ( s ) . 2 Error ( s ) BUILD FAILED.","title":"How does it fail"},{"location":"Release_Notes/1.0.0/1.0.0_Breaking_Changes/#how-to-fix-it","text":"Rename the Plugin Config fields. Make sure that the id is a UUID of the format [0-9a-fA-F]{8}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{4}\\-[0-9a-fA-F]{12} . A UUID can be generated manually using an online generator or via Python: $ python >>> import uuid >>> uuid.uuid4 () UUID ( '4174f1b8-45df-43cc-8e4c-21d309c17861' ) Previous releases name: my_plugin prettyName: My Plugin version: 0.1.0 language: PYTHON27 hostTypes: - UNIX pluginType: DIRECT manualDiscovery: true entryPoint: plugin_runner:plugin srcDir: src schemaFile: schema.json 1.0.0 id: 4174f1b8-45df-43cc-8e4c-21d309c17861 name: My Plugin version: 0.1.0 language: PYTHON27 hostTypes: - UNIX pluginType: DIRECT manualDiscovery: true entryPoint: plugin_runner:plugin srcDir: src schemaFile: schema.json","title":"How to fix it"},{"location":"Release_Notes/2.0.0/2.0.0/","text":"Release - GA (v2.0.0) \u00b6 To install or upgrade the SDK, refer to instructions here . New & Improved \u00b6 Added the ability for plugins to upgrade across plugin versions with schema changes. Some hightlights: Schema updates using data migrations. Flexiblity for plugins to pick any release strategy. Plugin upgrades supported across multiple plugin versions. Zero dSource and VDB downtime during plugin upgrade. More details about Plugin Upgrade can be found here . Added a new field externalVersion to the Plugin Config that allows plugins to display an end-user friendly version. More details here . Added a new option to init to select a host type for the plugin ( Unix or Windows ) to make it easier to get started with plugins that support either host platform. Added a new option to upload to block and wait for the upload job to finish on the Delphix Engine before the command returns. Breaking Changes \u00b6 The following field in the Plugin Config was renamed: Previous Updated version buidNumber Additionally buildNumber has to conform to the format described here . Detailed steps to detect and make changes.","title":"Release - GA (v2.0.0)"},{"location":"Release_Notes/2.0.0/2.0.0/#release-ga-v200","text":"To install or upgrade the SDK, refer to instructions here .","title":"Release - GA (v2.0.0)"},{"location":"Release_Notes/2.0.0/2.0.0/#new-improved","text":"Added the ability for plugins to upgrade across plugin versions with schema changes. Some hightlights: Schema updates using data migrations. Flexiblity for plugins to pick any release strategy. Plugin upgrades supported across multiple plugin versions. Zero dSource and VDB downtime during plugin upgrade. More details about Plugin Upgrade can be found here . Added a new field externalVersion to the Plugin Config that allows plugins to display an end-user friendly version. More details here . Added a new option to init to select a host type for the plugin ( Unix or Windows ) to make it easier to get started with plugins that support either host platform. Added a new option to upload to block and wait for the upload job to finish on the Delphix Engine before the command returns.","title":"New &amp; Improved"},{"location":"Release_Notes/2.0.0/2.0.0/#breaking-changes","text":"The following field in the Plugin Config was renamed: Previous Updated version buidNumber Additionally buildNumber has to conform to the format described here . Detailed steps to detect and make changes.","title":"Breaking Changes"},{"location":"Release_Notes/2.0.0/2.0.0_Breaking_Changes/","text":"Breaking Changes - GA (v.2.0.0) \u00b6 Plugin Config Field Renamed \u00b6 The following field in the Plugin Config were replaced: Previous Updated version buildNumber Additionally, the buildNumber must be a string that conforms to the following rules: The string must be composed of a sequence of non-negative integers, not all zero, separated by periods. Trailing zeros are ignored. So, \"1.0.0\" is treated the same as \"1\". Build numbers are sortable numerically, with earlier numbers having more significance than later numbers. So, \"2.0\" comes after \"1.99999\", and \"1.10\" comes after \"1.2\". The Delphix Engine will never allow installation of plugin with a build number that is ordered before the the already-installed build number. More details about the format are here . What is affected \u00b6 All plugins built with v1.0.0 or below will be affected. The Plugin Config field version will have to be updated to buildNumber . How does it fail \u00b6 dvp build will fail with the following error message if the Plugin Config version field is not updated to buildNumber : $ dvp build Error: Additional properties are not allowed ( 'version' was unexpected ) on [ 'additionalProperties' ] Error: 'buildNumber' is a required property on [ 'required' ] Validation failed on /private/var/tmp/fp/plugin_config.yml. 0 Warning ( s ) . 2 Error ( s ) BUILD FAILED. How to fix it \u00b6 Rename the Plugin Config version field to buildNumber . Make sure that the buildNumber conforms to the format described here . Previous releases id: 4174f1b8-45df-43cc-8e4c-21d309c17861 name: My Plugin version: 1.0.0 language: PYTHON27 hostTypes: - UNIX pluginType: DIRECT manualDiscovery: true entryPoint: plugin_runner:plugin srcDir: src schemaFile: schema.json 2.0.0 id: 4174f1b8-45df-43cc-8e4c-21d309c17861 name: My Plugin buildNumber: 1.0.0 language: PYTHON27 hostTypes: - UNIX pluginType: DIRECT manualDiscovery: true entryPoint: plugin_runner:plugin srcDir: src schemaFile: schema.json","title":"Breaking Changes - GA (v.2.0.0)"},{"location":"Release_Notes/2.0.0/2.0.0_Breaking_Changes/#breaking-changes-ga-v200","text":"","title":"Breaking Changes - GA (v.2.0.0)"},{"location":"Release_Notes/2.0.0/2.0.0_Breaking_Changes/#plugin-config-field-renamed","text":"The following field in the Plugin Config were replaced: Previous Updated version buildNumber Additionally, the buildNumber must be a string that conforms to the following rules: The string must be composed of a sequence of non-negative integers, not all zero, separated by periods. Trailing zeros are ignored. So, \"1.0.0\" is treated the same as \"1\". Build numbers are sortable numerically, with earlier numbers having more significance than later numbers. So, \"2.0\" comes after \"1.99999\", and \"1.10\" comes after \"1.2\". The Delphix Engine will never allow installation of plugin with a build number that is ordered before the the already-installed build number. More details about the format are here .","title":"Plugin Config Field Renamed"},{"location":"Release_Notes/2.0.0/2.0.0_Breaking_Changes/#what-is-affected","text":"All plugins built with v1.0.0 or below will be affected. The Plugin Config field version will have to be updated to buildNumber .","title":"What is affected"},{"location":"Release_Notes/2.0.0/2.0.0_Breaking_Changes/#how-does-it-fail","text":"dvp build will fail with the following error message if the Plugin Config version field is not updated to buildNumber : $ dvp build Error: Additional properties are not allowed ( 'version' was unexpected ) on [ 'additionalProperties' ] Error: 'buildNumber' is a required property on [ 'required' ] Validation failed on /private/var/tmp/fp/plugin_config.yml. 0 Warning ( s ) . 2 Error ( s ) BUILD FAILED.","title":"How does it fail"},{"location":"Release_Notes/2.0.0/2.0.0_Breaking_Changes/#how-to-fix-it","text":"Rename the Plugin Config version field to buildNumber . Make sure that the buildNumber conforms to the format described here . Previous releases id: 4174f1b8-45df-43cc-8e4c-21d309c17861 name: My Plugin version: 1.0.0 language: PYTHON27 hostTypes: - UNIX pluginType: DIRECT manualDiscovery: true entryPoint: plugin_runner:plugin srcDir: src schemaFile: schema.json 2.0.0 id: 4174f1b8-45df-43cc-8e4c-21d309c17861 name: My Plugin buildNumber: 1.0.0 language: PYTHON27 hostTypes: - UNIX pluginType: DIRECT manualDiscovery: true entryPoint: plugin_runner:plugin srcDir: src schemaFile: schema.json","title":"How to fix it"},{"location":"Versioning_And_Upgrade/Backports_And_Hotfixes/","text":"Backports and Hotfixes \u00b6 If your plugin uses an \"enterprise-style\" release strategy, then you'll probably want to occasionally provide new \"minor\" or \"patch\" versions that build atop older versions. Code changes that are applied atop old releases are usually called \"backports\". Sometimes, they are also called \"hotfixes\", if the change is specifically created for a single user. These releases present a problem: although they are built atop an older code branch, they are still newer than some releases from a newer code branch. Below, we'll walk through how we prevent users from \"upgrading\" to a new-branch release that would be incompatible with an installed old-branch release. Motivating Example \u00b6 Let's take a look at an example of a possible timeline of releases. February : The initial version of a plugin is released, with build number \"1.0\". This is a simple plugin that uses a simple strategy for syncing dSources. April : A new version is released, with build number \"1.1\". This adds some bugfixes and adds some small optimizations to improve the performance of syncing. August : A new version is released, with build number \"2.0\". This uses a completely new syncing strategy that is far more sophisticated and efficient. Let's assume that not all users will want to upgrade to the 2.0 release immediately. So, even months later, you expect to have a significant number of users still on version 1.0 or 1.1. Later, in October, a bug is found which impacts all releases. This bug is important enough that you want to fix it for all of your end users (not just the ones using 2.0). Here are the behaviors we need: Our 2.0 end users should be able to get the new bugfix without giving up any of the major new features that were part of 2.0. Our 1.0 and 1.1 end users should be able to get the new bugfix without also needing to accept all the major new features that were part of 2.0. Once an end user has received the bugfix, it should be impossible to lose the bugfix in an upgrade. Strategy \u00b6 You can include a data migration along with your bugfix. If your bugfix involves a schema change, you will have to do this anyways. If not, you can still include a data migration that simply does nothing. If a user with the bugfix attempts to \"upgrade\" to 2.0, the Delphix Engine will prevent it, because the 2.0 releases does not include this migration. You would typically follow these steps: Fix the bug by applying a code change atop the 2.0 code. Include the new data migration in your 2.1 release. Separately, apply the same bugfix atop the 1.1 code. Note: depending on how code changed between 1.1 and 2.0, this 1.1-based bugfix might not contain the exact same code as we used with 2.0. Make another new release of the plugin, this time with build number \"1.2\". This release includes the 1.1-based bugfix. It also should include the new data migration. This meets our requirements: Our 2.0 end users can install version 2.1. This gives them the bugfix, and keeps all the features from 2.0. Our 1.0 and 1.1 end users can install version 1.2. This gives them the bugfix without any of the 2.0 features. It is impossible for a 2.1 end user to lose the bugfix, because the Delphix Engine will not allow the build number to go \"backwards\". So, a 2.1 end user will not be able to install versions 2.0, 1.1, or 1.0. It is also impossible for a 1.2 end user to lose the bugfix. They cannot install 1.0 or 1.1 because the build number is not allowed to decrease. They also cannot install 2.0. The missing data migration on 2.0 will prevent this. Note that a 1.2 end user can still upgrade to 2.1 at any time. This will allow them to keep the bugfix, and also take advantage of the new features that were part of 2.0.","title":"Backports and Hotfixes"},{"location":"Versioning_And_Upgrade/Backports_And_Hotfixes/#backports-and-hotfixes","text":"If your plugin uses an \"enterprise-style\" release strategy, then you'll probably want to occasionally provide new \"minor\" or \"patch\" versions that build atop older versions. Code changes that are applied atop old releases are usually called \"backports\". Sometimes, they are also called \"hotfixes\", if the change is specifically created for a single user. These releases present a problem: although they are built atop an older code branch, they are still newer than some releases from a newer code branch. Below, we'll walk through how we prevent users from \"upgrading\" to a new-branch release that would be incompatible with an installed old-branch release.","title":"Backports and Hotfixes"},{"location":"Versioning_And_Upgrade/Backports_And_Hotfixes/#motivating-example","text":"Let's take a look at an example of a possible timeline of releases. February : The initial version of a plugin is released, with build number \"1.0\". This is a simple plugin that uses a simple strategy for syncing dSources. April : A new version is released, with build number \"1.1\". This adds some bugfixes and adds some small optimizations to improve the performance of syncing. August : A new version is released, with build number \"2.0\". This uses a completely new syncing strategy that is far more sophisticated and efficient. Let's assume that not all users will want to upgrade to the 2.0 release immediately. So, even months later, you expect to have a significant number of users still on version 1.0 or 1.1. Later, in October, a bug is found which impacts all releases. This bug is important enough that you want to fix it for all of your end users (not just the ones using 2.0). Here are the behaviors we need: Our 2.0 end users should be able to get the new bugfix without giving up any of the major new features that were part of 2.0. Our 1.0 and 1.1 end users should be able to get the new bugfix without also needing to accept all the major new features that were part of 2.0. Once an end user has received the bugfix, it should be impossible to lose the bugfix in an upgrade.","title":"Motivating Example"},{"location":"Versioning_And_Upgrade/Backports_And_Hotfixes/#strategy","text":"You can include a data migration along with your bugfix. If your bugfix involves a schema change, you will have to do this anyways. If not, you can still include a data migration that simply does nothing. If a user with the bugfix attempts to \"upgrade\" to 2.0, the Delphix Engine will prevent it, because the 2.0 releases does not include this migration. You would typically follow these steps: Fix the bug by applying a code change atop the 2.0 code. Include the new data migration in your 2.1 release. Separately, apply the same bugfix atop the 1.1 code. Note: depending on how code changed between 1.1 and 2.0, this 1.1-based bugfix might not contain the exact same code as we used with 2.0. Make another new release of the plugin, this time with build number \"1.2\". This release includes the 1.1-based bugfix. It also should include the new data migration. This meets our requirements: Our 2.0 end users can install version 2.1. This gives them the bugfix, and keeps all the features from 2.0. Our 1.0 and 1.1 end users can install version 1.2. This gives them the bugfix without any of the 2.0 features. It is impossible for a 2.1 end user to lose the bugfix, because the Delphix Engine will not allow the build number to go \"backwards\". So, a 2.1 end user will not be able to install versions 2.0, 1.1, or 1.0. It is also impossible for a 1.2 end user to lose the bugfix. They cannot install 1.0 or 1.1 because the build number is not allowed to decrease. They also cannot install 2.0. The missing data migration on 2.0 will prevent this. Note that a 1.2 end user can still upgrade to 2.1 at any time. This will allow them to keep the bugfix, and also take advantage of the new features that were part of 2.0.","title":"Strategy"},{"location":"Versioning_And_Upgrade/Compatibility/","text":"Compatibility \u00b6 Before we allow a newly-uploaded plugin to replace an already-installed plugin, we have to make sure that it will not cause any problems. For example: The newly-uploaded plugin must be able to accept any existing data that has been written using the already-installed plugin. The user should not unexpectedly lose any features or bug fixes that are present in the already-installed plugin. These restrictions are enforced by the Delphix Engine, and sometimes, the plugin itself. Delphix Engine Rules \u00b6 The Delphix Engine will enforce these rules before a newly-uploded plugin is allowed to be installed: The build number may only move forward, not backwards. All data migration IDs that are present in the already-installed plugin must also be present on the newly-uploaded plugin. The newly-uploaded plugin may add more data migrations, of course.","title":"Compatibility"},{"location":"Versioning_And_Upgrade/Compatibility/#compatibility","text":"Before we allow a newly-uploaded plugin to replace an already-installed plugin, we have to make sure that it will not cause any problems. For example: The newly-uploaded plugin must be able to accept any existing data that has been written using the already-installed plugin. The user should not unexpectedly lose any features or bug fixes that are present in the already-installed plugin. These restrictions are enforced by the Delphix Engine, and sometimes, the plugin itself.","title":"Compatibility"},{"location":"Versioning_And_Upgrade/Compatibility/#delphix-engine-rules","text":"The Delphix Engine will enforce these rules before a newly-uploded plugin is allowed to be installed: The build number may only move forward, not backwards. All data migration IDs that are present in the already-installed plugin must also be present on the newly-uploaded plugin. The newly-uploaded plugin may add more data migrations, of course.","title":"Delphix Engine Rules"},{"location":"Versioning_And_Upgrade/Overview/","text":"Overview \u00b6 Once you start writing and releasing your plugin, you\u2019ll reach a point when bug fixes or new features may require schema changes. The plugin upgrade process enables objects that have been created with a prior schema to be migrated to the newly defined schema. When this happens, a new version of the plugin must be created. The following few pages will walk through how versions need to change between upgrades and what needs to be written in the plugin to make sure upgrade is successful. Plugin Versioning \u00b6 Like any other piece of software, plugins change over time. Every so often, there will be a new release. To keep track of the different releases, each plugin release has its own versioning information. Depending on what changes are included in a particular release, there are different rules and recommendations for how the versioning information should be changed. More information on versioning is located here . Upgrade \u00b6 Upgrade is the process by which an older version of a plugin is replaced by a newer version. Depending on what has changed between the two versions, this process may also include modifying pre-existing plugin defined objects so they conform to the new schema expected by the new version of the plugin. Information on the upgrade process can be found here .","title":"Overview"},{"location":"Versioning_And_Upgrade/Overview/#overview","text":"Once you start writing and releasing your plugin, you\u2019ll reach a point when bug fixes or new features may require schema changes. The plugin upgrade process enables objects that have been created with a prior schema to be migrated to the newly defined schema. When this happens, a new version of the plugin must be created. The following few pages will walk through how versions need to change between upgrades and what needs to be written in the plugin to make sure upgrade is successful.","title":"Overview"},{"location":"Versioning_And_Upgrade/Overview/#plugin-versioning","text":"Like any other piece of software, plugins change over time. Every so often, there will be a new release. To keep track of the different releases, each plugin release has its own versioning information. Depending on what changes are included in a particular release, there are different rules and recommendations for how the versioning information should be changed. More information on versioning is located here .","title":"Plugin Versioning"},{"location":"Versioning_And_Upgrade/Overview/#upgrade","text":"Upgrade is the process by which an older version of a plugin is replaced by a newer version. Depending on what has changed between the two versions, this process may also include modifying pre-existing plugin defined objects so they conform to the new schema expected by the new version of the plugin. Information on the upgrade process can be found here .","title":"Upgrade"},{"location":"Versioning_And_Upgrade/Replication/","text":"Replication \u00b6 A Delphix Engine (source) can be setup to replicate data objects to another Delphix Engine (target). Plugins built using the Virtualization SDK work seamlessly with Delphix Engine replication with no additional development required from plugin developers. Only a single version of a plugin can be active on a Delphix Engine at a time. We discuss some basic scenarios below. For more detailed information refer to the Delphix Engine Documentation . Replica Provisioning \u00b6 Replicated dSource or VDB snapshots can be used to provision new VDBs onto a target Delphix Engine, without failing over any of the objects. When provisioning a VDB from a replicated snapshot: A version of the plugin has to be installed on the target Delphix Engine. The versions of the plugins installed on the source and target Delphix Engines have to be compatible . Once provisioned, the VDB on the target Delphix Engine will be associated with the version of the plugin installed on the target Delphix Engine, any required data migrations will be run as part of the provisioning process. For more details refer to the Delphix Engine Documentation . Replication Failover \u00b6 On failover, there are three scenarios for each plugin: Scenario Outcome Source plugin not installed on target Delphix Engine The plugin will be failed over and marked as active on the target Delphix Engine. Source plugin version is equal to the target plugin version The plugin from the source will be merged with the plugin on the target Delphix Engine. Source plugin version is not equal to the target plugin version The plugin from the source will be marked inactive on the target Delphix Engine. An inactive plugin can be subsequently activated, after failover, if it is compatible with the existing active plugin. Activating a plugin will do an upgrade and merge the inactive plugin, and all its associated objects, with the active plugin. For more details refer to the Delphix Engine Documentation .","title":"Replication"},{"location":"Versioning_And_Upgrade/Replication/#replication","text":"A Delphix Engine (source) can be setup to replicate data objects to another Delphix Engine (target). Plugins built using the Virtualization SDK work seamlessly with Delphix Engine replication with no additional development required from plugin developers. Only a single version of a plugin can be active on a Delphix Engine at a time. We discuss some basic scenarios below. For more detailed information refer to the Delphix Engine Documentation .","title":"Replication"},{"location":"Versioning_And_Upgrade/Replication/#replica-provisioning","text":"Replicated dSource or VDB snapshots can be used to provision new VDBs onto a target Delphix Engine, without failing over any of the objects. When provisioning a VDB from a replicated snapshot: A version of the plugin has to be installed on the target Delphix Engine. The versions of the plugins installed on the source and target Delphix Engines have to be compatible . Once provisioned, the VDB on the target Delphix Engine will be associated with the version of the plugin installed on the target Delphix Engine, any required data migrations will be run as part of the provisioning process. For more details refer to the Delphix Engine Documentation .","title":"Replica Provisioning"},{"location":"Versioning_And_Upgrade/Replication/#replication-failover","text":"On failover, there are three scenarios for each plugin: Scenario Outcome Source plugin not installed on target Delphix Engine The plugin will be failed over and marked as active on the target Delphix Engine. Source plugin version is equal to the target plugin version The plugin from the source will be merged with the plugin on the target Delphix Engine. Source plugin version is not equal to the target plugin version The plugin from the source will be marked inactive on the target Delphix Engine. An inactive plugin can be subsequently activated, after failover, if it is compatible with the existing active plugin. Activating a plugin will do an upgrade and merge the inactive plugin, and all its associated objects, with the active plugin. For more details refer to the Delphix Engine Documentation .","title":"Replication Failover"},{"location":"Versioning_And_Upgrade/Upgrade/","text":"Upgrade \u00b6 Upgrade is the process of moving from an older version of a plugin to a newer version. Upgrading is not as simple as just replacing the installed plugin with a newer one. The main complication comes when the new plugin version makes changes to its schemas . Consider the case of a plugin that works with collections of text files -- the user points it to a directory tree containing text files, and the plugin syncs the files from there. The first release of such a plugin might have no link-related user options. So the plugin's linked source schema might define no properties at all: \"linkedSourceDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { } } And, the syncing code is very simple: @plugin . linked . pre_snapshot () def linked_pre_snapshot ( direct_source , repository , source_config ): libs . run_sync ( remote_connection = direct_source . connection , source_directory = source_config . path ) But, later, some users request a new feature -- they want to avoid syncing any backup or hidden files. So, a new plugin version is released. This time, there is a new boolean property in the linked source schema where users can elect to skip these files, if desired. \"linkedSourceDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"skipHiddenAndBackup\" ], \"properties\" : { \"skipHiddenAndBackup\" : { \"type\" : \"boolean\" } } } The plugin code that handles the syncing can now pay attention to this new boolean property: _HIDDEN_AND_BACKUP_SPECS = [ \"*.bak\" , \"*~\" , # Backup files from certain editors \".*\" # Unix-style hidden files ] @plugin . linked . pre_snapshot () def linked_pre_snapshot ( direct_source , repository , source_config ): exclude_spec = _HIDDEN_AND_BACKUP_SPECS if direct_source . parameters . skip_hidden_and_backup else [] libs . run_sync ( remote_connection = direct_source . connection , source_directory = source_config . path , exclude_paths = exclude_spec ) Suppose a user has an engine with linked sources created by the older version of this plugin. That is, the existing linked sources have no skipHiddenAndBackup property. If the user installs the new version of the plugin, we have a problem! The above pre_snapshot code from the new plugin will attempt to access the skip_hidden_and_backup property, which we've just seen will not exist! The solution to this problem is to use data migrations , explained below. Zero dSource and VDB downtime during plugin upgrade dSources and VDBs do not need to be disabled before a plugin upgrade is initiated. End users can continue to access data from existing VDBs during a plugin upgrade. However, while a particular plugin is in the process of being upgraded, no administrative Delphix Engine operations like: VDB Refresh, VDB Provision, dSource Disable/Enable etc will be allowed on the objects associated with that plugin. Objects associated with other plugins will not be affected. Data Migrations \u00b6 What is a Data Migration? \u00b6 Whenever a new version of a plugin is installed on a Delphix Engine, the engine needs to migrate pre-existing data from its old format (as specified by the schemas in the old version of the plugin), to its new format (as specified by the schemas in the new version of the plugin). A data migration is a function that is responsible for doing this conversion. It is provided by the plugin. Thus, when the new plugin version is installed, the engine will call all applicable data migrations provided by the new plugin. This ensures that all data is always in the format expected by the new plugin. A Simple Example \u00b6 Let's go back to the above example of the plugin that adds a new boolean option to allow users to avoid syncing backup and hidden files. Here is a data migration that the new plugin can provide to handle the data format change: @plugin . upgrade . linked_source ( \"2019.11.20\" ) def add_skip_option ( old_linked_source ): return { \"skipHiddenAndBackup\" : false } The exact rules for data migrations are covered in detail below . Here, we'll just walk through this code line by line and make some observations. @plugin . upgrade . linked_source ( \"2019.11.20\" ) The above line is a decorator that identifies the following function as a data migration. This particular migration will handle linked sources. It is given an ID of 2019.11.20 -- this controls when this migration is run in relation to other data migrations. def add_skip_option ( old_linked_source ): Note that the data migration takes an argument representing the old-format data. In this simple example, we know that there are no properties in the old-format data, so we can just ignore it. return { \"skipHiddenAndBackup\" : false } Here, we are returning a Python dictionary representing the new format of the data. In this example, the dictionary has only one field: skipHiddenAndBackup . Because the old version of the plugin had no ability to skip files, we default this property to false to match the new schema. Rules for Data Migrations \u00b6 As shown above, the a data migration receives old-format input and produces new-format output. The rules and recommendations for data migrations follow: Rules \u00b6 Input and output are Python dictionaries, with properties named exactly as specified in the schemas. Note that this differs from other plugin operations, where the inputs are defined with autogenerated Python classes , and whose properties use Python-style naming. Each data migration must be tagged with an ID string. This string must consist of one or more positive integers separated by periods. Data migration IDs must be numerically unique. Note that \"1.2\" , \"01.02\" , and \" 1.2.0.0.0\" are all considered to be identical. Once released, a data migration must never be deleted. An attempted upgrade will fail if the already-installed plugin version has a data migration that does not appear in the to-be-installed version. At upgrade time, the engine will find the set of new migrations provided by the new version that are not already part of the already-installed version. Each of these migrations will then be run, in the order specified below. After running all applicable migrations, the engine will confirm that the resultant data conforms to the new version's schemas. If not, the upgrade will fail. Note that there is no requirement or guarantee that the input or output of any particular data migration will conform to a schema. We only guarantee that the input to the first data migration conforms to the schema of the already-installed plugin version. And, we only require that the output of the final data migration conforms to the schema of the new plugin version. Data migrations are run in the order specified by their IDs. The ordering is numerical, not lexicographical. Thus \"1\" would run before \"2\" , which would run before \"10\" . Data migrations have no access to Platform Libraries or remote hosts. For example: If a data migration attempts to use run_bash the upgrade will fail. Note that the above rules imply that at least one data migration is required any time a schema change is made that would invalidate any data produced using a previous version of the plugin. For example: adding a \"required\" property to the new schema. Recommendations \u00b6 We recommend using a \"Year.Month.Date\" format like \"2019.11.04\" for migration IDs. You can use trailing integers as necessary (e.g. use \"2019.11.04.5\" if you need something to be run between \"2019.11.04\" and \"2019.11.05\" ). Even though they follow similar naming rules, migration IDs are not the same thing as plugin versions. We do not recommend using your plugin version in your migration IDs. We recommend using small, single-purpose data migrations. That is, if you end up making four schema changes over the course of developing a new plugin version, we recommend writing four different data migrations, one for each change. Data Migration Example \u00b6 Here is a very simple data migration. @plugin . upgrade . repository ( \"2019.12.15\" ) def add_new_flag_to_repo ( old_repository ): new_repository = dict ( old_repository ) new_repository [ \"useNewFeature\" ] = False return new_repository Debugging Data Migration Problems \u00b6 During the process of upgrading to a new version, the Delphix Engine will run all applicable data migrations, and then ensure that the resulting object matches the new schema. But, what if there is a bug, and the resulting object does not match the schema? Security Concerns Prevent Detailed Error Messages \u00b6 One problem here is that the Delphix Engine is limited in the information that it can provide in the error message. Ideally, the engine would say exactly what was wrong with the object (e.g.: \"The field port has the value 15 , but the schema says it has to have a value between 256 and 1024 \"). But, the Delphix Engine cannot do this for security reasons. Ordinarily, the Delphix Engine knows which fields contain sensitive information, and can redact such fields from error messages. But, the only reason the Delphix Engine has that knowledge is because the schema provides that information. If an object does not conform to the schema, then the Delphix Engine can't know what is sensitive and what isn't. Therefore, the error message here might lack the detail necessary to debug the problem. One Solution: Temporary Logging \u00b6 During development of a new plugin version, you may find yourself trying to find and fix such a bug. One technique is to use temporary logging. For example, while you are trying to locate and fix the bug, you could put a log statement at the very end of each of your data migrations, like so: logger.debug(\"Migration 2010.03.01 returning {}\".format(new_object)) return new_object See the Logging section for more information about logging works. From the logs, you'll be able to see exactly what each migration is returning. From there, hopefully the problem will become apparent. As a supplemental tool, consider pasting these results (along with your schema) into an online JSON validator for more information. Warning It is very important that you only use logging as a temporary debugging strategy. Such logging must be removed before you release the plugin to end users . If this logging ends up in your end product, it could cause a serious security concern. Please see our sensitive data best practices for more information. When Data Migrations Are Insufficient \u00b6 New versions of plugins often require some modification of data that was written using an older version of the same plugin. Data migrations handle this modification. Unfortunately, data migrations cannot always fully handle all possible upgrade scenarios by themselves. For example, a new plugin version might want to add a new required field to one of its schemas. But, the correct value for this new field might not be knowable while the upgrade is underway -- perhaps it must be entered by the user, or perhaps it would require automatic discovery to be rerun. Such a situation will require some user intervention after the upgrade. In all cases, of course you will want to clearly document to your users that there will extra work required so they can make sure they known what they are getting into before they decide to upgrade. Tip It should also be said that you should try to avoid cases like this. As much as possible, try to make your post-upgrade plugin function with no user intervention. Only resort to user intervention as a last resort. The recommended strategy here is to arrange for the affected objects to be in an \"invalid\" state, and for your plugin code to detect this state, and throw errors when the objects are used. For such a situation, we recommend the following process: Make your schema changes so that the affected property can be set in such a way that plugin code can identify it as being invalid. Typically this is done by allowing for some \"sentinel\" value. This may require you to have a less-strict schema definition than you might otherwise want. In your data migrations, make sure the affected properties are indeed marked invalid. In any plugin code that needs to use these properties, first check them for validity. If they are invalid, then raise an error that explains the situation to the user, and tells them what steps they need to take. Following are two examples of schema changes that need extra user intervention after upgrade. One will require a rediscovery, and the other will require the user to enter information. Autodiscovery Example \u00b6 Suppose that a new plugin version adds a new required field to its repository schema. This new field specifies a full path to a database installation. The following listing shows what we'd ideally like the new repository schema to look like ( installationPath is the new required property) \"repositoryDefinition\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\" }, \"installationPath\": { \"type\": \"string\", \"format\": \"unixpath\"} }, \"required\": [\"name\", \"installationPath\"], \"nameField\": \"name\", \"identityFields\": [\"name\"] } The new plugin's autodiscovery code will know how to find this full path. Therefore, any repositories that are discovered (or rediscovered) after the upgrade will have this path filled in correctly. But, there may be repositories that were discovered before the upgrade. The data migrations will have to ensure that some value is provided for this new field. However, a data migration will not be able to determine what the correct final value is. One way to handle this is to modify the schema to allow a special value to indicate that the object needs to be rediscovered. In this example, we'll change the schema from the ideal version above, removing the unixpath constraint on this string: \"installationPath\": { \"type\": \"string\" } Now, our data migration can set this property to some special sentinel value that will never be mistaken for an actual installation path. _REDISCOVERY_TOKEN = \"###_REPOSITORY_NEEDS_REDISCOVERY_###\" @plugin.upgrade.repository(\"2020.02.04.01\") def repo_path(old_repository): # We need to add in a repository path, but there is no way for us to know # what the correct path is here, so we cannot set this to anything useful. # Instead, we'll set a special sentinel value that will indicate that the # repository is unusable until the remote host is rediscovered. old_repository[\"installationPath\"] = _REDISCOVERY_TOKEN return old_repository Now, wherever the plugin needs to use this path, we'll need to check for this sentinel value, and error out if we find it. For example, we might need a valid path during the configure operation: @plugin.virtual.configure() def configure(virtual_source, snapshot, repository): if repository.installation_path == _REDISCOVERY_TOKEN: # We cannot use this repository as/is -- it must be rediscovered. msg = 'Unable to use repository \"{}\" because it has not been updated ' \\ 'since upgrade. Please re-run discovery and try again' raise UserError(msg.format(repository.name)) # ... actual configure code goes here Manual Entry \u00b6 Above, we looked at an example where the plugin could handle filling in new values for a new field at discovery time, so the user was simply asked to rediscover. Sometimes, though, users themselves will have to be the ones to supply new values. Suppose that a new plugin version wants to add a required field to the virtualSource object. This new property will tell which port the database should be accessible on. Ideally, we might want our new field to look like this: \"port\": {\"type\": \"integer\", \"minimum\": 1024, \"maximum\": 65535} Again, however, the data migration will not know which value is correct here. This is something the user must decide. Still, the data migration must provide some value. As before, we'll change the schema a bit from what would be ideal: \"port\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 65535} Now, our data migration can use the value 0 as code for \"this VDB needs user intervention\". @plugin.upgrade.virtual_source(\"2020.02.04.02\") def add_dummy_port(old_virtual_source): # Set the \"port\" property to 0 to act as a placeholder. old_virtual_source[\"port\"] = 0 return old_virtual_source As with the previous example, our plugin code will need to look for this special value, and raise an error so that the user knows what to do. This example shows the Virtual Source Reconfigure operation, but of course, similar code will be needed anywhere else that the new port property is required. @plugin.virtual.reconfigure() def virtual_reconfigure(virtual_source, repository, source_config, snapshot): if virtual_source.parameters.port == 0: raise UserError('VDB \"{}\" cannot function properly. Please choose a ' \\ 'port number for this VDB to use.'.format(virtual_source.parameters.name)) # ... actual reconfigure code goes here","title":"Upgrade"},{"location":"Versioning_And_Upgrade/Upgrade/#upgrade","text":"Upgrade is the process of moving from an older version of a plugin to a newer version. Upgrading is not as simple as just replacing the installed plugin with a newer one. The main complication comes when the new plugin version makes changes to its schemas . Consider the case of a plugin that works with collections of text files -- the user points it to a directory tree containing text files, and the plugin syncs the files from there. The first release of such a plugin might have no link-related user options. So the plugin's linked source schema might define no properties at all: \"linkedSourceDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { } } And, the syncing code is very simple: @plugin . linked . pre_snapshot () def linked_pre_snapshot ( direct_source , repository , source_config ): libs . run_sync ( remote_connection = direct_source . connection , source_directory = source_config . path ) But, later, some users request a new feature -- they want to avoid syncing any backup or hidden files. So, a new plugin version is released. This time, there is a new boolean property in the linked source schema where users can elect to skip these files, if desired. \"linkedSourceDefinition\" : { \"type\" : \"object\" , \"additionalProperties\" : false , \"required\" : [ \"skipHiddenAndBackup\" ], \"properties\" : { \"skipHiddenAndBackup\" : { \"type\" : \"boolean\" } } } The plugin code that handles the syncing can now pay attention to this new boolean property: _HIDDEN_AND_BACKUP_SPECS = [ \"*.bak\" , \"*~\" , # Backup files from certain editors \".*\" # Unix-style hidden files ] @plugin . linked . pre_snapshot () def linked_pre_snapshot ( direct_source , repository , source_config ): exclude_spec = _HIDDEN_AND_BACKUP_SPECS if direct_source . parameters . skip_hidden_and_backup else [] libs . run_sync ( remote_connection = direct_source . connection , source_directory = source_config . path , exclude_paths = exclude_spec ) Suppose a user has an engine with linked sources created by the older version of this plugin. That is, the existing linked sources have no skipHiddenAndBackup property. If the user installs the new version of the plugin, we have a problem! The above pre_snapshot code from the new plugin will attempt to access the skip_hidden_and_backup property, which we've just seen will not exist! The solution to this problem is to use data migrations , explained below. Zero dSource and VDB downtime during plugin upgrade dSources and VDBs do not need to be disabled before a plugin upgrade is initiated. End users can continue to access data from existing VDBs during a plugin upgrade. However, while a particular plugin is in the process of being upgraded, no administrative Delphix Engine operations like: VDB Refresh, VDB Provision, dSource Disable/Enable etc will be allowed on the objects associated with that plugin. Objects associated with other plugins will not be affected.","title":"Upgrade"},{"location":"Versioning_And_Upgrade/Upgrade/#data-migrations","text":"","title":"Data Migrations"},{"location":"Versioning_And_Upgrade/Upgrade/#what-is-a-data-migration","text":"Whenever a new version of a plugin is installed on a Delphix Engine, the engine needs to migrate pre-existing data from its old format (as specified by the schemas in the old version of the plugin), to its new format (as specified by the schemas in the new version of the plugin). A data migration is a function that is responsible for doing this conversion. It is provided by the plugin. Thus, when the new plugin version is installed, the engine will call all applicable data migrations provided by the new plugin. This ensures that all data is always in the format expected by the new plugin.","title":"What is a Data Migration?"},{"location":"Versioning_And_Upgrade/Upgrade/#a-simple-example","text":"Let's go back to the above example of the plugin that adds a new boolean option to allow users to avoid syncing backup and hidden files. Here is a data migration that the new plugin can provide to handle the data format change: @plugin . upgrade . linked_source ( \"2019.11.20\" ) def add_skip_option ( old_linked_source ): return { \"skipHiddenAndBackup\" : false } The exact rules for data migrations are covered in detail below . Here, we'll just walk through this code line by line and make some observations. @plugin . upgrade . linked_source ( \"2019.11.20\" ) The above line is a decorator that identifies the following function as a data migration. This particular migration will handle linked sources. It is given an ID of 2019.11.20 -- this controls when this migration is run in relation to other data migrations. def add_skip_option ( old_linked_source ): Note that the data migration takes an argument representing the old-format data. In this simple example, we know that there are no properties in the old-format data, so we can just ignore it. return { \"skipHiddenAndBackup\" : false } Here, we are returning a Python dictionary representing the new format of the data. In this example, the dictionary has only one field: skipHiddenAndBackup . Because the old version of the plugin had no ability to skip files, we default this property to false to match the new schema.","title":"A Simple Example"},{"location":"Versioning_And_Upgrade/Upgrade/#rules-for-data-migrations","text":"As shown above, the a data migration receives old-format input and produces new-format output. The rules and recommendations for data migrations follow:","title":"Rules for Data Migrations"},{"location":"Versioning_And_Upgrade/Upgrade/#rules","text":"Input and output are Python dictionaries, with properties named exactly as specified in the schemas. Note that this differs from other plugin operations, where the inputs are defined with autogenerated Python classes , and whose properties use Python-style naming. Each data migration must be tagged with an ID string. This string must consist of one or more positive integers separated by periods. Data migration IDs must be numerically unique. Note that \"1.2\" , \"01.02\" , and \" 1.2.0.0.0\" are all considered to be identical. Once released, a data migration must never be deleted. An attempted upgrade will fail if the already-installed plugin version has a data migration that does not appear in the to-be-installed version. At upgrade time, the engine will find the set of new migrations provided by the new version that are not already part of the already-installed version. Each of these migrations will then be run, in the order specified below. After running all applicable migrations, the engine will confirm that the resultant data conforms to the new version's schemas. If not, the upgrade will fail. Note that there is no requirement or guarantee that the input or output of any particular data migration will conform to a schema. We only guarantee that the input to the first data migration conforms to the schema of the already-installed plugin version. And, we only require that the output of the final data migration conforms to the schema of the new plugin version. Data migrations are run in the order specified by their IDs. The ordering is numerical, not lexicographical. Thus \"1\" would run before \"2\" , which would run before \"10\" . Data migrations have no access to Platform Libraries or remote hosts. For example: If a data migration attempts to use run_bash the upgrade will fail. Note that the above rules imply that at least one data migration is required any time a schema change is made that would invalidate any data produced using a previous version of the plugin. For example: adding a \"required\" property to the new schema.","title":"Rules"},{"location":"Versioning_And_Upgrade/Upgrade/#recommendations","text":"We recommend using a \"Year.Month.Date\" format like \"2019.11.04\" for migration IDs. You can use trailing integers as necessary (e.g. use \"2019.11.04.5\" if you need something to be run between \"2019.11.04\" and \"2019.11.05\" ). Even though they follow similar naming rules, migration IDs are not the same thing as plugin versions. We do not recommend using your plugin version in your migration IDs. We recommend using small, single-purpose data migrations. That is, if you end up making four schema changes over the course of developing a new plugin version, we recommend writing four different data migrations, one for each change.","title":"Recommendations"},{"location":"Versioning_And_Upgrade/Upgrade/#data-migration-example","text":"Here is a very simple data migration. @plugin . upgrade . repository ( \"2019.12.15\" ) def add_new_flag_to_repo ( old_repository ): new_repository = dict ( old_repository ) new_repository [ \"useNewFeature\" ] = False return new_repository","title":"Data Migration Example"},{"location":"Versioning_And_Upgrade/Upgrade/#debugging-data-migration-problems","text":"During the process of upgrading to a new version, the Delphix Engine will run all applicable data migrations, and then ensure that the resulting object matches the new schema. But, what if there is a bug, and the resulting object does not match the schema?","title":"Debugging Data Migration Problems"},{"location":"Versioning_And_Upgrade/Upgrade/#security-concerns-prevent-detailed-error-messages","text":"One problem here is that the Delphix Engine is limited in the information that it can provide in the error message. Ideally, the engine would say exactly what was wrong with the object (e.g.: \"The field port has the value 15 , but the schema says it has to have a value between 256 and 1024 \"). But, the Delphix Engine cannot do this for security reasons. Ordinarily, the Delphix Engine knows which fields contain sensitive information, and can redact such fields from error messages. But, the only reason the Delphix Engine has that knowledge is because the schema provides that information. If an object does not conform to the schema, then the Delphix Engine can't know what is sensitive and what isn't. Therefore, the error message here might lack the detail necessary to debug the problem.","title":"Security Concerns Prevent Detailed Error Messages"},{"location":"Versioning_And_Upgrade/Upgrade/#one-solution-temporary-logging","text":"During development of a new plugin version, you may find yourself trying to find and fix such a bug. One technique is to use temporary logging. For example, while you are trying to locate and fix the bug, you could put a log statement at the very end of each of your data migrations, like so: logger.debug(\"Migration 2010.03.01 returning {}\".format(new_object)) return new_object See the Logging section for more information about logging works. From the logs, you'll be able to see exactly what each migration is returning. From there, hopefully the problem will become apparent. As a supplemental tool, consider pasting these results (along with your schema) into an online JSON validator for more information. Warning It is very important that you only use logging as a temporary debugging strategy. Such logging must be removed before you release the plugin to end users . If this logging ends up in your end product, it could cause a serious security concern. Please see our sensitive data best practices for more information.","title":"One Solution: Temporary Logging"},{"location":"Versioning_And_Upgrade/Upgrade/#when-data-migrations-are-insufficient","text":"New versions of plugins often require some modification of data that was written using an older version of the same plugin. Data migrations handle this modification. Unfortunately, data migrations cannot always fully handle all possible upgrade scenarios by themselves. For example, a new plugin version might want to add a new required field to one of its schemas. But, the correct value for this new field might not be knowable while the upgrade is underway -- perhaps it must be entered by the user, or perhaps it would require automatic discovery to be rerun. Such a situation will require some user intervention after the upgrade. In all cases, of course you will want to clearly document to your users that there will extra work required so they can make sure they known what they are getting into before they decide to upgrade. Tip It should also be said that you should try to avoid cases like this. As much as possible, try to make your post-upgrade plugin function with no user intervention. Only resort to user intervention as a last resort. The recommended strategy here is to arrange for the affected objects to be in an \"invalid\" state, and for your plugin code to detect this state, and throw errors when the objects are used. For such a situation, we recommend the following process: Make your schema changes so that the affected property can be set in such a way that plugin code can identify it as being invalid. Typically this is done by allowing for some \"sentinel\" value. This may require you to have a less-strict schema definition than you might otherwise want. In your data migrations, make sure the affected properties are indeed marked invalid. In any plugin code that needs to use these properties, first check them for validity. If they are invalid, then raise an error that explains the situation to the user, and tells them what steps they need to take. Following are two examples of schema changes that need extra user intervention after upgrade. One will require a rediscovery, and the other will require the user to enter information.","title":"When Data Migrations Are Insufficient"},{"location":"Versioning_And_Upgrade/Upgrade/#autodiscovery-example","text":"Suppose that a new plugin version adds a new required field to its repository schema. This new field specifies a full path to a database installation. The following listing shows what we'd ideally like the new repository schema to look like ( installationPath is the new required property) \"repositoryDefinition\": { \"type\": \"object\", \"properties\": { \"name\": { \"type\": \"string\" }, \"installationPath\": { \"type\": \"string\", \"format\": \"unixpath\"} }, \"required\": [\"name\", \"installationPath\"], \"nameField\": \"name\", \"identityFields\": [\"name\"] } The new plugin's autodiscovery code will know how to find this full path. Therefore, any repositories that are discovered (or rediscovered) after the upgrade will have this path filled in correctly. But, there may be repositories that were discovered before the upgrade. The data migrations will have to ensure that some value is provided for this new field. However, a data migration will not be able to determine what the correct final value is. One way to handle this is to modify the schema to allow a special value to indicate that the object needs to be rediscovered. In this example, we'll change the schema from the ideal version above, removing the unixpath constraint on this string: \"installationPath\": { \"type\": \"string\" } Now, our data migration can set this property to some special sentinel value that will never be mistaken for an actual installation path. _REDISCOVERY_TOKEN = \"###_REPOSITORY_NEEDS_REDISCOVERY_###\" @plugin.upgrade.repository(\"2020.02.04.01\") def repo_path(old_repository): # We need to add in a repository path, but there is no way for us to know # what the correct path is here, so we cannot set this to anything useful. # Instead, we'll set a special sentinel value that will indicate that the # repository is unusable until the remote host is rediscovered. old_repository[\"installationPath\"] = _REDISCOVERY_TOKEN return old_repository Now, wherever the plugin needs to use this path, we'll need to check for this sentinel value, and error out if we find it. For example, we might need a valid path during the configure operation: @plugin.virtual.configure() def configure(virtual_source, snapshot, repository): if repository.installation_path == _REDISCOVERY_TOKEN: # We cannot use this repository as/is -- it must be rediscovered. msg = 'Unable to use repository \"{}\" because it has not been updated ' \\ 'since upgrade. Please re-run discovery and try again' raise UserError(msg.format(repository.name)) # ... actual configure code goes here","title":"Autodiscovery Example"},{"location":"Versioning_And_Upgrade/Upgrade/#manual-entry","text":"Above, we looked at an example where the plugin could handle filling in new values for a new field at discovery time, so the user was simply asked to rediscover. Sometimes, though, users themselves will have to be the ones to supply new values. Suppose that a new plugin version wants to add a required field to the virtualSource object. This new property will tell which port the database should be accessible on. Ideally, we might want our new field to look like this: \"port\": {\"type\": \"integer\", \"minimum\": 1024, \"maximum\": 65535} Again, however, the data migration will not know which value is correct here. This is something the user must decide. Still, the data migration must provide some value. As before, we'll change the schema a bit from what would be ideal: \"port\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 65535} Now, our data migration can use the value 0 as code for \"this VDB needs user intervention\". @plugin.upgrade.virtual_source(\"2020.02.04.02\") def add_dummy_port(old_virtual_source): # Set the \"port\" property to 0 to act as a placeholder. old_virtual_source[\"port\"] = 0 return old_virtual_source As with the previous example, our plugin code will need to look for this special value, and raise an error so that the user knows what to do. This example shows the Virtual Source Reconfigure operation, but of course, similar code will be needed anywhere else that the new port property is required. @plugin.virtual.reconfigure() def virtual_reconfigure(virtual_source, repository, source_config, snapshot): if virtual_source.parameters.port == 0: raise UserError('VDB \"{}\" cannot function properly. Please choose a ' \\ 'port number for this VDB to use.'.format(virtual_source.parameters.name)) # ... actual reconfigure code goes here","title":"Manual Entry"},{"location":"Versioning_And_Upgrade/Versioning/","text":"Versioning \u00b6 Almost all software products are periodically updated to include new features and bug fixes. Plugins are no exception -- a plugin's code will very likely be different two years from now. To deal with this, plugins use versioning . This just means that a plugin communicates (to the user, and to the Delphix Engine) exactly what code is in use. Versioning Information \u00b6 There are three different pieces of version-related information, each used for different purposes. External Version \u00b6 This field is intended only for use by the end user. The Delphix Engine does not use this field, and therefore imposes no restrictions on its content. This is a free-form string which the plugin can use in any way it feels like. Examples might be \"5.3.0\", \"2012B\", \"MyPlugin Millennium Edition, Service Pack 3\", \"Playful Platypus\" or \"Salton City\". The external version is specified using the externalVersion property in your plugin config file. Tip Use an external version that makes it easier for end users to determine newer vs older plugins. Build Number \u00b6 Unlike \"external version\", this field is intended to convey information to the Delphix Engine. This is a string of integers, separated by periods. Examples would be \"5.3.0\", \"7\", \"5.3.0.0.0.157\". The Delphix Engine uses the build number to guard against end users trying to \"downgrade\" their plugin to an older, incompatible version. So, if a user has build number \"3.4.1\" installed, then they may not install a version with a build number like \"2.x.y\", \"3.3.y\" or \"3.4.0\". The build number is specified using the buildNumber property in your plugin config file. This field is required to be a string. You might need to enclose your build number in quotes in order to prevent YAML from interpreting the field as a number. Examples: buildNumber Allowed Details 1 No YAML will interpret this as an integer. 1.2 No YAML will interpret this as a floating-point number. \"1\" Yes The quotes mean this is a string. \"1.2\" Yes The quotes mean this is a string. 1.2.3 Yes YAML treats this as a string, since it cannot be a number. Build Number Format Rules \u00b6 Your build number must be a string, conforming to these rules: The string must be composed of a sequence of non-negative integers, not all zero, separated by periods. Trailing zeros are ignored. So, \"1.0.0\" is treated the same as \"1\". Build numbers are sortable numerically, with earlier numbers having more significance than later numbers. So, \"2.0\" comes after \"1.99999\", and \"1.10\" comes after \"1.2\". The Delphix Engine will never allow installation of plugin with a build number that is ordered before the the already-installed build number. Tip You can upload a plugin with the same buildNumber as the installed plugin. However this should only be done while a plugin is being developed. Plugin releases for end users should never have the same buildNumber Please also see the App-Style vs. Enterprise-Style section below. We generally recommend using a single integer build number for app-style development. Build numbers need to have multiple parts if you are doing enterprise-style development. Release Strategies \u00b6 There are two main strategies for releasing software: \"App-style\" Release Strategy \u00b6 Here, all users are expected to use the latest available version of the software. Most consumer software works this way today -- websites, phone apps, etc. An app-style strategy is much simpler, but also more limiting: At any time, there is only one branch under active development. Customers that want bugfixes must upgrade to the latest version. The plugin's build number can be a simple integer that is incremented with each new release. \"Enterprise-style\" Release Strategy \u00b6 Here, you might distinguish \"major\" releases of your software from \"minor\" releases. You might expect some customers to continue to use older major releases for a long time, even after a new major release comes out. This strategy is often used for software like operating systems and DBMSs, where upgrading can cause significant disruption. An enterprise-style strategy is more flexible, but also more cumbersome: There may be multiple branches under active development at any time. Typically one branch for every \"major release\" that is still being supported. This requires careful coordination to make sure that each new code change ends up on the correct branch (or branches). It is possible to supply bugfix-only minor releases (often called \"patch releases\") which build atop older major releases. Customers do not need to move to the new major version in order to get these bugfixes. The plugin's build number needs to be composed of multiple integers. If you are using this strategy read more here about how to deal with backports and hotfixes. You may use whichever of these strategies works best for you. The SDK and the Delphix Engine support either strategy. You can even change your mind later and switch to the other strategy. Recommendations \u00b6 Build your plugin with the newest Virtualization SDK version available. Only publish one artifact built for a given official version of the plugin. The official release of a plugin should not use the same build number as a development build. Use an external version that helps easily identify newer plugins. Publish a plugin version compatibility matrix which lists out the plugin version, the Virtualization SDK it was built with and the Delphix Engine version(s) it supports.","title":"Versioning"},{"location":"Versioning_And_Upgrade/Versioning/#versioning","text":"Almost all software products are periodically updated to include new features and bug fixes. Plugins are no exception -- a plugin's code will very likely be different two years from now. To deal with this, plugins use versioning . This just means that a plugin communicates (to the user, and to the Delphix Engine) exactly what code is in use.","title":"Versioning"},{"location":"Versioning_And_Upgrade/Versioning/#versioning-information","text":"There are three different pieces of version-related information, each used for different purposes.","title":"Versioning Information"},{"location":"Versioning_And_Upgrade/Versioning/#external-version","text":"This field is intended only for use by the end user. The Delphix Engine does not use this field, and therefore imposes no restrictions on its content. This is a free-form string which the plugin can use in any way it feels like. Examples might be \"5.3.0\", \"2012B\", \"MyPlugin Millennium Edition, Service Pack 3\", \"Playful Platypus\" or \"Salton City\". The external version is specified using the externalVersion property in your plugin config file. Tip Use an external version that makes it easier for end users to determine newer vs older plugins.","title":"External Version"},{"location":"Versioning_And_Upgrade/Versioning/#build-number","text":"Unlike \"external version\", this field is intended to convey information to the Delphix Engine. This is a string of integers, separated by periods. Examples would be \"5.3.0\", \"7\", \"5.3.0.0.0.157\". The Delphix Engine uses the build number to guard against end users trying to \"downgrade\" their plugin to an older, incompatible version. So, if a user has build number \"3.4.1\" installed, then they may not install a version with a build number like \"2.x.y\", \"3.3.y\" or \"3.4.0\". The build number is specified using the buildNumber property in your plugin config file. This field is required to be a string. You might need to enclose your build number in quotes in order to prevent YAML from interpreting the field as a number. Examples: buildNumber Allowed Details 1 No YAML will interpret this as an integer. 1.2 No YAML will interpret this as a floating-point number. \"1\" Yes The quotes mean this is a string. \"1.2\" Yes The quotes mean this is a string. 1.2.3 Yes YAML treats this as a string, since it cannot be a number.","title":"Build Number"},{"location":"Versioning_And_Upgrade/Versioning/#build-number-format-rules","text":"Your build number must be a string, conforming to these rules: The string must be composed of a sequence of non-negative integers, not all zero, separated by periods. Trailing zeros are ignored. So, \"1.0.0\" is treated the same as \"1\". Build numbers are sortable numerically, with earlier numbers having more significance than later numbers. So, \"2.0\" comes after \"1.99999\", and \"1.10\" comes after \"1.2\". The Delphix Engine will never allow installation of plugin with a build number that is ordered before the the already-installed build number. Tip You can upload a plugin with the same buildNumber as the installed plugin. However this should only be done while a plugin is being developed. Plugin releases for end users should never have the same buildNumber Please also see the App-Style vs. Enterprise-Style section below. We generally recommend using a single integer build number for app-style development. Build numbers need to have multiple parts if you are doing enterprise-style development.","title":"Build Number Format Rules"},{"location":"Versioning_And_Upgrade/Versioning/#release-strategies","text":"There are two main strategies for releasing software:","title":"Release Strategies"},{"location":"Versioning_And_Upgrade/Versioning/#app-style-release-strategy","text":"Here, all users are expected to use the latest available version of the software. Most consumer software works this way today -- websites, phone apps, etc. An app-style strategy is much simpler, but also more limiting: At any time, there is only one branch under active development. Customers that want bugfixes must upgrade to the latest version. The plugin's build number can be a simple integer that is incremented with each new release.","title":"\"App-style\" Release Strategy"},{"location":"Versioning_And_Upgrade/Versioning/#enterprise-style-release-strategy","text":"Here, you might distinguish \"major\" releases of your software from \"minor\" releases. You might expect some customers to continue to use older major releases for a long time, even after a new major release comes out. This strategy is often used for software like operating systems and DBMSs, where upgrading can cause significant disruption. An enterprise-style strategy is more flexible, but also more cumbersome: There may be multiple branches under active development at any time. Typically one branch for every \"major release\" that is still being supported. This requires careful coordination to make sure that each new code change ends up on the correct branch (or branches). It is possible to supply bugfix-only minor releases (often called \"patch releases\") which build atop older major releases. Customers do not need to move to the new major version in order to get these bugfixes. The plugin's build number needs to be composed of multiple integers. If you are using this strategy read more here about how to deal with backports and hotfixes. You may use whichever of these strategies works best for you. The SDK and the Delphix Engine support either strategy. You can even change your mind later and switch to the other strategy.","title":"\"Enterprise-style\" Release Strategy"},{"location":"Versioning_And_Upgrade/Versioning/#recommendations","text":"Build your plugin with the newest Virtualization SDK version available. Only publish one artifact built for a given official version of the plugin. The official release of a plugin should not use the same build number as a development build. Use an external version that helps easily identify newer plugins. Publish a plugin version compatibility matrix which lists out the plugin version, the Virtualization SDK it was built with and the Delphix Engine version(s) it supports.","title":"Recommendations"}]}